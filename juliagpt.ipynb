{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "name": "JuliaGPT - Optimized GPT in Pure Julia",
   "include_colab_link": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": "<a href=\"https://colab.research.google.com/github/DavinciDreams/JuliaGPT/blob/main/juliagpt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# JuliaGPT — A Minimal GPT in Pure Julia\n\nFaithful port of Karpathy's JuliaGPT: the most atomic way to train a GPT.  \nArray-based autograd via AutoGrad.jl, transformer, Adam optimizer.  \nNo external dependencies beyond Julia stdlib + AutoGrad.jl.\n\n**Architecture** (following GPT-2 with simplifications):\n- AutoGrad.jl array-based automatic differentiation (`Param` wrapped matrices)\n- Single-layer transformer with multi-head attention\n- RMSNorm (not LayerNorm), no biases, ReLU (not GELU)\n- KV cache for natural causal masking\n- Adam optimizer with linear LR decay\n- Temperature-controlled generation\n- Best-model checkpointing with validation loss tracking (local + HuggingFace Hub)\n\nBased on: https://gist.github.com/karpathy/8627fe009c40f57531cb18360106ce95"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Setup\n",
    "\n",
    "**Credentials** (needed for checkpoint sync + logging):\n",
    "- Option A: Upload a `.env` file with `HF_TOKEN`, `WANDB_API_KEY`, and `HF_REPO`\n",
    "- Option B: Add secrets via Colab sidebar (key icon) with the same names\n",
    "- Option C: Set environment variables before launching Julia\n",
    "\n",
    "**Then just run all cells.** Colab already has Julia — no installation needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "using JSON3\n",
    "\n",
    "# ── Load environment variables from .env file ──\n",
    "function load_dotenv(paths=[\".env\", joinpath(homedir(), \".env\")])\n",
    "    for path in paths\n",
    "        isfile(path) || continue\n",
    "        println(\"Loading secrets from $path\")\n",
    "        for line in eachline(path)\n",
    "            stripped = strip(line)\n",
    "            (isempty(stripped) || startswith(stripped, '#')) && continue\n",
    "            m = match(r\"^([A-Za-z_][A-Za-z0-9_]*)\\s*=\\s*(.*)\", stripped)\n",
    "            m === nothing && continue\n",
    "            key, val = string(m[1]), strip(string(m[2]))\n",
    "            if length(val) >= 2 && val[1] == val[end] && val[1] in ('\"', '\\'')\n",
    "                val = val[2:end-1]\n",
    "            end\n",
    "            ENV[key] = val\n",
    "        end\n",
    "        return true\n",
    "    end\n",
    "    return false\n",
    "end\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# ── Read credentials from ENV ──\n",
    "if haskey(ENV, \"HF_TOKEN\") && !isempty(ENV[\"HF_TOKEN\"])\n",
    "    println(\"HF token: found\")\n",
    "else\n",
    "    println(\"HF token: not found (set HF_TOKEN in .env or Colab secrets)\")\n",
    "end\n",
    "\n",
    "if haskey(ENV, \"WANDB_API_KEY\") && !isempty(ENV[\"WANDB_API_KEY\"])\n",
    "    println(\"W&B API key: found\")\n",
    "else\n",
    "    println(\"W&B API key: not found (set WANDB_API_KEY in .env or Colab secrets)\")\n",
    "end\n",
    "\n",
    "HF_REPO_ID = get(ENV, \"HF_REPO\", \"\")\n",
    "if !isempty(HF_REPO_ID)\n",
    "    println(\"HF repo: \", HF_REPO_ID)\n",
    "else\n",
    "    println(\"HF repo: not set (set HF_REPO in .env or Colab secrets)\")\n",
    "end\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# ═══════════════════════════════════════════════════════════════\n# HuggingFace Hub helpers (pure Julia via HTTP.jl)\n# ═══════════════════════════════════════════════════════════════\n\nusing HTTP, Downloads\nimport Base64\nusing SHA\nusing HuggingFaceApi\n\n# ── Download files from HuggingFace ──\nfunction hf_download(repo_id::String, filename::String;\n                     local_dir::String=\".\", repo_type::String=\"dataset\")\n    local_path = joinpath(local_dir, filename)\n    isfile(local_path) && return local_path\n    mkpath(local_dir)\n    token = get(ENV, \"HF_TOKEN\", \"\")\n    try\n        path = HuggingFaceApi.hf_hub_download(repo_id, filename;\n                    repo_type=repo_type, auth_token=token)\n        cp(path, local_path; force=true)\n        println(\"  Downloaded: $filename ($(filesize(local_path)) bytes)\")\n    catch e\n        prefix = repo_type == \"dataset\" ? \"datasets/\" : \"\"\n        url = \"https://huggingface.co/$(prefix)$(repo_id)/resolve/main/$(filename)\"\n        headers = isempty(token) ? Pair{String,String}[] : [\"Authorization\" => \"Bearer $token\"]\n        Downloads.download(url, local_path; headers)\n        println(\"  Downloaded (HTTP fallback): $filename ($(filesize(local_path)) bytes)\")\n    end\n    return local_path\nend\n\n# ── Upload files to HuggingFace ──\n# Small files (<5MB): JSON commit API with base64 content\n# Large files (>=5MB): Git LFS batch API\nfunction hf_upload(repo_id::String, local_path::String;\n                   remote_path::String=\"\", repo_type::String=\"model\")\n    token = get(ENV, \"HF_TOKEN\", \"\")\n    isempty(token) && (@warn \"Cannot upload: no HF_TOKEN set\"; return)\n    rp = isempty(remote_path) ? basename(local_path) : remote_path\n    prefix = repo_type == \"model\" ? \"models\" : \"datasets\"\n    base_url = \"https://huggingface.co/api/$(prefix)/$(repo_id)\"\n    data = read(local_path)\n    auth_headers = [\"Authorization\" => \"Bearer $token\"]\n\n    if length(data) < 5_000_000\n        encoded = Base64.base64encode(data)\n        headers = vcat(auth_headers, [\"Content-Type\" => \"application/json\"])\n        body = JSON3.write(Dict(\n            \"summary\" => \"Upload $rp\",\n            \"files\" => [Dict(\"path\" => rp, \"content\" => encoded, \"encoding\" => \"base64\")]\n        ))\n        try\n            HTTP.post(\"$(base_url)/commit/main\", headers, body)\n            println(\"Pushed $local_path -> $repo_id/$rp ($(length(data)) bytes)\")\n        catch e\n            @warn \"Upload failed: $e\"\n        end\n    else\n        file_sha = bytes2hex(sha256(data))\n        headers = vcat(auth_headers, [\"Content-Type\" => \"application/json\"])\n        println(\"Uploading $(round(length(data)/1024/1024, digits=1)) MB via LFS...\")\n        lfs_url = \"https://huggingface.co/$(repo_id).git/info/lfs/objects/batch\"\n        lfs_body = JSON3.write(Dict(\n            \"operation\" => \"upload\",\n            \"transfers\" => [\"basic\"],\n            \"objects\" => [Dict(\"oid\" => file_sha, \"size\" => length(data))]\n        ))\n        lfs_headers = vcat(auth_headers, [\n            \"Content-Type\" => \"application/vnd.git-lfs+json\",\n            \"Accept\" => \"application/vnd.git-lfs+json\"\n        ])\n        try\n            resp = HTTP.post(lfs_url, lfs_headers, lfs_body)\n            lfs_resp = JSON3.read(String(resp.body))\n            obj = lfs_resp.objects[1]\n            if haskey(obj, :actions) && haskey(obj.actions, :upload)\n                upload_action = obj.actions.upload\n                upload_hdrs = Pair{String,String}[]\n                if haskey(upload_action, :header)\n                    for (k, v) in pairs(upload_action.header)\n                        push!(upload_hdrs, string(k) => string(v))\n                    end\n                end\n                HTTP.put(string(upload_action.href), upload_hdrs, data)\n            end\n            commit_body = JSON3.write(Dict(\n                \"summary\" => \"Upload $rp ($(round(length(data)/1024/1024, digits=1)) MB)\",\n                \"lfsFiles\" => [Dict(\"path\" => rp, \"algo\" => \"sha256\", \"oid\" => file_sha, \"size\" => length(data))]\n            ))\n            HTTP.post(\"$(base_url)/commit/main\", headers, commit_body)\n            println(\"Pushed $local_path -> $repo_id/$rp ($(length(data)) bytes, LFS)\")\n        catch e\n            @warn \"LFS upload failed: $e\"\n        end\n    end\nend\n\nfunction hf_push(repo_id::String, local_path::String; remote_path::String=\"\")\n    hf_upload(repo_id, local_path; remote_path)\nend\n\nfunction hf_pull(repo_id::String, filename::String; local_dir::String=\"checkpoints\")\n    hf_download(repo_id, filename; local_dir=local_dir, repo_type=\"model\")\nend\n\nfunction hf_push_checkpoint(repo_id::String; checkpoint_path::String=\"checkpoints/best_model.json\")\n    isfile(checkpoint_path) || error(\"Checkpoint not found: $checkpoint_path\")\n    hf_push(repo_id, checkpoint_path)\nend\n\nfunction hf_create_repo(repo_id::String)\n    token = get(ENV, \"HF_TOKEN\", \"\")\n    isempty(token) && return\n    parts = split(repo_id, \"/\")\n    name = length(parts) >= 2 ? parts[end] : repo_id\n    try\n        HTTP.post(\"https://huggingface.co/api/repos/create\",\n            [\"Authorization\" => \"Bearer $token\", \"Content-Type\" => \"application/json\"],\n            JSON3.write(Dict(\"name\" => name, \"type\" => \"model\", \"private\" => false)))\n        println(\"Created HF repo: $repo_id\")\n    catch\n        println(\"HF repo already exists or creation skipped: $repo_id\")\n    end\nend\n\nfunction hf_sync(local_path::String)\n    if !@isdefined(HF_REPO_ID) || isempty(HF_REPO_ID)\n        return\n    end\n    try\n        hf_push(HF_REPO_ID, local_path)\n    catch e\n        println(\"  HF sync failed: $e\")\n    end\nend\n\n# ═══════════════════════════════════════════════════════════════\n# W&B logging (pure Julia — logs to console, no Python needed)\n# ═══════════════════════════════════════════════════════════════\n\nwandb_init() = nothing\nwandb_log(; kwargs...) = nothing\nwandb_finish() = nothing\n\nprintln(\"HuggingFace + logging helpers defined (pure Julia, LFS-capable)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import Pkg\nPkg.add([\"JSON3\", \"AutoGrad\", \"HTTP\", \"HuggingFaceApi\", \"SHA\", \"Downloads\", \"CUDA\"])"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## 1. AutoGrad.jl Setup\n\nArray-based automatic differentiation using AutoGrad.jl.  \nParameters are wrapped with `Param()` for gradient tracking; `@diff` builds the computation tape and `grad()` extracts gradients."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Random\n",
    "using Printf\n",
    "using JSON3\n",
    "using AutoGrad\n",
    "using LinearAlgebra\n",
    "using Downloads\n",
    "\n",
    "Random.seed!(42)\n",
    "\n",
    "println(\"AutoGrad.jl loaded - array-based automatic differentiation\")\n"
   ]
  },
  {
   "cell_type": "code",
   "source": "RESOURCE_LIMIT = 0.50  # Use at most 50% of available resources\n\n# ── CPU Detection ──\ntotal_threads = Sys.CPU_THREADS\ntotal_cores = div(total_threads, 2)  # assume hyperthreading\nmax_blas_threads = max(1, Int(floor(total_cores * RESOURCE_LIMIT)))\n\nusing LinearAlgebra\nBLAS.set_num_threads(max_blas_threads)\n\nprintln(\"=== Resource Detection ===\")\nprintln(\"CPU: $(total_cores) cores / $(total_threads) threads → BLAS: $max_blas_threads threads\")\n\n# ── Memory Detection ──\ntotal_ram_gb = round(Sys.total_memory() / 1024^3, digits=1)\nfree_ram_gb = round(Sys.free_memory() / 1024^3, digits=1)\nram_limit_gb = round(free_ram_gb * RESOURCE_LIMIT, digits=1)\nprintln(\"RAM: $(total_ram_gb) GB total, $(free_ram_gb) GB free → limit: $(ram_limit_gb) GB\")\n\n# ── GPU Detection (informational — AutoGrad.jl is CPU-only) ──\nHAS_GPU = false\nGPU_NAME = \"\"\nGPU_VRAM_GB = 0.0\ntry\n    using CUDA\n    if CUDA.functional()\n        HAS_GPU = true\n        GPU_NAME = CUDA.name(CUDA.device())\n        GPU_VRAM_GB = round(CUDA.total_memory() / 1024^3, digits=1)\n        println(\"GPU: $GPU_NAME ($(GPU_VRAM_GB) GB) — detected but AutoGrad.jl runs on CPU\")\n    end\ncatch; end\n!HAS_GPU && println(\"GPU: not available\")\n\n# ═══════════════════════════════════════════════════════════════\n# Dynamic Compute Tier — scale model to available resources\n# AutoGrad.jl wraps plain Julia arrays (no CuArray support),\n# so we scale by RAM, not GPU VRAM.\n# ═══════════════════════════════════════════════════════════════\n\nif free_ram_gb >= 16.0\n    COMPUTE_TIER = :large\n    n_layer    = 4\n    n_embd     = 256\n    n_head     = 8\n    block_size = 256\nelseif free_ram_gb >= 8.0\n    COMPUTE_TIER = :medium\n    n_layer    = 2\n    n_embd     = 128\n    n_head     = 4\n    block_size = 256\nelse\n    COMPUTE_TIER = :small\n    n_layer    = 1\n    n_embd     = 64\n    n_head     = 4\n    block_size = 256\nend\n\nhead_dim = n_embd ÷ n_head\n\n# ── Training defaults (overridden after tokenization by data size) ──\ntarget_epochs  = 3\nlr             = 3e-3\nmin_lr         = 1e-5\nb1             = 0.9\nb2             = 0.999\neps            = 1e-8\nmax_grad_norm  = 1.0\nwarmup_frac    = 0.05\n\n# ── GC Configuration ──\nGC_INTERVAL = 100\n\nprintln(\"\\n=== Compute Tier: $COMPUTE_TIER ===\")\nprintln(\"  n_layer=$n_layer, n_embd=$n_embd, n_head=$n_head, head_dim=$head_dim\")\nprintln(\"  block_size=$block_size, lr=$lr, grad_clip=$max_grad_norm\")\nprintln(\"===================================\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n## 1c. Resource Sensing & Configuration\n\nAuto-detects CPU, RAM, and GPU resources and configures training to use at most 50% of available capacity.\nInspired by Unsloth's resource-aware training patterns: sense hardware, set optimal BLAS threads, and configure memory limits automatically.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Array-based neural network primitives (AutoGrad-compatible) ──\n",
    "# All operations use standard Julia array ops that AutoGrad can differentiate.\n",
    "# No in-place mutation inside @diff context.\n",
    "\n",
    "# ReLU for arrays (element-wise)\n",
    "relu_ag(x) = max.(x, Float32(0))\n",
    "\n",
    "# RMSNorm for a vector\n",
    "function rmsnorm_ag(x)\n",
    "    n = length(x)\n",
    "    ms = sum(x .* x) / n\n",
    "    scale = (ms + Float32(1e-5)) ^ Float32(-0.5)\n",
    "    return x .* scale\n",
    "end\n",
    "\n",
    "# Softmax for a vector (numerically stable)\n",
    "function softmax_ag(logits)\n",
    "    mx = maximum(logits)\n",
    "    exps = exp.(logits .- mx)\n",
    "    s = sum(exps)\n",
    "    return exps ./ s\n",
    "end\n",
    "\n",
    "# Type-dispatch gradient densifier (replaces try/catch in hot loop)\n",
    "function to_dense_grad(g)\n",
    "    if g isa AutoGrad.Sparse\n",
    "        Float32.(AutoGrad.full(g))\n",
    "    else\n",
    "        Float32.(g)\n",
    "    end\n",
    "end\n",
    "\n",
    "println(\"Array-based primitives defined (relu_ag, rmsnorm_ag, softmax_ag, to_dense_grad)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# backward! is no longer needed -- AutoGrad.jl handles backpropagation via @diff + grad()\n# Usage:\n#   tape = @diff loss_expression    # builds computation tape\n#   g = grad(tape, some_param)      # extracts gradient for a Param\n#   loss_val = value(tape)          # extracts scalar loss value\nprintln(\"Using AutoGrad.jl for automatic differentiation (no manual backward! needed)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## 2. Dataset — Download & Load Training Data\n\nDownloads **54 classical philosophy texts** from Project Gutenberg and MIT Classics Archive, spanning:\n- **Plato** (12 works): Republic, Apology, Symposium, Phaedo, Crito, Meno, Phaedrus, Timaeus, Laws, Gorgias, Protagoras, Theaetetus\n- **Aristotle** (12 works): Categories, Ethics, Rhetoric, Physics, Metaphysics, Poetics, Politics, On the Soul, On the Heavens, Prior/Posterior Analytics, Topics, On Generation & Corruption\n- **Stoics** (4 works): Marcus Aurelius Meditations, Epictetus Discourses & Enchiridion, Seneca Moral Essays\n- **Roman** (4 works): Lucretius, Cicero (On Duties, Nature of Gods, On Friendship)\n- **Early Modern** (6 works): Descartes, Kant, Spinoza, Hobbes, Locke, Bacon\n- **Enlightenment/19th c.** (10 works): Hume, Rousseau, Nietzsche (x2), Mill (x2), Machiavelli, Emerson, Thoreau, Montaigne\n- **Other** (6 works): Boethius, Diogenes/Epicurus, Latin Grammar, Schopenhauer\n\nEach text is split into paragraphs to form the `TRAINING_DATA` document array.\nVocabulary is built dynamically from the data (a-z + punctuation + BOS)."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ── Load data from HuggingFace dataset repo ──\n# Same pre-cleaned philosophy corpus used by juliaflux_v2\n# (produced by the text-pipeline: clean → chunk → train.txt/val.txt)\n\nDATA_DIR = \"data\"\nmkpath(DATA_DIR)\n\nfor f in [\"train.txt\", \"val.txt\"]\n    if !isfile(joinpath(DATA_DIR, f))\n        println(\"Downloading $f from $HF_DATA_REPO ...\")\n        hf_download(HF_DATA_REPO, f; local_dir=DATA_DIR, repo_type=\"dataset\")\n    end\nend\n\n# Try to get BPE tokenizer\ntokenizer_file = joinpath(DATA_DIR, \"tokenizer.json\")\nif !isfile(tokenizer_file)\n    try\n        hf_download(HF_DATA_REPO, \"tokenizer.json\"; local_dir=DATA_DIR, repo_type=\"dataset\")\n    catch\n        println(\"  No tokenizer.json available (will use char-level tokenizer)\")\n    end\nend\n\ntrain_text = read(joinpath(DATA_DIR, \"train.txt\"), String)\nval_text = read(joinpath(DATA_DIR, \"val.txt\"), String)\n\nprintln(\"Data loaded from $HF_DATA_REPO:\")\nprintln(\"  train.txt: $(length(train_text)) chars\")\nprintln(\"  val.txt:   $(length(val_text)) chars\")\n\n# Data sanity check\nunique_chars = sort(unique(filter(c -> c != '\\n', train_text)))\nn_punct = count(c -> c in \".,;:?!'\\\"-()[]\", train_text)\nprintln(\"  unique chars: $(length(unique_chars))  |  punctuation: $(n_punct) ($(round(100*n_punct/length(train_text), digits=1))%)\")\nif length(unique_chars) < 10\n    @warn \"Very few unique characters ($(length(unique_chars))) — data may be over-cleaned\"\nend\n\n# ── Split into paragraphs/chunks for token-by-token processing ──\n# The HF data has newline-separated chunks; each becomes a training document\nTRAINING_DATA = String[]\nfor text in [train_text, val_text]\n    for chunk in split(text, '\\n')\n        cleaned = strip(String(chunk))\n        if length(cleaned) >= 20\n            # Respect block_size: split long chunks\n            while length(cleaned) > block_size * 2\n                cutoff = min(block_size * 2, length(cleaned))\n                dot_pos = findlast('.', cleaned[1:cutoff])\n                if dot_pos !== nothing && dot_pos > 50\n                    push!(TRAINING_DATA, strip(cleaned[1:dot_pos]))\n                    cleaned = strip(cleaned[dot_pos+1:end])\n                else\n                    push!(TRAINING_DATA, strip(cleaned[1:cutoff]))\n                    cleaned = strip(cleaned[cutoff+1:end])\n                end\n            end\n            if length(cleaned) >= 20\n                push!(TRAINING_DATA, cleaned)\n            end\n        end\n    end\nend\n\nprintln(\"\\nTRAINING_DATA: $(length(TRAINING_DATA)) chunks\")\nprintln(\"Total characters: $(sum(length, TRAINING_DATA))\")\nprintln(\"Sample: \\\"$(TRAINING_DATA[1][1:min(80, end)])...\\\"\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## 3. Neural Network Helpers\n\nWeight matrices are `Param(Matrix{Float32})` -- AutoGrad.jl tracks gradients through array operations automatically.  \nHelper functions for parameter initialization, key ordering, and checkpoint support."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Helper: deterministic parameter key ordering (for checkpoint serialization)\nfunction get_param_keys(n_layer::Int)\n    keys = [\"wte\", \"wpe\", \"lm_head\"]\n    for i in 0:n_layer-1\n        append!(keys, [\n            \"layer$i.attn_wq\", \"layer$i.attn_wk\", \"layer$i.attn_wv\", \"layer$i.attn_wo\",\n            \"layer$i.mlp_fc1\", \"layer$i.mlp_fc2\"\n        ])\n    end\n    return keys\nend\n\n# Helper: initialize a Param-wrapped weight matrix\nfunction init_param(nout::Int, nin::Int; std=0.08f0)\n    Param(randn(Float32, nout, nin) .* std)\nend\n\n# Helper: collect all Param objects from state_dict in deterministic order\nfunction collect_params(state_dict, param_keys)\n    ps = []\n    for key in param_keys\n        push!(ps, state_dict[key])\n    end\n    return ps\nend\n\nprintln(\"Neural network helpers defined (get_param_keys, init_param, collect_params)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## 4. GPT Forward Pass\n\nProcesses one token at a time with KV cache for causal masking.  \nAll operations use array matrix multiplications via AutoGrad.jl.  \nKV cache stores detached (plain Float32) vectors -- gradients flow only through the current token."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "function gpt(token_id::Int, pos_id::Int,\n             kv_key_mats::Vector{Matrix{Float32}},\n             kv_val_mats::Vector{Matrix{Float32}},\n             kv_lens::Vector{Int},\n             params,\n             n_layer::Int, n_head::Int, head_dim::Int)\n\n    d_model = n_head * head_dim\n\n    # Embedding lookup: row indexing into Param matrices\n    tok_emb = params.wte[token_id, :]\n    pos_emb = params.wpe[pos_id, :]\n    x = tok_emb .+ pos_emb\n    x = rmsnorm_ag(x)\n\n    for li in 1:n_layer\n        layer = params.layers[li]\n        x_res = x\n        x = rmsnorm_ag(x)\n\n        # Linear projections: W * x where W is (d_model, d_model)\n        q = layer.attn_wq * x\n        k = layer.attn_wk * x\n        v = layer.attn_wv * x\n\n        # Detach K, V for cache storage (gradients don't flow through cached entries)\n        k_detached = Float32.(value(k))\n        v_detached = Float32.(value(v))\n\n        # In-place column write into pre-allocated cache (plain Float32, not tracked)\n        idx = kv_lens[li] + 1\n        kv_key_mats[li][:, idx] = k_detached\n        kv_val_mats[li][:, idx] = v_detached\n        kv_lens[li] = idx\n\n        # Zero-allocation view instead of hcat\n        n_cached = idx\n        K_mat = @view kv_key_mats[li][:, 1:n_cached]\n        V_mat = @view kv_val_mats[li][:, 1:n_cached]\n\n        # Multi-head attention with typed ntuple (compiler can unroll)\n        head_results = ntuple(n_head) do hh\n            h = hh - 1\n            hs = h * head_dim + 1\n            he = hs + head_dim - 1\n\n            q_h = q[hs:he]                   # (head_dim,) tracked\n            K_h = K_mat[hs:he, :]             # (head_dim, n_cached) plain Float32\n            V_h = V_mat[hs:he, :]             # (head_dim, n_cached) plain Float32\n\n            # Attention scores: K_h' * q_h / sqrt(head_dim)\n            scores = (K_h' * q_h) ./ Float32(sqrt(head_dim))\n            attn_w = softmax_ag(scores)\n\n            # Weighted sum: V_h * attn_w\n            V_h * attn_w\n        end\n\n        x_attn = vcat(head_results...)    # (d_model,) tracked\n\n        # Output projection\n        x = layer.attn_wo * x_attn\n        x = x .+ x_res\n\n        # MLP block\n        x_res = x\n        x = rmsnorm_ag(x)\n        x = layer.mlp_fc1 * x\n        x = relu_ag(x)\n        x = layer.mlp_fc2 * x\n        x = x .+ x_res\n    end\n\n    # LM head: (vocab_size, d_model) * (d_model,) = (vocab_size,)\n    logits = params.lm_head * x\n    return logits\nend\n\n# Helper: build type-stable params NamedTuple from state_dict\nfunction build_params(state_dict, n_layer::Int)\n    layers = ntuple(n_layer) do li\n        i = li - 1\n        (\n            attn_wq = state_dict[\"layer$i.attn_wq\"],\n            attn_wk = state_dict[\"layer$i.attn_wk\"],\n            attn_wv = state_dict[\"layer$i.attn_wv\"],\n            attn_wo = state_dict[\"layer$i.attn_wo\"],\n            mlp_fc1 = state_dict[\"layer$i.mlp_fc1\"],\n            mlp_fc2 = state_dict[\"layer$i.mlp_fc2\"],\n        )\n    end\n    return (\n        wte = state_dict[\"wte\"],\n        wpe = state_dict[\"wpe\"],\n        lm_head = state_dict[\"lm_head\"],\n        layers = layers,\n    )\nend\n\n# Helper: allocate KV cache buffers\nfunction alloc_kv_cache(n_layer::Int, d_model::Int, block_size::Int)\n    kv_key_mats = [zeros(Float32, d_model, block_size) for _ in 1:n_layer]\n    kv_val_mats = [zeros(Float32, d_model, block_size) for _ in 1:n_layer]\n    kv_lens = zeros(Int, n_layer)\n    return kv_key_mats, kv_val_mats, kv_lens\nend\n\n# Helper: reset KV cache for a new sequence\nfunction reset_kv_cache!(kv_lens::Vector{Int})\n    kv_lens .= 0\nend\n\nprintln(\"GPT forward pass defined (optimized: pre-allocated KV cache, typed params, unrolled heads)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## 5. Checkpoint Save/Load\n\nSave and load model weights + optimizer state as JSON.\nCheckpoints are synced to HuggingFace Hub for persistence across Colab sessions."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function save_checkpoint(path::String, state_dict, param_keys, uchars, hyperparams;\n",
    "                         adam_m=nothing, adam_v=nothing, step::Int=0,\n",
    "                         lr::Float64=0.01, b1::Float64=0.85, b2::Float64=0.99,\n",
    "                         best_val_loss::Float64=Inf,\n",
    "                         train_losses::Vector{Float64}=Float64[],\n",
    "                         val_losses::Vector{Float64}=Float64[],\n",
    "                         total_steps::Int=0, num_steps_target::Int=0)\n",
    "\n",
    "    sd_data = Dict{String,Any}()\n",
    "    for k in param_keys\n",
    "        W = value(state_dict[k])  # unwrap Param -> Matrix{Float32}\n",
    "        sd_data[k] = [Float64.(W[i, :]) for i in 1:size(W, 1)]\n",
    "    end\n",
    "\n",
    "    # Serialize Adam state per param key\n",
    "    adam_m_data = Dict{String,Any}()\n",
    "    adam_v_data = Dict{String,Any}()\n",
    "    if adam_m !== nothing\n",
    "        for k in param_keys\n",
    "            if haskey(adam_m, k)\n",
    "                adam_m_data[k] = [Float64.(adam_m[k][i, :]) for i in 1:size(adam_m[k], 1)]\n",
    "                adam_v_data[k] = [Float64.(adam_v[k][i, :]) for i in 1:size(adam_v[k], 1)]\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "\n",
    "    checkpoint = Dict{String,Any}(\n",
    "        \"format\" => \"autograd_v2\",\n",
    "        \"uchars\" => [string(c) for c in uchars],\n",
    "        \"hyperparams\" => hyperparams,\n",
    "        \"state_dict\" => sd_data,\n",
    "        \"optimizer\" => Dict{String,Any}(\n",
    "            \"adam_m\" => adam_m_data,\n",
    "            \"adam_v\" => adam_v_data,\n",
    "            \"step\" => step,\n",
    "            \"lr\" => lr,\n",
    "            \"beta1\" => b1,\n",
    "            \"beta2\" => b2\n",
    "        ),\n",
    "        \"training\" => Dict{String,Any}(\n",
    "            \"best_val_loss\" => best_val_loss,\n",
    "            \"train_losses\" => train_losses,\n",
    "            \"val_losses\" => val_losses,\n",
    "            \"total_steps_completed\" => total_steps,\n",
    "            \"num_steps_target\" => num_steps_target\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Replace Inf with large number for JSON compatibility\n",
    "    if best_val_loss == Inf\n",
    "        checkpoint[\"training\"][\"best_val_loss\"] = 1e30\n",
    "    end\n",
    "    mkpath(dirname(path))\n",
    "    open(path, \"w\") do f\n",
    "        JSON3.write(f, checkpoint)\n",
    "    end\n",
    "    vl_str = best_val_loss == Inf ? \"Inf\" : @sprintf(\"%.4f\", best_val_loss)\n",
    "    println(\"Checkpoint saved: $path (step $step, best_val_loss=$vl_str)\")\n",
    "end\n",
    "\n",
    "function load_checkpoint(path::String)\n",
    "    println(\"Loading checkpoint from $path ...\")\n",
    "    raw = JSON3.read(read(path, String))\n",
    "\n",
    "    uchars = [only(String(s)) for s in raw[\"uchars\"]]\n",
    "    BOS = length(uchars) + 1\n",
    "    vocab_size = BOS\n",
    "\n",
    "    hp = raw[\"hyperparams\"]\n",
    "    n_layer = Int(hp[\"n_layer\"])\n",
    "    n_embd = Int(hp[\"n_embd\"])\n",
    "    block_size = Int(hp[\"block_size\"])\n",
    "    n_head = Int(hp[\"n_head\"])\n",
    "    head_dim = n_embd ÷ n_head\n",
    "\n",
    "    # Detect format version\n",
    "    fmt = haskey(raw, \"format\") ? String(raw[\"format\"]) : \"v1\"\n",
    "\n",
    "    state_dict = Dict{String, Any}()\n",
    "    for (key, matrix_rows) in pairs(raw[\"state_dict\"])\n",
    "        rows = [Float32.(collect(row)) for row in matrix_rows]\n",
    "        W = vcat([reshape(r, 1, :) for r in rows]...)  # stack rows into Matrix\n",
    "        state_dict[string(key)] = Param(W)\n",
    "    end\n",
    "\n",
    "    # Load Adam state\n",
    "    opt_raw = raw[\"optimizer\"]\n",
    "    adam_m = Dict{String, Matrix{Float32}}()\n",
    "    adam_v = Dict{String, Matrix{Float32}}()\n",
    "\n",
    "    if fmt == \"autograd_v2\" && haskey(opt_raw, \"adam_m\")\n",
    "        am_raw = opt_raw[\"adam_m\"]\n",
    "        av_raw = opt_raw[\"adam_v\"]\n",
    "        if !isempty(am_raw)\n",
    "            for (key, matrix_rows) in pairs(am_raw)\n",
    "                rows = [Float32.(collect(row)) for row in matrix_rows]\n",
    "                adam_m[string(key)] = vcat([reshape(r, 1, :) for r in rows]...)\n",
    "            end\n",
    "            for (key, matrix_rows) in pairs(av_raw)\n",
    "                rows = [Float32.(collect(row)) for row in matrix_rows]\n",
    "                adam_v[string(key)] = vcat([reshape(r, 1, :) for r in rows]...)\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "\n",
    "    step = Int(opt_raw[\"step\"])\n",
    "    lr = Float64(opt_raw[\"lr\"])\n",
    "    b1 = Float64(opt_raw[\"beta1\"])\n",
    "    b2 = Float64(opt_raw[\"beta2\"])\n",
    "\n",
    "    trn = raw[\"training\"]\n",
    "    best_val_loss = Float64(trn[\"best_val_loss\"])\n",
    "    train_losses = Float64.(collect(trn[\"train_losses\"]))\n",
    "    val_losses = Float64.(collect(trn[\"val_losses\"]))\n",
    "    total_steps = Int(trn[\"total_steps_completed\"])\n",
    "    num_steps_target = Int(trn[\"num_steps_target\"])\n",
    "\n",
    "    println(\"  vocab=$vocab_size, embd=$n_embd, layers=$n_layer, step=$step\")\n",
    "\n",
    "    return (;\n",
    "        state_dict, uchars, BOS, vocab_size,\n",
    "        n_layer, n_embd, block_size, n_head, head_dim,\n",
    "        adam_m, adam_v, step, lr, b1, b2,\n",
    "        best_val_loss, train_losses, val_losses,\n",
    "        total_steps, num_steps_target\n",
    "    )\n",
    "end\n",
    "\n",
    "println(\"Checkpoint save/load defined (autograd_v2 format)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Setup — Dataset, Tokenizer, Parameters\n",
    "\n",
    "Character-level tokenizer with a BOS token. 90/10 train/val split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ── Dataset with train/val split (ordered, not shuffled) ──\ndocs = copy(TRAINING_DATA)\nsplit_idx = max(1, Int(floor(0.9 * length(docs))))\ntrain_docs = docs[1:split_idx]\nval_docs = docs[split_idx+1:end]\nif isempty(val_docs)\n    val_docs = docs[max(1, end-4):end]\n    train_docs = docs[1:max(1, end-5)]\nend\nprintln(\"train: $(length(train_docs)) docs, val: $(length(val_docs)) docs\")\n\n# ── Tokenizer: BPE with character-level fallback ──\nTOKENIZER_PATH = joinpath(DATA_DIR, \"tokenizer.json\")\nUSE_BPE = isfile(TOKENIZER_PATH)\n\nif USE_BPE\n    println(\"Loading BPE tokenizer from $TOKENIZER_PATH ...\")\n    tok_raw = read(TOKENIZER_PATH, String)\n    tok_json = JSON3.read(tok_raw)\n\n    bpe_vocab = Dict{String, Int}()\n    for (tok_str, id) in pairs(tok_json.model.vocab)\n        bpe_vocab[string(tok_str)] = Int(id) + 1  # 1-indexed for Julia\n    end\n\n    bpe_merges = Vector{Tuple{String,String}}()\n    for merge_str in tok_json.model.merges\n        parts = split(string(merge_str), \" \", limit=2)\n        if length(parts) == 2\n            push!(bpe_merges, (String(parts[1]), String(parts[2])))\n        end\n    end\n\n    bpe_id_to_token = Dict{Int, String}(id => tok for (tok, id) in bpe_vocab)\n    BOS = length(bpe_vocab) + 1\n    vocab_size = BOS\n\n    # ── GPT-2 byte-to-unicode mapping ──\n    # HuggingFace ByteLevel BPE maps each byte to a printable unicode char.\n    function build_byte_to_unicode()\n        bs = UInt8[]\n        cs = Char[]\n        for r in [0x21:0x7e, 0xa1:0xac, 0xae:0xff]\n            for b in r\n                push!(bs, b)\n                push!(cs, Char(b))\n            end\n        end\n        n = 0\n        for b in 0x00:0xff\n            if b ∉ bs\n                push!(bs, b)\n                push!(cs, Char(256 + n))\n                n += 1\n            end\n        end\n        return Dict(bs[i] => string(cs[i]) for i in eachindex(bs))\n    end\n\n    const _b2u = build_byte_to_unicode()\n    const _u2b = Dict(v[1] => k for (k, v) in _b2u)\n\n    # ── BPE merge (per word, not whole corpus!) ──\n    function bpe_encode_word(word::Vector{String})\n        tokens = copy(word)\n        for (a, b) in bpe_merges\n            new_tokens = String[]\n            i = 1\n            while i <= length(tokens)\n                if i < length(tokens) && tokens[i] == a && tokens[i+1] == b\n                    push!(new_tokens, a * b)\n                    i += 2\n                else\n                    push!(new_tokens, tokens[i])\n                    i += 1\n                end\n            end\n            tokens = new_tokens\n            length(tokens) <= 1 && break\n        end\n        return tokens\n    end\n\n    # ── GPT-2 pre-tokenization regex ──\n    const _GPT2_PAT = r\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\n\n    function tokenize_doc(doc::String, _unused1, BOS::Int)\n        ids = [BOS]\n        for m in eachmatch(_GPT2_PAT, doc)\n            word_bytes = Vector{UInt8}(m.match)\n            chars = [_b2u[b] for b in word_bytes]\n            tokens = bpe_encode_word(chars)\n            for tok in tokens\n                id = get(bpe_vocab, tok, nothing)\n                id !== nothing && push!(ids, id)\n            end\n        end\n        push!(ids, BOS)\n        return ids\n    end\n\n    # For decode in inference\n    uchars = Char[]  # not used with BPE, but checkpoint format expects it\n    function decode_ids(ids::Vector{Int})\n        text = join(get(bpe_id_to_token, id, \"\") for id in ids if id != BOS)\n        bytes = UInt8[get(_u2b, c, UInt8(c)) for c in text if haskey(_u2b, c)]\n        return String(bytes)\n    end\n\n    println(\"BPE tokenizer: vocab_size=$vocab_size ($(length(bpe_vocab)) tokens + BOS)\")\n    println(\"  $(length(bpe_merges)) merges\")\n\nelse\n    # ── Fallback: character-level tokenizer ──\n    println(\"No tokenizer.json — using character-level tokenizer\")\n\n    uchars = sort(unique(join(docs)))\n    BOS = length(uchars) + 1\n    vocab_size = BOS\n    char_to_id = Dict{Char, Int}(ch => i for (i, ch) in enumerate(uchars))\n\n    function tokenize_doc(doc::String, char_to_id::Dict{Char,Int}, BOS::Int)\n        vcat([BOS], [char_to_id[ch] for ch in doc if haskey(char_to_id, ch)], [BOS])\n    end\n\n    function decode_ids(ids::Vector{Int})\n        join(id <= length(uchars) ? uchars[id] : ' ' for id in ids if id != BOS)\n    end\n\n    println(\"Char-level tokenizer: vocab_size=$vocab_size ($(length(uchars)) chars + BOS)\")\nend\n\n# ── Pre-tokenize all documents ──\nprintln(\"Pre-tokenizing documents...\")\nt_enc = time()\nif USE_BPE\n    train_tokens = [tokenize_doc(doc, nothing, BOS) for doc in train_docs]\n    val_tokens = [tokenize_doc(doc, nothing, BOS) for doc in val_docs]\nelse\n    train_tokens = [tokenize_doc(doc, char_to_id, BOS) for doc in train_docs]\n    val_tokens = [tokenize_doc(doc, char_to_id, BOS) for doc in val_docs]\nend\nprintln(\"Tokenization done in $(round(time() - t_enc, digits=1))s\")\nprintln(\"Pre-tokenized: $(length(train_tokens)) train, $(length(val_tokens)) val docs\")\n\ntotal_train_tokens = sum(length, train_tokens)\navg_doc_len = round(total_train_tokens / length(train_tokens), digits=1)\nprintln(\"Total train tokens: $total_train_tokens (avg doc: $avg_doc_len tokens)\")\n\n# ── Hyperparameters (n_layer, n_embd, n_head, head_dim set in cell 8) ──\nhyperparams = Dict{String,Any}(\n    \"n_layer\" => n_layer, \"n_embd\" => n_embd,\n    \"block_size\" => block_size, \"n_head\" => n_head,\n    \"use_bpe\" => USE_BPE, \"compute_tier\" => string(COMPUTE_TIER)\n)\n\n# ── Initialize parameters as Param(Matrix{Float32}) ──\nstate_dict = Dict{String, Any}()\nstate_dict[\"wte\"]     = init_param(vocab_size, n_embd)\nstate_dict[\"wpe\"]     = init_param(block_size, n_embd)\nstate_dict[\"lm_head\"] = init_param(vocab_size, n_embd)\nfor i in 0:n_layer-1\n    state_dict[\"layer$i.attn_wq\"]  = init_param(n_embd, n_embd)\n    state_dict[\"layer$i.attn_wk\"]  = init_param(n_embd, n_embd)\n    state_dict[\"layer$i.attn_wv\"]  = init_param(n_embd, n_embd)\n    state_dict[\"layer$i.attn_wo\"]  = init_param(n_embd, n_embd)\n    state_dict[\"layer$i.mlp_fc1\"]  = init_param(4 * n_embd, n_embd)\n    state_dict[\"layer$i.mlp_fc2\"]  = init_param(n_embd, 4 * n_embd)\nend\n\nparam_keys = get_param_keys(n_layer)\nall_params = collect_params(state_dict, param_keys)\ntotal_num_params = sum(length(value(p)) for p in all_params)\nprintln(\"\\nModel: $total_num_params params ($(round(total_num_params/1e3, digits=1))K) — tier: $COMPUTE_TIER\")\n\nparams = build_params(state_dict, n_layer)\n\n# ── Dynamic training schedule ──\nsteps_per_epoch = length(train_tokens)\nnum_steps = clamp(target_epochs * steps_per_epoch, 500, 100000)\nwarmup_iters = max(20, round(Int, warmup_frac * num_steps))\neval_interval = max(20, num_steps ÷ 20)\n\nprintln(\"\\n── Training schedule (computed from data) ──\")\nprintln(\"  steps_per_epoch: $steps_per_epoch\")\nprintln(\"  num_steps:       $num_steps  ($target_epochs epochs)\")\nprintln(\"  eval_interval:   $eval_interval\")\nprintln(\"  warmup_iters:    $warmup_iters\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## 7. Training Loop with Validation + Best-Model Checkpointing\n\nAdam optimizer with linear LR decay.\nValidates every 50 steps, saves `best_model.json` when val loss improves.\nCheckpoints sync to HuggingFace Hub automatically.\nPeriodic checkpoints every 200 steps."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "function compute_val_loss(val_tokens, params, BOS, block_size, n_layer, n_head, head_dim, n_embd)\n    total_loss = 0.0\n    total_tokens = 0\n    kv_key_mats, kv_val_mats, kv_lens = alloc_kv_cache(n_layer, n_embd, block_size)\n    for tokens in val_tokens\n        n = min(block_size, length(tokens) - 1)\n        reset_kv_cache!(kv_lens)\n        for pos in 1:n\n            token_id = tokens[pos]\n            target_id = tokens[pos + 1]\n            # Outside @diff, Params behave like plain arrays\n            logits = gpt(token_id, pos, kv_key_mats, kv_val_mats, kv_lens, params, n_layer, n_head, head_dim)\n            probs = softmax_ag(logits)\n            p_val = Float64.(value(probs))\n            total_loss += -log(max(p_val[target_id], 1e-10))\n            total_tokens += 1\n        end\n    end\n    return total_loss / max(total_tokens, 1)\nend\n\nprintln(\"compute_val_loss defined (optimized: pre-allocated KV cache, typed params)\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ── Adam optimizer state (per-parameter matrices) ──\nadam_m = Dict{String, Matrix{Float32}}()\nadam_v = Dict{String, Matrix{Float32}}()\nfor k in param_keys\n    sz = size(value(state_dict[k]))\n    adam_m[k] = zeros(Float32, sz)\n    adam_v[k] = zeros(Float32, sz)\nend\n\nbest_val_loss = Inf\ntrain_loss_history = Float64[]\nval_loss_history = Float64[]\n\nLOCAL_CKPT = \"checkpoints\"\nmkpath(LOCAL_CKPT)\n\nfunction save_and_sync(path, sd, pk, uc, hp; kwargs...)\n    save_checkpoint(path, sd, pk, uc, hp; kwargs...)\n    hf_sync(path)\nend\n\n# ── Cosine LR with warmup ──\nfunction get_lr(step, max_steps, base_lr, min_lr, warmup_steps)\n    if step < warmup_steps\n        return base_lr * step / warmup_steps\n    end\n    progress = (step - warmup_steps) / max(1, max_steps - warmup_steps)\n    return min_lr + 0.5 * (base_lr - min_lr) * (1.0 + cos(Float64(pi) * progress))\nend\n\n# ── Training loop with gradient clipping + cosine LR ──\nfunction train_loop!(state_dict, params, param_keys, train_tokens, val_tokens,\n                     adam_m, adam_v, uchars, hyperparams;\n                     num_steps::Int, lr::Float64, b1::Float64, b2::Float64, eps::Float64,\n                     n_layer::Int, n_head::Int, head_dim::Int, n_embd::Int,\n                     block_size::Int, BOS::Int,\n                     max_grad_norm::Float64=1.0,\n                     min_lr::Float64=1e-5,\n                     warmup_iters::Int=50,\n                     best_val_loss::Float64=Inf,\n                     train_loss_history::Vector{Float64}=Float64[],\n                     val_loss_history::Vector{Float64}=Float64[],\n                     start_step::Int=1)\n\n    kv_key_mats, kv_val_mats, kv_lens = alloc_kv_cache(n_layer, n_embd, block_size)\n\n    hf_status = !isempty(get(ENV, \"HF_REPO\", \"\")) ? \"HF:$(ENV[\"HF_REPO\"])\" : ((@isdefined(HF_REPO_ID) && !isempty(HF_REPO_ID)) ? \"HF:$HF_REPO_ID\" : \"HF:(not configured)\")\n    println(\"--- training $num_steps steps (steps $start_step..$(start_step + num_steps - 1)) ---\")\n    println(\"    LR: cosine $lr → $min_lr, warmup: $warmup_iters, grad_clip: $max_grad_norm\")\n    println(\"    Local: checkpoints/  |  $hf_status\")\n    t_start = time()\n    last_save_time = time()\n    SAVE_INTERVAL = 600\n    completed_steps = start_step - 1\n\n    try\n        for step in start_step:(start_step + num_steps - 1)\n            completed_steps = step\n            tokens = train_tokens[mod1(step, length(train_tokens))]\n            n = min(block_size, length(tokens) - 1)\n\n            reset_kv_cache!(kv_lens)\n\n            tape = @diff begin\n                loss_sum = Float32(0)\n                for pos in 1:n\n                    token_id  = tokens[pos]\n                    target_id = tokens[pos + 1]\n                    logits = gpt(token_id, pos, kv_key_mats, kv_val_mats, kv_lens, params, n_layer, n_head, head_dim)\n                    probs = softmax_ag(logits)\n                    loss_sum = loss_sum + (-log(probs[target_id]))\n                end\n                loss_sum / Float32(n)\n            end\n\n            avg_loss = Float64(value(tape))\n            push!(train_loss_history, avg_loss)\n\n            # Extract all gradients first\n            grads = Dict{String, Matrix{Float32}}()\n            for k in param_keys\n                g = grad(tape, state_dict[k])\n                if g !== nothing\n                    grads[k] = to_dense_grad(g)\n                end\n            end\n\n            # Gradient clipping (global norm)\n            grad_norm_sq = 0.0\n            for (_, g) in grads\n                grad_norm_sq += sum(g .^ 2)\n            end\n            grad_norm = sqrt(grad_norm_sq)\n            clip_scale = grad_norm > max_grad_norm ? Float32(max_grad_norm / grad_norm) : Float32(1.0)\n\n            # Adam update with cosine LR\n            lr_t = get_lr(step, start_step + num_steps - 1, lr, min_lr, warmup_iters)\n            for k in param_keys\n                haskey(grads, k) || continue\n                g_dense = grads[k] .* clip_scale\n                adam_m[k] .= Float32(b1) .* adam_m[k] .+ Float32(1 - b1) .* g_dense\n                adam_v[k] .= Float32(b2) .* adam_v[k] .+ Float32(1 - b2) .* g_dense .^ 2\n                m_hat = adam_m[k] ./ Float32(1 - b1^step)\n                v_hat = adam_v[k] ./ Float32(1 - b2^step)\n                value(state_dict[k]) .-= Float32(lr_t) .* m_hat ./ (sqrt.(v_hat) .+ Float32(eps))\n            end\n\n            # Validate + checkpoint\n            if step % eval_interval == 0 || step == start_step\n                val_loss = compute_val_loss(val_tokens, params, BOS, block_size, n_layer, n_head, head_dim, n_embd)\n                push!(val_loss_history, val_loss)\n                elapsed = time() - t_start\n                wandb_log(; step=step, train_loss=avg_loss, val_loss=val_loss, lr=lr_t)\n\n                improved = \"\"\n                if val_loss < best_val_loss\n                    best_val_loss = val_loss\n                    save_and_sync(\"checkpoints/best_model.json\", state_dict, param_keys, uchars, hyperparams;\n                        adam_m=adam_m, adam_v=adam_v, step=step,\n                        lr=lr, b1=b1, b2=b2,\n                        best_val_loss=best_val_loss,\n                        train_losses=train_loss_history, val_losses=val_loss_history,\n                        total_steps=step, num_steps_target=start_step + num_steps - 1)\n                    improved = \" << best!\"\n                end\n\n                @printf(\"step %5d / %5d | train %.4f | val %.4f | lr %.2e | gnorm %.1f | %.1fs%s\\n\",\n                        step, start_step + num_steps - 1, avg_loss, val_loss, lr_t, grad_norm, elapsed, improved)\n            elseif step % max(1, eval_interval ÷ 5) == 0\n                elapsed = time() - t_start\n                @printf(\"step %5d / %5d | train %.4f | lr %.2e | %.1fs\\n\",\n                        step, start_step + num_steps - 1, avg_loss, lr_t, elapsed)\n            end\n\n            if step % max(100, eval_interval * 2) == 0\n                save_and_sync(\"checkpoints/checkpoint_latest.json\", state_dict, param_keys, uchars, hyperparams;\n                    adam_m=adam_m, adam_v=adam_v, step=step,\n                    lr=lr, b1=b1, b2=b2,\n                    best_val_loss=best_val_loss,\n                    train_losses=train_loss_history, val_losses=val_loss_history,\n                    total_steps=step, num_steps_target=start_step + num_steps - 1)\n                last_save_time = time()\n            end\n\n            if time() - last_save_time > SAVE_INTERVAL\n                save_and_sync(\"checkpoints/checkpoint_latest.json\", state_dict, param_keys, uchars, hyperparams;\n                    adam_m=adam_m, adam_v=adam_v, step=step,\n                    lr=lr, b1=b1, b2=b2,\n                    best_val_loss=best_val_loss,\n                    train_losses=train_loss_history, val_losses=val_loss_history,\n                    total_steps=step, num_steps_target=start_step + num_steps - 1)\n                last_save_time = time()\n                println(\"  [auto-save at step $step]\")\n            end\n\n            if step % GC_INTERVAL == 0\n                GC.gc(false)\n            end\n        end\n    catch e\n        if e isa InterruptException\n            println(\"\\n\\nTraining interrupted at step $completed_steps!\")\n        else\n            println(\"\\n\\nTraining error at step $completed_steps: $e\")\n        end\n        save_and_sync(\"checkpoints/checkpoint_interrupted.json\", state_dict, param_keys, uchars, hyperparams;\n            adam_m=adam_m, adam_v=adam_v, step=completed_steps,\n            lr=lr, b1=b1, b2=b2,\n            best_val_loss=best_val_loss,\n            train_losses=train_loss_history, val_losses=val_loss_history,\n            total_steps=completed_steps, num_steps_target=start_step + num_steps - 1)\n        if !(e isa InterruptException)\n            rethrow(e)\n        end\n    end\n\n    elapsed = time() - t_start\n    @printf(\"\\ntraining complete in %.1f seconds\\n\", elapsed)\n    return best_val_loss, train_loss_history, val_loss_history, completed_steps\nend\n\n# ── Run training ──\nbest_val_loss, train_loss_history, val_loss_history, final_step = train_loop!(\n    state_dict, params, param_keys, train_tokens, val_tokens,\n    adam_m, adam_v, uchars, hyperparams;\n    num_steps=num_steps, lr=lr, b1=b1, b2=b2, eps=eps,\n    n_layer=n_layer, n_head=n_head, head_dim=head_dim, n_embd=n_embd,\n    block_size=block_size, BOS=BOS,\n    max_grad_norm=max_grad_norm, min_lr=min_lr, warmup_iters=warmup_iters,\n    best_val_loss=best_val_loss,\n    train_loss_history=train_loss_history,\n    val_loss_history=val_loss_history)\n\nwandb_finish()\n\nsave_and_sync(\"checkpoints/final_model.json\", state_dict, param_keys, uchars, hyperparams;\n    adam_m=adam_m, adam_v=adam_v, step=final_step,\n    lr=lr, b1=b1, b2=b2,\n    best_val_loss=best_val_loss,\n    train_losses=train_loss_history, val_losses=val_loss_history,\n    total_steps=final_step, num_steps_target=num_steps)\n\nprintln(\"\\nBest val loss: $(@sprintf(\"%.4f\", best_val_loss))\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Inference — Hallucinated Philosophy\n",
    "\n",
    "Generate new philosophy-like text using temperature-controlled sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "function generate_text(state_dict, n_layer, n_head, head_dim, n_embd, block_size, vocab_size, BOS;\n                       max_tokens=200, temperature=0.8)\n    params = build_params(state_dict, n_layer)\n    kv_key_mats, kv_val_mats, kv_lens = alloc_kv_cache(n_layer, n_embd, block_size)\n    reset_kv_cache!(kv_lens)\n\n    token_id = BOS\n    generated_ids = Int[]\n\n    for pos in 1:max_tokens\n        logits = gpt(token_id, pos, kv_key_mats, kv_val_mats, kv_lens, params, n_layer, n_head, head_dim)\n        logits_val = Float64.(value(logits))\n\n        # Temperature-scaled sampling\n        logits_val ./= temperature\n        logits_val .-= maximum(logits_val)\n        probs = exp.(logits_val)\n        probs ./= sum(probs)\n\n        # Sample from distribution\n        r = rand()\n        cum = 0.0\n        next_id = 1\n        for (i, p) in enumerate(probs)\n            cum += p\n            if r <= cum\n                next_id = i\n                break\n            end\n        end\n\n        next_id == BOS && break  # stop at BOS\n        push!(generated_ids, next_id)\n        token_id = next_id\n    end\n\n    return decode_ids(generated_ids)\nend\n\nprintln(\"--- Generated Philosophy ---\")\nfor i in 1:5\n    text = generate_text(state_dict, n_layer, n_head, head_dim, n_embd, block_size, vocab_size, BOS;\n                         max_tokens=300, temperature=0.8)\n    @printf(\"\\nSample %d:\\n%s\\n\", i, text[1:min(end, 500)])\n    println(\"---\")\nend"
  },
  {
   "cell_type": "markdown",
   "source": "---\n## 8a. Push Model to HuggingFace Hub\n\nPush your trained checkpoint to HuggingFace for persistence across Colab sessions.  \nSet `HF_REPO_ID` in the login cell above (e.g. `\"yourusername/microgpt-philosophy\"`).",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Push checkpoint to HuggingFace Hub\n# Make sure HF_REPO_ID is set in the login cell above\n\nif @isdefined(HF_REPO_ID) && !isempty(HF_REPO_ID)\n    # Create repo if it doesn't exist yet\n    hf_create_repo(HF_REPO_ID)\n\n    # Push best model checkpoint\n    if isfile(\"checkpoints/best_model.json\")\n        hf_push_checkpoint(HF_REPO_ID; checkpoint_path=\"checkpoints/best_model.json\")\n    else\n        println(\"No best_model.json found — train first!\")\n    end\n\n    # Also push final model if it exists\n    if isfile(\"checkpoints/final_model.json\")\n        hf_push(HF_REPO_ID, \"checkpoints/final_model.json\")\n    end\n\n    println(\"\\nDone! View your model at: https://huggingface.co/$HF_REPO_ID\")\nelse\n    println(\"Set HF_REPO_ID in the login cell (e.g. \\\"yourusername/microgpt-philosophy\\\")\")\nend",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n## 8b. Pull Checkpoint from HuggingFace Hub\n\nDownload a previously pushed checkpoint to resume training in a new Colab session.  \nRun this before the Resume Training cell below.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Pull checkpoint from HuggingFace to resume training\n# Make sure HF_REPO_ID is set in the login cell above\n\nif @isdefined(HF_REPO_ID) && !isempty(HF_REPO_ID)\n    mkpath(\"checkpoints\")\n    hf_pull(HF_REPO_ID, \"best_model.json\"; local_dir=\"checkpoints\")\n    println(\"\\nReady to resume from checkpoints/best_model.json\")\n    println(\"Run the 'Resume Training' cell below.\")\nelse\n    println(\"Set HF_REPO_ID in the login cell (e.g. \\\"yourusername/microgpt-philosophy\\\")\")\nend",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Resume Training from Checkpoint\n",
    "\n",
    "Load a saved checkpoint and continue training for more steps.  \n",
    "Skip this cell if you're training from scratch above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ── Load checkpoint ──\n# Change the path to load a different checkpoint\nRESUME_FROM = \"checkpoints/best_model.json\"\nEXTRA_STEPS = clamp(length(train_docs), 500, 25000)  # ~1 extra epoch\n\nif !isfile(RESUME_FROM)\n    # Try pulling from HuggingFace\n    if @isdefined(HF_REPO_ID) && !isempty(HF_REPO_ID)\n        println(\"Checkpoint not found locally, pulling from HuggingFace...\")\n        hf_pull(HF_REPO_ID, basename(RESUME_FROM); local_dir=\"checkpoints\")\n    end\n    isfile(RESUME_FROM) || error(\"Checkpoint not found: $RESUME_FROM\")\nend\n\nckpt = load_checkpoint(RESUME_FROM)\nstate_dict = ckpt.state_dict\nuchars = ckpt.uchars\nBOS = ckpt.BOS\nn_layer = ckpt.n_layer\nn_embd = ckpt.n_embd\nblock_size = ckpt.block_size\nn_head = ckpt.n_head\nhead_dim = ckpt.head_dim\n\nhyperparams = Dict{String,Any}(\n    \"n_layer\" => n_layer, \"n_embd\" => n_embd,\n    \"block_size\" => block_size, \"n_head\" => n_head\n)\n\n# Reconstruct dataset and split (ordered, not shuffled)\ndocs = copy(TRAINING_DATA)\nsplit_idx = max(1, Int(floor(0.9 * length(docs))))\ntrain_docs = docs[1:split_idx]\nval_docs = docs[split_idx+1:end]\nif isempty(val_docs)\n    val_docs = docs[max(1, end-4):end]\n    train_docs = docs[1:max(1, end-5)]\nend\n\n# Rebuild O(1) tokenizer and pre-tokenize\nchar_to_id = Dict{Char, Int}(ch => i for (i, ch) in enumerate(uchars))\ntrain_tokens = [tokenize_doc(doc, char_to_id, BOS) for doc in train_docs]\nval_tokens = [tokenize_doc(doc, char_to_id, BOS) for doc in val_docs]\n\nparam_keys = get_param_keys(n_layer)\n\n# Build type-stable params tuple\nparams = build_params(state_dict, n_layer)\n\n# Restore optimizer (Adam state per param key)\nlr = ckpt.lr; b1 = ckpt.b1; b2 = ckpt.b2; eps = 1e-8\nadam_m = if !isempty(ckpt.adam_m)\n    ckpt.adam_m\nelse\n    Dict{String, Matrix{Float32}}(k => zeros(Float32, size(value(state_dict[k]))) for k in param_keys)\nend\nadam_v = if !isempty(ckpt.adam_v)\n    ckpt.adam_v\nelse\n    Dict{String, Matrix{Float32}}(k => zeros(Float32, size(value(state_dict[k]))) for k in param_keys)\nend\n\nstart_step = ckpt.step + 1\nbest_val_loss = ckpt.best_val_loss\ntrain_loss_history = copy(ckpt.train_losses)\nval_loss_history = copy(ckpt.val_losses)\n\n# ── Initialize W&B logging (if API key is set) ──\nif haskey(ENV, \"WANDB_API_KEY\") && !isempty(ENV[\"WANDB_API_KEY\"])\n    wandb_init()\nend\n\nprintln(\"\\nResuming from step $(ckpt.step) -> training $EXTRA_STEPS more steps\")\nprintln(\"Best val loss so far: $(round(best_val_loss, digits=4))\")\n\nbest_val_loss, train_loss_history, val_loss_history, final_step = train_loop!(\n    state_dict, params, param_keys, train_tokens, val_tokens,\n    adam_m, adam_v, uchars, hyperparams;\n    num_steps=EXTRA_STEPS, lr=lr, b1=b1, b2=b2, eps=Float64(eps),\n    n_layer=n_layer, n_head=n_head, head_dim=head_dim, n_embd=n_embd,\n    block_size=block_size, BOS=BOS,\n    best_val_loss=best_val_loss,\n    train_loss_history=train_loss_history,\n    val_loss_history=val_loss_history,\n    start_step=start_step)\n\nwandb_finish()\n\nsave_and_sync(\"checkpoints/final_model.json\", state_dict, param_keys, uchars, hyperparams;\n    adam_m=adam_m, adam_v=adam_v, step=final_step,\n    lr=lr, b1=b1, b2=b2,\n    best_val_loss=best_val_loss,\n    train_losses=train_loss_history, val_losses=val_loss_history,\n    total_steps=final_step, num_steps_target=start_step + EXTRA_STEPS - 1)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## 10. Download Checkpoint\n\nDownload the best model checkpoint to use with the inference server.\nIn Colab, use the Files panel (left sidebar) to download, or pull from HuggingFace Hub."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List saved checkpoints\n",
    "if isdir(\"checkpoints\")\n",
    "    files = readdir(\"checkpoints\")\n",
    "    println(\"Saved checkpoints:\")\n",
    "    for f in files\n",
    "        path = joinpath(\"checkpoints\", f)\n",
    "        size_kb = round(filesize(path) / 1024, digits=1)\n",
    "        println(\"  $path ($(size_kb) KB)\")\n",
    "    end\n",
    "else\n",
    "    println(\"No checkpoints directory found. Train first!\")\n",
    "end"
   ]
  }
 ]
}