{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "name": "JuliaGPT - Optimized GPT in Pure Julia",
   "include_colab_link": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": "<a href=\"https://colab.research.google.com/github/DavinciDreams/JuliaGPT/blob/main/juliagpt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# JuliaGPT \u2014 A Minimal GPT in Pure Julia\n\nFaithful port of Karpathy's JuliaGPT: the most atomic way to train a GPT.  \nArray-based autograd via AutoGrad.jl, transformer, Adam optimizer.  \nNo external dependencies beyond Julia stdlib + AutoGrad.jl.\n\n**Architecture** (following GPT-2 with simplifications):\n- AutoGrad.jl array-based automatic differentiation (`Param` wrapped matrices)\n- Single-layer transformer with multi-head attention\n- RMSNorm (not LayerNorm), no biases, ReLU (not GELU)\n- KV cache for natural causal masking\n- Adam optimizer with linear LR decay\n- Temperature-controlled generation\n- Best-model checkpointing with validation loss tracking (local + HuggingFace Hub)\n\nBased on: https://gist.github.com/karpathy/8627fe009c40f57531cb18360106ce95"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 0. Login & Setup\n\nThis cell runs in **Python** to read your Colab secrets and save them for Julia.\n\n1. Add secrets via the key icon in the left sidebar:\n   - `HF_TOKEN` \u2014 your HuggingFace access token\n   - `WANDB_KEY` \u2014 your Weights & Biases API key\n   - `HF_REPO` \u2014 your model repo (e.g. `LisaMegaWatts/JuliaGPT`)\n2. Run cells 0-1 (login + install Julia, ~3-5 min)\n3. **Runtime > Change runtime type > Julia 1.10**\n4. Continue with the remaining Julia cells"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# \u2500\u2500 Minimal Python setup: install tools + save Colab secrets for Julia \u2500\u2500\n!pip install -q wandb huggingface_hub\n\nimport json, pathlib\n\nsecrets = {}\ntry:\n    from google.colab import userdata\n    for key in (\"HF_TOKEN\", \"WANDB_KEY\", \"HF_REPO\"):\n        try: secrets[key] = userdata.get(key)\n        except Exception: pass\nexcept ImportError:\n    pass\n\nsecrets_path = pathlib.Path.home() / \".julia_secrets.json\"\nsecrets_path.write_text(json.dumps(secrets))\nsecrets_path.chmod(0o600)\n\nfound = [k for k in secrets if secrets[k]]\nprint(f\"Secrets saved: {', '.join(found) if found else 'none found'}\")\nprint(f\"  -> {secrets_path}\")\nif not found:\n    print(\"Add HF_TOKEN, WANDB_KEY, HF_REPO via the key icon in the sidebar.\")\n\nprint(\"\\nDone! Now run the next cell to install Julia (~3-5 min).\")\nprint(\"Then: Runtime > Change runtime type > Julia 1.10\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1. Install Julia Kernel\n\nThis cell downloads and installs Julia + IJulia. **Takes ~3-5 minutes** on first run.\n\n**After it finishes:**\n1. Go to **Runtime \u2192 Change runtime type**\n2. You may see both \"Julia\" and \"Julia 1.10\" \u2014 pick **Julia 1.10**\n3. Continue running the cells below"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "%%shell\nset -e\n\nJULIA_VERSION=\"1.10.5\"\nJULIA_MINOR=\"1.10\"\n\nif [ ! -d \"/usr/local/julia-${JULIA_VERSION}\" ]; then\n    echo \"Downloading Julia ${JULIA_VERSION}...\"\n    wget -q https://julialang-s3.julialang.org/bin/linux/x64/${JULIA_MINOR}/julia-${JULIA_VERSION}-linux-x86_64.tar.gz\n    tar xzf julia-${JULIA_VERSION}-linux-x86_64.tar.gz -C /usr/local/\n    rm julia-${JULIA_VERSION}-linux-x86_64.tar.gz\n    ln -sf /usr/local/julia-${JULIA_VERSION}/bin/julia /usr/local/bin/julia\n    echo \"Julia installed.\"\nelse\n    echo \"Julia already installed.\"\nfi\n\njulia -e '\n    using Pkg\n    Pkg.add(\"IJulia\")\n    Pkg.add(\"JSON3\")\n    Pkg.add(\"AutoGrad\")\n    using IJulia\n    installkernel(\"Julia\")\n'\n\necho \"\"\necho \"===========================================================\"\necho \"  Julia kernel installed!                                   \"\necho \"  Now: Runtime -> Change runtime type -> pick Julia 1.10       \"\necho \"  Then run the cells below.                              \"\necho \"===========================================================\"",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## 1b. Load Credentials + W&B / HuggingFace Helpers (Julia)\n\nReads tokens from `~/.julia_secrets.json` (written by the Python setup cell).\nW&B logging uses a persistent Python subprocess fed JSON lines from Julia.\nHuggingFace helpers use `huggingface-cli` to push/pull checkpoints.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "using JSON3\n\n# \u2500\u2500 Ensure pip-installed binaries (huggingface-cli, wandb) are on PATH \u2500\u2500\nfor p in [\"/usr/local/bin\", joinpath(homedir(), \".local/bin\"), \"/root/.local/bin\"]\n    if isdir(p) && !occursin(p, get(ENV, \"PATH\", \"\"))\n        ENV[\"PATH\"] = p * \":\" * get(ENV, \"PATH\", \"\")\n    end\nend\n\n# \u2500\u2500 Read credentials from ~/.julia_secrets.json (written by Python setup cell) \u2500\u2500\nfunction load_secrets()\n    path = expanduser(\"~/.julia_secrets.json\")\n    if !isfile(path)\n        @warn \"No secrets file found at $path \u2014 run the Python setup cell first\"\n        return Dict{String,String}()\n    end\n    raw = JSON3.read(read(path, String))\n    return Dict{String,String}(string(k) => string(v) for (k, v) in pairs(raw) if !isempty(string(v)))\nend\n\nsecrets = load_secrets()\n\n# W&B\nif haskey(secrets, \"WANDB_KEY\")\n    ENV[\"WANDB_API_KEY\"] = secrets[\"WANDB_KEY\"]\n    println(\"W&B API key: found\")\nelse\n    println(\"W&B API key: not found (add WANDB_KEY to Colab secrets)\")\nend\n\n# HuggingFace token\nif haskey(secrets, \"HF_TOKEN\")\n    ENV[\"HF_TOKEN\"] = secrets[\"HF_TOKEN\"]\n    println(\"HF token: found\")\nelse\n    println(\"HF token: not found (add HF_TOKEN to Colab secrets)\")\nend\n\n# HuggingFace repo ID\nHF_REPO_ID = get(secrets, \"HF_REPO\", \"\")\nif !isempty(HF_REPO_ID)\n    println(\"HF repo: \", HF_REPO_ID)\nelse\n    println(\"HF repo: not set (add HF_REPO to Colab secrets or set HF_REPO_ID manually)\")\nend",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n# W&B logging via persistent Python subprocess\n# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n\nWANDB_PROJECT = \"microgpt-philosophy\"\nWANDB_RUN_ID = \"microgpt-\" * join(rand('a':'z', 6))\n\n# Write a tiny Python helper that reads JSON lines on stdin\nwrite(\"_wandb_log.py\", \"\"\"\nimport wandb, json, sys, os\nproject = os.environ.get(\"WANDB_PROJECT\", \"microgpt-philosophy\")\nrun_id = os.environ.get(\"WANDB_RUN_ID\", None)\nrun = wandb.init(project=project, id=run_id, resume=\"allow\",\n                 config={\"model\": \"microgpt\", \"architecture\": \"1-layer transformer\"})\nprint(f\"W&B run: {run.url}\", flush=True)\nfor line in sys.stdin:\n    line = line.strip()\n    if not line:\n        continue\n    try:\n        data = json.loads(line)\n        wandb.log(data)\n    except Exception as e:\n        print(f\"wandb log error: {e}\", file=sys.stderr, flush=True)\nwandb.finish()\n\"\"\")\n\nwandb_proc = nothing\n\nfunction wandb_init()\n    global wandb_proc, WANDB_PROJECT, WANDB_RUN_ID\n    if !haskey(ENV, \"WANDB_API_KEY\") || isempty(ENV[\"WANDB_API_KEY\"])\n        println(\"W&B: skipped (no API key)\")\n        return\n    end\n    ENV[\"WANDB_PROJECT\"] = WANDB_PROJECT\n    ENV[\"WANDB_RUN_ID\"] = WANDB_RUN_ID\n    wandb_proc = open(`python3 _wandb_log.py`, \"r+\")\n    println(\"W&B: initialized ($WANDB_PROJECT / $WANDB_RUN_ID)\")\nend\n\nfunction wandb_log(; kwargs...)\n    global wandb_proc\n    wandb_proc === nothing && return\n    metrics = Dict(string(k) => v for (k, v) in kwargs)\n    try\n        println(wandb_proc, JSON3.write(metrics))\n        flush(wandb_proc)\n    catch e\n        println(\"W&B log error: $e\")\n    end\nend\n\nfunction wandb_finish()\n    global wandb_proc\n    wandb_proc === nothing && return\n    try close(wandb_proc) catch end\n    wandb_proc = nothing\n    println(\"W&B: run finished\")\nend\n\n# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n# HuggingFace Hub helpers\n# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n\nfunction hf_push(repo_id::String, local_path::String; remote_path::String=\"\")\n    rp = isempty(remote_path) ? basename(local_path) : remote_path\n    run(`huggingface-cli upload $repo_id $local_path $rp`)\n    println(\"Pushed $local_path -> $repo_id/$rp\")\nend\n\nfunction hf_pull(repo_id::String, remote_path::String; local_dir::String=\"checkpoints\")\n    mkpath(local_dir)\n    run(`huggingface-cli download $repo_id $remote_path --local-dir $local_dir`)\n    println(\"Pulled $repo_id/$remote_path -> $local_dir/\")\nend\n\nfunction hf_push_checkpoint(repo_id::String; checkpoint_path::String=\"checkpoints/best_model.json\")\n    isfile(checkpoint_path) || error(\"Checkpoint not found: $checkpoint_path\")\n    hf_push(repo_id, checkpoint_path)\nend\n\nfunction hf_create_repo(repo_id::String)\n    try\n        run(`huggingface-cli repo create $repo_id --type model`)\n        println(\"Created HF repo: $repo_id\")\n    catch\n        println(\"HF repo already exists or creation skipped: $repo_id\")\n    end\nend\n\n# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n# Sync helper: push checkpoint to HuggingFace Hub\n# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n\nfunction hf_sync(local_path::String)\n    if !@isdefined(HF_REPO_ID) || isempty(HF_REPO_ID)\n        return\n    end\n    try\n        hf_push(HF_REPO_ID, local_path)\n    catch e\n        println(\"  HF sync failed: $e\")\n    end\nend\n\nprintln(\"W&B + HuggingFace helpers defined\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## 1. AutoGrad.jl Setup\n\nArray-based automatic differentiation using AutoGrad.jl.  \nParameters are wrapped with `Param()` for gradient tracking; `@diff` builds the computation tape and `grad()` extracts gradients."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "using Pkg\nPkg.add(\"JSON3\")\nPkg.add(\"AutoGrad\")\n\nusing Random\nusing Printf\nusing JSON3\nusing AutoGrad\nusing LinearAlgebra\n\nRandom.seed!(42)\n\nprintln(\"AutoGrad.jl loaded - array-based automatic differentiation\")"
  },
  {
   "cell_type": "code",
   "source": "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n# Resource Sensing & Auto-Configuration\n# Inspired by Unsloth's resource-aware training patterns:\n#   - Sense available hardware (CPU cores, RAM, GPU VRAM)\n#   - Limit usage to RESOURCE_LIMIT (default 50%)\n#   - Configure BLAS threads, GC hints, and training params\n# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n\nRESOURCE_LIMIT = 0.50  # Use at most 50% of available resources\n\n# \u2500\u2500 CPU Detection \u2500\u2500\ntotal_threads = Sys.CPU_THREADS\ntotal_cores = div(total_threads, 2)  # assume hyperthreading\nmax_blas_threads = max(1, Int(floor(total_cores * RESOURCE_LIMIT)))\n\n# Set BLAS thread count (OpenBLAS)\nusing LinearAlgebra\nBLAS.set_num_threads(max_blas_threads)\n\nprintln(\"=== Resource Configuration ($(Int(RESOURCE_LIMIT * 100))% limit) ===\")\nprintln(\"CPU: $(total_cores) cores / $(total_threads) threads\")\nprintln(\"  BLAS threads: $max_blas_threads ($(Int(RESOURCE_LIMIT * 100))% of $total_cores cores)\")\n\n# \u2500\u2500 Memory Detection \u2500\u2500\ntotal_ram_gb = round(Sys.total_memory() / 1024^3, digits=1)\nfree_ram_gb = round(Sys.free_memory() / 1024^3, digits=1)\nram_limit_gb = round(free_ram_gb * RESOURCE_LIMIT, digits=1)\nprintln(\"RAM: $(total_ram_gb) GB total, $(free_ram_gb) GB free\")\nprintln(\"  Training limit: $(ram_limit_gb) GB\")\n\n# \u2500\u2500 GPU Detection \u2500\u2500\nHAS_GPU = false\nGPU_NAME = \"\"\nGPU_VRAM_GB = 0.0\nGPU_VRAM_FREE_GB = 0.0\nGPU_VRAM_LIMIT_GB = 0.0\n\ntry\n    using CUDA\n    if CUDA.functional()\n        HAS_GPU = true\n        dev = CUDA.device()\n        GPU_NAME = CUDA.name(dev)\n        GPU_VRAM_GB = round(CUDA.total_memory() / 1024^3, digits=2)\n        GPU_VRAM_FREE_GB = round(CUDA.available_memory() / 1024^3, digits=2)\n        GPU_VRAM_LIMIT_GB = round(GPU_VRAM_FREE_GB * RESOURCE_LIMIT, digits=2)\n        # Set CUDA memory pool limit\n        CUDA.pool.max_memory!(Int(floor(GPU_VRAM_LIMIT_GB * 1024^3)))\n        println(\"GPU: $GPU_NAME ($(GPU_VRAM_GB) GB total, $(GPU_VRAM_FREE_GB) GB free)\")\n        println(\"  VRAM limit: $(GPU_VRAM_LIMIT_GB) GB\")\n    else\n        println(\"GPU: CUDA available but not functional\")\n    end\ncatch e\n    println(\"GPU: not available ($e)\")\nend\n\n# \u2500\u2500 Compute Device Selection \u2500\u2500\n# For tiny models (<100K params), CPU is faster than GPU due to transfer overhead.\n# GPU becomes beneficial for larger models or batched operations.\nUSE_GPU = false  # Default to CPU for this tiny model\nGPU_PARAM_THRESHOLD = 100_000  # Switch to GPU above this param count\n\nprintln(\"\\nDevice: CPU (model too small for GPU benefit; threshold=$(GPU_PARAM_THRESHOLD) params)\")\nprintln(\"  Override: set USE_GPU = true to force GPU\")\n\n# \u2500\u2500 GC Configuration \u2500\u2500\n# Periodic GC between training steps to prevent memory accumulation\n# (Unsloth pattern: controlled memory cleanup between batches)\nGC_INTERVAL = 100  # Run GC.gc(false) every N steps\n\n# \u2500\u2500 Sequence Packing Configuration (Unsloth-inspired) \u2500\u2500\n# Pack multiple short documents into block_size-length sequences\n# to eliminate wasted computation on short sequences\nENABLE_PACKING = true  # Unsloth's padding-free packing pattern\n\nprintln(\"\\nOptimizations:\")\nprintln(\"  Sequence packing: $(ENABLE_PACKING ? \"enabled\" : \"disabled\") (Unsloth-inspired)\")\nprintln(\"  GC interval: every $GC_INTERVAL steps\")\nprintln(\"===========================================\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n## 1c. Resource Sensing & Configuration\n\nAuto-detects CPU, RAM, and GPU resources and configures training to use at most 50% of available capacity.\nInspired by Unsloth's resource-aware training patterns: sense hardware, set optimal BLAS threads, and configure memory limits automatically.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2500\u2500 Array-based neural network primitives (AutoGrad-compatible) \u2500\u2500\n",
    "# All operations use standard Julia array ops that AutoGrad can differentiate.\n",
    "# No in-place mutation inside @diff context.\n",
    "\n",
    "# ReLU for arrays (element-wise)\n",
    "relu_ag(x) = max.(x, Float32(0))\n",
    "\n",
    "# RMSNorm for a vector\n",
    "function rmsnorm_ag(x)\n",
    "    n = length(x)\n",
    "    ms = sum(x .* x) / n\n",
    "    scale = (ms + Float32(1e-5)) ^ Float32(-0.5)\n",
    "    return x .* scale\n",
    "end\n",
    "\n",
    "# Softmax for a vector (numerically stable)\n",
    "function softmax_ag(logits)\n",
    "    mx = maximum(logits)\n",
    "    exps = exp.(logits .- mx)\n",
    "    s = sum(exps)\n",
    "    return exps ./ s\n",
    "end\n",
    "\n",
    "# Type-dispatch gradient densifier (replaces try/catch in hot loop)\n",
    "function to_dense_grad(g)\n",
    "    if g isa AutoGrad.Sparse\n",
    "        Float32.(AutoGrad.full(g))\n",
    "    else\n",
    "        Float32.(g)\n",
    "    end\n",
    "end\n",
    "\n",
    "println(\"Array-based primitives defined (relu_ag, rmsnorm_ag, softmax_ag, to_dense_grad)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# backward! is no longer needed -- AutoGrad.jl handles backpropagation via @diff + grad()\n# Usage:\n#   tape = @diff loss_expression    # builds computation tape\n#   g = grad(tape, some_param)      # extracts gradient for a Param\n#   loss_val = value(tape)          # extracts scalar loss value\nprintln(\"Using AutoGrad.jl for automatic differentiation (no manual backward! needed)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## 2. Dataset \u2014 Download & Load Training Data\n\nDownloads **54 classical philosophy texts** from Project Gutenberg and MIT Classics Archive, spanning:\n- **Plato** (12 works): Republic, Apology, Symposium, Phaedo, Crito, Meno, Phaedrus, Timaeus, Laws, Gorgias, Protagoras, Theaetetus\n- **Aristotle** (12 works): Categories, Ethics, Rhetoric, Physics, Metaphysics, Poetics, Politics, On the Soul, On the Heavens, Prior/Posterior Analytics, Topics, On Generation & Corruption\n- **Stoics** (4 works): Marcus Aurelius Meditations, Epictetus Discourses & Enchiridion, Seneca Moral Essays\n- **Roman** (4 works): Lucretius, Cicero (On Duties, Nature of Gods, On Friendship)\n- **Early Modern** (6 works): Descartes, Kant, Spinoza, Hobbes, Locke, Bacon\n- **Enlightenment/19th c.** (10 works): Hume, Rousseau, Nietzsche (x2), Mill (x2), Machiavelli, Emerson, Thoreau, Montaigne\n- **Other** (6 works): Boethius, Diogenes/Epicurus, Latin Grammar, Schopenhauer\n\nEach text is split into paragraphs to form the `TRAINING_DATA` document array.\nVocabulary is built dynamically from the data (a-z + punctuation + BOS)."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "using Downloads\n\nfunction download_and_clean(url::String, fn::String; is_gutenberg=true)\n    if !isfile(fn)\n        println(\"Downloading $fn ...\")\n        try\n            Downloads.download(url, fn)\n        catch e\n            @warn \"Download failed: $url -> $e\"\n            return \"\"\n        end\n    end\n    txt = read(fn, String)\n    if is_gutenberg\n        txt = replace(txt, r\"(?is)^.*?\\*{3}\\s*START OF (THE|THIS) PROJECT GUTENBERG.*?\\*{3}[\\r\\n]*\" => \"\")\n        txt = replace(txt, r\"(?is)\\*{3}\\s*END OF (THE|THIS) PROJECT GUTENBERG.*$\" => \"\")\n    end\n    txt = replace(txt, r\"\\r\\n\" => \"\\n\")\n    txt = replace(txt, r\"\\n{3,}\" => \"\\n\\n\")\n    return strip(txt)\nend\n\nsources = Dict(\n    # \u2500\u2500 Original sources (14) \u2500\u2500\n    \"grammar\"             => (\"https://www.gutenberg.org/files/15665/15665-0.txt\",        \"latin_grammar.txt\",         true),\n    \"categories\"          => (\"https://www.gutenberg.org/ebooks/2412.txt.utf-8\",          \"aristotle_categories.txt\",  true),\n    \"rhetoric\"            => (\"http://classics.mit.edu/Aristotle/rhetoric.mb.txt\",        \"aristotle_rhetoric.txt\",    false),\n    \"prior_analytics\"     => (\"http://classics.mit.edu/Aristotle/prior.mb.txt\",           \"prior_analytics.txt\",       false),\n    \"posterior_analytics\"  => (\"http://classics.mit.edu/Aristotle/posterior.mb.txt\",       \"posterior_analytics.txt\",   false),\n    \"topics\"              => (\"http://classics.mit.edu/Aristotle/topics.mb.txt\",          \"topics.txt\",                false),\n    \"boethius\"            => (\"https://www.gutenberg.org/files/14328/14328-0.txt\",        \"boethius_consolation.txt\",  true),\n    \"heavens\"             => (\"http://classics.mit.edu/Aristotle/heavens.mb.txt\",         \"aristotle_heavens.txt\",     false),\n    \"republic\"            => (\"https://www.gutenberg.org/files/1497/1497-0.txt\",          \"plato_republic.txt\",        true),\n    \"apology\"             => (\"https://www.gutenberg.org/files/1656/1656-0.txt\",          \"plato_apology.txt\",         true),\n    \"ethics\"              => (\"https://www.gutenberg.org/files/8438/8438-0.txt\",          \"aristotle_ethics.txt\",      true),\n    \"emerson\"             => (\"https://www.gutenberg.org/files/2944/2944-0.txt\",          \"emerson_essays.txt\",        true),\n    \"walden\"              => (\"https://www.gutenberg.org/files/205/205-0.txt\",            \"thoreau_walden.txt\",        true),\n    \"epicurus\"            => (\"https://www.gutenberg.org/files/57342/57342-0.txt\",        \"diogenes_epicurus.txt\",     true),\n    # \u2500\u2500 Plato (10 new) \u2500\u2500\n    \"plato_symposium\"     => (\"https://www.gutenberg.org/ebooks/1600.txt.utf-8\",          \"plato_symposium.txt\",       true),\n    \"plato_phaedo\"        => (\"https://www.gutenberg.org/ebooks/1658.txt.utf-8\",          \"plato_phaedo.txt\",          true),\n    \"plato_crito\"         => (\"https://www.gutenberg.org/ebooks/1657.txt.utf-8\",          \"plato_crito.txt\",           true),\n    \"plato_meno\"          => (\"https://www.gutenberg.org/ebooks/1643.txt.utf-8\",          \"plato_meno.txt\",            true),\n    \"plato_phaedrus\"      => (\"https://www.gutenberg.org/ebooks/1636.txt.utf-8\",          \"plato_phaedrus.txt\",        true),\n    \"plato_timaeus\"       => (\"https://www.gutenberg.org/ebooks/1572.txt.utf-8\",          \"plato_timaeus.txt\",         true),\n    \"plato_laws\"          => (\"https://www.gutenberg.org/ebooks/1750.txt.utf-8\",          \"plato_laws.txt\",            true),\n    \"plato_gorgias\"       => (\"https://www.gutenberg.org/ebooks/1672.txt.utf-8\",          \"plato_gorgias.txt\",         true),\n    \"plato_protagoras\"    => (\"https://www.gutenberg.org/ebooks/1591.txt.utf-8\",          \"plato_protagoras.txt\",      true),\n    \"plato_theaetetus\"    => (\"https://www.gutenberg.org/ebooks/1726.txt.utf-8\",          \"plato_theaetetus.txt\",      true),\n    # \u2500\u2500 Aristotle (6 new) \u2500\u2500\n    \"aristotle_physics\"   => (\"http://classics.mit.edu/Aristotle/physics.mb.txt\",         \"aristotle_physics.txt\",     false),\n    \"aristotle_metaphysics\" => (\"http://classics.mit.edu/Aristotle/metaphysics.mb.txt\",   \"aristotle_metaphysics.txt\", false),\n    \"aristotle_soul\"      => (\"http://classics.mit.edu/Aristotle/soul.mb.txt\",            \"aristotle_soul.txt\",        false),\n    \"aristotle_poetics\"   => (\"https://www.gutenberg.org/files/1974/1974.txt\",            \"aristotle_poetics.txt\",     true),\n    \"aristotle_politics\"  => (\"https://www.gutenberg.org/ebooks/6762.txt.utf-8\",          \"aristotle_politics.txt\",    true),\n    \"aristotle_generation\" => (\"http://classics.mit.edu/Aristotle/gener_corr.mb.txt\",     \"aristotle_generation.txt\",  false),\n    # \u2500\u2500 Stoics (4 new) \u2500\u2500\n    \"marcus_meditations\"  => (\"https://www.gutenberg.org/files/2680/2680-0.txt\",          \"marcus_meditations.txt\",    true),\n    \"epictetus_discourses\" => (\"https://www.gutenberg.org/ebooks/10661.txt.utf-8\",        \"epictetus_discourses.txt\",  true),\n    \"epictetus_enchiridion\" => (\"https://www.gutenberg.org/ebooks/45109.txt.utf-8\",       \"epictetus_enchiridion.txt\", true),\n    \"seneca_moral_essays\" => (\"https://www.gutenberg.org/ebooks/64576.txt.utf-8\",         \"seneca_moral_essays.txt\",   true),\n    # \u2500\u2500 Roman philosophers (4 new) \u2500\u2500\n    \"lucretius\"           => (\"https://www.gutenberg.org/ebooks/785.txt.utf-8\",           \"lucretius_nature.txt\",      true),\n    \"cicero_duties\"       => (\"https://www.gutenberg.org/ebooks/47001.txt.utf-8\",         \"cicero_duties.txt\",         true),\n    \"cicero_nature_gods\"  => (\"https://www.gutenberg.org/files/14988/14988.txt\",          \"cicero_nature_gods.txt\",    true),\n    \"cicero_friendship\"   => (\"https://www.gutenberg.org/ebooks/2808.txt.utf-8\",          \"cicero_friendship.txt\",     true),\n    # \u2500\u2500 Early Modern (6 new) \u2500\u2500\n    \"descartes_method\"    => (\"https://www.gutenberg.org/ebooks/59.txt.utf-8\",            \"descartes_method.txt\",      true),\n    \"descartes_meditations\" => (\"https://www.gutenberg.org/ebooks/70091.txt.utf-8\",       \"descartes_meditations.txt\", true),\n    \"kant_pure_reason\"    => (\"https://www.gutenberg.org/ebooks/4280.txt.utf-8\",          \"kant_pure_reason.txt\",      true),\n    \"spinoza_ethics\"      => (\"https://www.gutenberg.org/ebooks/3800.txt.utf-8\",          \"spinoza_ethics.txt\",        true),\n    \"hobbes_leviathan\"    => (\"https://www.gutenberg.org/ebooks/3207.txt.utf-8\",          \"hobbes_leviathan.txt\",      true),\n    \"locke_government\"    => (\"https://www.gutenberg.org/ebooks/7370.txt.utf-8\",          \"locke_government.txt\",      true),\n    # \u2500\u2500 Enlightenment & 19th century (8 new) \u2500\u2500\n    \"hume_understanding\"  => (\"https://www.gutenberg.org/ebooks/9662.txt.utf-8\",          \"hume_understanding.txt\",    true),\n    \"rousseau_social_contract\" => (\"https://www.gutenberg.org/files/46333/46333-0.txt\",   \"rousseau_social_contract.txt\", true),\n    \"nietzsche_beyond\"    => (\"https://www.gutenberg.org/ebooks/4363.txt.utf-8\",          \"nietzsche_beyond.txt\",      true),\n    \"nietzsche_zarathustra\" => (\"https://www.gutenberg.org/ebooks/1998.txt.utf-8\",        \"nietzsche_zarathustra.txt\", true),\n    \"mill_liberty\"        => (\"https://www.gutenberg.org/ebooks/34901.txt.utf-8\",         \"mill_liberty.txt\",          true),\n    \"mill_utilitarianism\" => (\"https://www.gutenberg.org/ebooks/11224.txt.utf-8\",         \"mill_utilitarianism.txt\",   true),\n    \"machiavelli_prince\"  => (\"https://www.gutenberg.org/files/57037/57037-0.txt\",        \"machiavelli_prince.txt\",    true),\n    \"bacon_essays\"        => (\"https://www.gutenberg.org/ebooks/575.txt.utf-8\",           \"bacon_essays.txt\",          true),\n    # \u2500\u2500 Essays (2 new) \u2500\u2500\n    \"montaigne_essays\"    => (\"https://www.gutenberg.org/ebooks/3600.txt.utf-8\",          \"montaigne_essays.txt\",      true),\n    \"schopenhauer_essays\" => (\"https://www.gutenberg.org/ebooks/11945.txt.utf-8\",         \"schopenhauer_essays.txt\",   true)\n)\n\n# Download all texts\ntexts = Dict{String,String}()\nfor (key, (url, fn, is_gut)) in sources\n    texts[key] = download_and_clean(url, fn; is_gutenberg=is_gut)\nend\nprintln(\"Downloaded $(length(texts)) texts.\")\n\n# Unicode normalization\nfor (_, (_, fn, _)) in sources\n    isfile(fn) || continue\n    txt = read(fn, String)\n    txt = replace(txt,\n        \"\\u201c\" => \"\\\"\", \"\\u201d\" => \"\\\"\",\n        \"\\u2018\" => \"'\", \"\\u2019\" => \"'\",\n        \"\\u2014\" => \"--\", \"\\u2013\" => \"-\",\n        \"\\u2026\" => \"...\", \"\\u00A0\" => \" \"\n    )\n    txt = replace(txt, r\"\\n{3,}\" => \"\\n\\n\")\n    open(fn, \"w\") do io; write(io, strip(txt)); end\nend\n\n# Split all texts into paragraphs -> TRAINING_DATA array\nTRAINING_DATA = String[]\nfor (_, (_, fn, _)) in sources\n    isfile(fn) || continue\n    txt = read(fn, String)\n    # Normalize to lowercase, keep a-z, space, period, newlines\n    txt = lowercase(txt)\n    txt = replace(txt, r\"[^a-z \\.\\n]\" => \" \")\n    txt = replace(txt, r\"  +\" => \" \")\n    # Split on blank lines into paragraphs\n    paragraphs = split(txt, r\"\\n\\n+\")\n    for p in paragraphs\n        cleaned = strip(replace(String(p), r\"\\n\" => \" \"))\n        cleaned = replace(cleaned, r\"  +\" => \" \")\n        # Only keep paragraphs with meaningful content (20+ chars)\n        if length(cleaned) >= 20\n            # Truncate very long paragraphs to block_size-friendly chunks (~512 chars)\n            while length(cleaned) > 512\n                # Find a sentence break near 512\n                cutoff = min(512, length(cleaned))\n                dot_pos = findlast('.', cleaned[1:cutoff])\n                if dot_pos !== nothing && dot_pos > 100\n                    push!(TRAINING_DATA, strip(cleaned[1:dot_pos]))\n                    cleaned = strip(cleaned[dot_pos+1:end])\n                else\n                    push!(TRAINING_DATA, strip(cleaned[1:cutoff]))\n                    cleaned = strip(cleaned[cutoff+1:end])\n                end\n            end\n            if length(cleaned) >= 20\n                push!(TRAINING_DATA, cleaned)\n            end\n        end\n    end\nend\n\nprintln(\"TRAINING_DATA: $(length(TRAINING_DATA)) paragraphs from 54 philosophy texts\")\nprintln(\"Total characters: $(sum(length, TRAINING_DATA))\")\nprintln(\"Sample: \\\"$(TRAINING_DATA[1][1:min(80, end)])...\\\"\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## 3. Neural Network Helpers\n\nWeight matrices are `Param(Matrix{Float32})` -- AutoGrad.jl tracks gradients through array operations automatically.  \nHelper functions for parameter initialization, key ordering, and checkpoint support."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Helper: deterministic parameter key ordering (for checkpoint serialization)\nfunction get_param_keys(n_layer::Int)\n    keys = [\"wte\", \"wpe\", \"lm_head\"]\n    for i in 0:n_layer-1\n        append!(keys, [\n            \"layer$i.attn_wq\", \"layer$i.attn_wk\", \"layer$i.attn_wv\", \"layer$i.attn_wo\",\n            \"layer$i.mlp_fc1\", \"layer$i.mlp_fc2\"\n        ])\n    end\n    return keys\nend\n\n# Helper: initialize a Param-wrapped weight matrix\nfunction init_param(nout::Int, nin::Int; std=0.08f0)\n    Param(randn(Float32, nout, nin) .* std)\nend\n\n# Helper: collect all Param objects from state_dict in deterministic order\nfunction collect_params(state_dict, param_keys)\n    ps = []\n    for key in param_keys\n        push!(ps, state_dict[key])\n    end\n    return ps\nend\n\nprintln(\"Neural network helpers defined (get_param_keys, init_param, collect_params)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## 4. GPT Forward Pass\n\nProcesses one token at a time with KV cache for causal masking.  \nAll operations use array matrix multiplications via AutoGrad.jl.  \nKV cache stores detached (plain Float32) vectors -- gradients flow only through the current token."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "function gpt(token_id::Int, pos_id::Int,\n             kv_key_mats::Vector{Matrix{Float32}},\n             kv_val_mats::Vector{Matrix{Float32}},\n             kv_lens::Vector{Int},\n             params,\n             n_layer::Int, n_head::Int, head_dim::Int)\n\n    d_model = n_head * head_dim\n\n    # Embedding lookup: row indexing into Param matrices\n    tok_emb = params.wte[token_id, :]\n    pos_emb = params.wpe[pos_id, :]\n    x = tok_emb .+ pos_emb\n    x = rmsnorm_ag(x)\n\n    for li in 1:n_layer\n        layer = params.layers[li]\n        x_res = x\n        x = rmsnorm_ag(x)\n\n        # Linear projections: W * x where W is (d_model, d_model)\n        q = layer.attn_wq * x\n        k = layer.attn_wk * x\n        v = layer.attn_wv * x\n\n        # Detach K, V for cache storage (gradients don't flow through cached entries)\n        k_detached = Float32.(value(k))\n        v_detached = Float32.(value(v))\n\n        # In-place column write into pre-allocated cache (plain Float32, not tracked)\n        idx = kv_lens[li] + 1\n        kv_key_mats[li][:, idx] = k_detached\n        kv_val_mats[li][:, idx] = v_detached\n        kv_lens[li] = idx\n\n        # Zero-allocation view instead of hcat\n        n_cached = idx\n        K_mat = @view kv_key_mats[li][:, 1:n_cached]\n        V_mat = @view kv_val_mats[li][:, 1:n_cached]\n\n        # Multi-head attention with typed ntuple (compiler can unroll)\n        head_results = ntuple(n_head) do hh\n            h = hh - 1\n            hs = h * head_dim + 1\n            he = hs + head_dim - 1\n\n            q_h = q[hs:he]                   # (head_dim,) tracked\n            K_h = K_mat[hs:he, :]             # (head_dim, n_cached) plain Float32\n            V_h = V_mat[hs:he, :]             # (head_dim, n_cached) plain Float32\n\n            # Attention scores: K_h' * q_h / sqrt(head_dim)\n            scores = (K_h' * q_h) ./ Float32(sqrt(head_dim))\n            attn_w = softmax_ag(scores)\n\n            # Weighted sum: V_h * attn_w\n            V_h * attn_w\n        end\n\n        x_attn = vcat(head_results...)    # (d_model,) tracked\n\n        # Output projection\n        x = layer.attn_wo * x_attn\n        x = x .+ x_res\n\n        # MLP block\n        x_res = x\n        x = rmsnorm_ag(x)\n        x = layer.mlp_fc1 * x\n        x = relu_ag(x)\n        x = layer.mlp_fc2 * x\n        x = x .+ x_res\n    end\n\n    # LM head: (vocab_size, d_model) * (d_model,) = (vocab_size,)\n    logits = params.lm_head * x\n    return logits\nend\n\n# Helper: build type-stable params NamedTuple from state_dict\nfunction build_params(state_dict, n_layer::Int)\n    layers = ntuple(n_layer) do li\n        i = li - 1\n        (\n            attn_wq = state_dict[\"layer$i.attn_wq\"],\n            attn_wk = state_dict[\"layer$i.attn_wk\"],\n            attn_wv = state_dict[\"layer$i.attn_wv\"],\n            attn_wo = state_dict[\"layer$i.attn_wo\"],\n            mlp_fc1 = state_dict[\"layer$i.mlp_fc1\"],\n            mlp_fc2 = state_dict[\"layer$i.mlp_fc2\"],\n        )\n    end\n    return (\n        wte = state_dict[\"wte\"],\n        wpe = state_dict[\"wpe\"],\n        lm_head = state_dict[\"lm_head\"],\n        layers = layers,\n    )\nend\n\n# Helper: allocate KV cache buffers\nfunction alloc_kv_cache(n_layer::Int, d_model::Int, block_size::Int)\n    kv_key_mats = [zeros(Float32, d_model, block_size) for _ in 1:n_layer]\n    kv_val_mats = [zeros(Float32, d_model, block_size) for _ in 1:n_layer]\n    kv_lens = zeros(Int, n_layer)\n    return kv_key_mats, kv_val_mats, kv_lens\nend\n\n# Helper: reset KV cache for a new sequence\nfunction reset_kv_cache!(kv_lens::Vector{Int})\n    kv_lens .= 0\nend\n\nprintln(\"GPT forward pass defined (optimized: pre-allocated KV cache, typed params, unrolled heads)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## 5. Checkpoint Save/Load\n\nSave and load model weights + optimizer state as JSON.\nCheckpoints are synced to HuggingFace Hub for persistence across Colab sessions."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function save_checkpoint(path::String, state_dict, param_keys, uchars, hyperparams;\n",
    "                         adam_m=nothing, adam_v=nothing, step::Int=0,\n",
    "                         lr::Float64=0.01, b1::Float64=0.85, b2::Float64=0.99,\n",
    "                         best_val_loss::Float64=Inf,\n",
    "                         train_losses::Vector{Float64}=Float64[],\n",
    "                         val_losses::Vector{Float64}=Float64[],\n",
    "                         total_steps::Int=0, num_steps_target::Int=0)\n",
    "\n",
    "    sd_data = Dict{String,Any}()\n",
    "    for k in param_keys\n",
    "        W = value(state_dict[k])  # unwrap Param -> Matrix{Float32}\n",
    "        sd_data[k] = [Float64.(W[i, :]) for i in 1:size(W, 1)]\n",
    "    end\n",
    "\n",
    "    # Serialize Adam state per param key\n",
    "    adam_m_data = Dict{String,Any}()\n",
    "    adam_v_data = Dict{String,Any}()\n",
    "    if adam_m !== nothing\n",
    "        for k in param_keys\n",
    "            if haskey(adam_m, k)\n",
    "                adam_m_data[k] = [Float64.(adam_m[k][i, :]) for i in 1:size(adam_m[k], 1)]\n",
    "                adam_v_data[k] = [Float64.(adam_v[k][i, :]) for i in 1:size(adam_v[k], 1)]\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "\n",
    "    checkpoint = Dict{String,Any}(\n",
    "        \"format\" => \"autograd_v2\",\n",
    "        \"uchars\" => [string(c) for c in uchars],\n",
    "        \"hyperparams\" => hyperparams,\n",
    "        \"state_dict\" => sd_data,\n",
    "        \"optimizer\" => Dict{String,Any}(\n",
    "            \"adam_m\" => adam_m_data,\n",
    "            \"adam_v\" => adam_v_data,\n",
    "            \"step\" => step,\n",
    "            \"lr\" => lr,\n",
    "            \"beta1\" => b1,\n",
    "            \"beta2\" => b2\n",
    "        ),\n",
    "        \"training\" => Dict{String,Any}(\n",
    "            \"best_val_loss\" => best_val_loss,\n",
    "            \"train_losses\" => train_losses,\n",
    "            \"val_losses\" => val_losses,\n",
    "            \"total_steps_completed\" => total_steps,\n",
    "            \"num_steps_target\" => num_steps_target\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Replace Inf with large number for JSON compatibility\n",
    "    if best_val_loss == Inf\n",
    "        checkpoint[\"training\"][\"best_val_loss\"] = 1e30\n",
    "    end\n",
    "    mkpath(dirname(path))\n",
    "    open(path, \"w\") do f\n",
    "        JSON3.write(f, checkpoint)\n",
    "    end\n",
    "    vl_str = best_val_loss == Inf ? \"Inf\" : @sprintf(\"%.4f\", best_val_loss)\n",
    "    println(\"Checkpoint saved: $path (step $step, best_val_loss=$vl_str)\")\n",
    "end\n",
    "\n",
    "function load_checkpoint(path::String)\n",
    "    println(\"Loading checkpoint from $path ...\")\n",
    "    raw = JSON3.read(read(path, String))\n",
    "\n",
    "    uchars = [only(String(s)) for s in raw[\"uchars\"]]\n",
    "    BOS = length(uchars) + 1\n",
    "    vocab_size = BOS\n",
    "\n",
    "    hp = raw[\"hyperparams\"]\n",
    "    n_layer = Int(hp[\"n_layer\"])\n",
    "    n_embd = Int(hp[\"n_embd\"])\n",
    "    block_size = Int(hp[\"block_size\"])\n",
    "    n_head = Int(hp[\"n_head\"])\n",
    "    head_dim = n_embd \u00f7 n_head\n",
    "\n",
    "    # Detect format version\n",
    "    fmt = haskey(raw, \"format\") ? String(raw[\"format\"]) : \"v1\"\n",
    "\n",
    "    state_dict = Dict{String, Any}()\n",
    "    for (key, matrix_rows) in pairs(raw[\"state_dict\"])\n",
    "        rows = [Float32.(collect(row)) for row in matrix_rows]\n",
    "        W = vcat([reshape(r, 1, :) for r in rows]...)  # stack rows into Matrix\n",
    "        state_dict[string(key)] = Param(W)\n",
    "    end\n",
    "\n",
    "    # Load Adam state\n",
    "    opt_raw = raw[\"optimizer\"]\n",
    "    adam_m = Dict{String, Matrix{Float32}}()\n",
    "    adam_v = Dict{String, Matrix{Float32}}()\n",
    "\n",
    "    if fmt == \"autograd_v2\" && haskey(opt_raw, \"adam_m\")\n",
    "        am_raw = opt_raw[\"adam_m\"]\n",
    "        av_raw = opt_raw[\"adam_v\"]\n",
    "        if !isempty(am_raw)\n",
    "            for (key, matrix_rows) in pairs(am_raw)\n",
    "                rows = [Float32.(collect(row)) for row in matrix_rows]\n",
    "                adam_m[string(key)] = vcat([reshape(r, 1, :) for r in rows]...)\n",
    "            end\n",
    "            for (key, matrix_rows) in pairs(av_raw)\n",
    "                rows = [Float32.(collect(row)) for row in matrix_rows]\n",
    "                adam_v[string(key)] = vcat([reshape(r, 1, :) for r in rows]...)\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "\n",
    "    step = Int(opt_raw[\"step\"])\n",
    "    lr = Float64(opt_raw[\"lr\"])\n",
    "    b1 = Float64(opt_raw[\"beta1\"])\n",
    "    b2 = Float64(opt_raw[\"beta2\"])\n",
    "\n",
    "    trn = raw[\"training\"]\n",
    "    best_val_loss = Float64(trn[\"best_val_loss\"])\n",
    "    train_losses = Float64.(collect(trn[\"train_losses\"]))\n",
    "    val_losses = Float64.(collect(trn[\"val_losses\"]))\n",
    "    total_steps = Int(trn[\"total_steps_completed\"])\n",
    "    num_steps_target = Int(trn[\"num_steps_target\"])\n",
    "\n",
    "    println(\"  vocab=$vocab_size, embd=$n_embd, layers=$n_layer, step=$step\")\n",
    "\n",
    "    return (;\n",
    "        state_dict, uchars, BOS, vocab_size,\n",
    "        n_layer, n_embd, block_size, n_head, head_dim,\n",
    "        adam_m, adam_v, step, lr, b1, b2,\n",
    "        best_val_loss, train_losses, val_losses,\n",
    "        total_steps, num_steps_target\n",
    "    )\n",
    "end\n",
    "\n",
    "println(\"Checkpoint save/load defined (autograd_v2 format)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Setup \u2014 Dataset, Tokenizer, Parameters\n",
    "\n",
    "Character-level tokenizer with a BOS token. 90/10 train/val split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# \u2500\u2500 Dataset with train/val split (ordered, not shuffled) \u2500\u2500\ndocs = copy(TRAINING_DATA)\nsplit_idx = max(1, Int(floor(0.9 * length(docs))))\ntrain_docs = docs[1:split_idx]\nval_docs = docs[split_idx+1:end]\nif isempty(val_docs)\n    val_docs = docs[max(1, end-4):end]\n    train_docs = docs[1:max(1, end-5)]\nend\nprintln(\"train: $(length(train_docs)) docs, val: $(length(val_docs)) docs\")\n\n# \u2500\u2500 Tokenizer \u2500\u2500\nuchars = sort(unique(join(docs)))\nBOS = length(uchars) + 1\nvocab_size = BOS\nprintln(\"vocab size: $vocab_size ($(length(uchars)) chars + BOS)\")\n\n# O(1) character-to-index lookup (replaces O(n) findfirst)\nchar_to_id = Dict{Char, Int}(ch => i for (i, ch) in enumerate(uchars))\n\n# Pre-tokenize all documents once (avoids re-tokenizing every training step)\nfunction tokenize_doc(doc::String, char_to_id::Dict{Char,Int}, BOS::Int)\n    vcat([BOS], [char_to_id[ch] for ch in doc], [BOS])\nend\n\ntrain_tokens = [tokenize_doc(doc, char_to_id, BOS) for doc in train_docs]\nval_tokens = [tokenize_doc(doc, char_to_id, BOS) for doc in val_docs]\nprintln(\"Pre-tokenized: $(length(train_tokens)) train, $(length(val_tokens)) val docs\")\n\n# \u2500\u2500 Hyperparameters \u2500\u2500\nn_layer    = 1\nn_embd     = 16\nblock_size = 256\nn_head     = 4\nhead_dim   = n_embd \u00f7 n_head\n\nhyperparams = Dict{String,Any}(\n    \"n_layer\" => n_layer, \"n_embd\" => n_embd,\n    \"block_size\" => block_size, \"n_head\" => n_head\n)\n\n# \u2500\u2500 Initialize parameters as Param(Matrix{Float32}) \u2500\u2500\nstate_dict = Dict{String, Any}()\nstate_dict[\"wte\"]     = init_param(vocab_size, n_embd)   # (vocab_size, d_model)\nstate_dict[\"wpe\"]     = init_param(block_size, n_embd)   # (block_size, d_model)\nstate_dict[\"lm_head\"] = init_param(vocab_size, n_embd)   # (vocab_size, d_model)\nfor i in 0:n_layer-1\n    state_dict[\"layer$i.attn_wq\"]  = init_param(n_embd, n_embd)\n    state_dict[\"layer$i.attn_wk\"]  = init_param(n_embd, n_embd)\n    state_dict[\"layer$i.attn_wv\"]  = init_param(n_embd, n_embd)\n    state_dict[\"layer$i.attn_wo\"]  = init_param(n_embd, n_embd)\n    state_dict[\"layer$i.mlp_fc1\"]  = init_param(4 * n_embd, n_embd)\n    state_dict[\"layer$i.mlp_fc2\"]  = init_param(n_embd, 4 * n_embd)\nend\n\nparam_keys = get_param_keys(n_layer)\nall_params = collect_params(state_dict, param_keys)\ntotal_num_params = sum(length(value(p)) for p in all_params)\nprintln(\"num params: $total_num_params ($(length(all_params)) weight matrices)\")\n\n# Build type-stable params NamedTuple (avoids Dict lookup in hot loop)\nparams = build_params(state_dict, n_layer)\nprintln(\"Type-stable params tuple built\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## 7. Training Loop with Validation + Best-Model Checkpointing\n\nAdam optimizer with linear LR decay.\nValidates every 50 steps, saves `best_model.json` when val loss improves.\nCheckpoints sync to HuggingFace Hub automatically.\nPeriodic checkpoints every 200 steps."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "function compute_val_loss(val_tokens, params, BOS, block_size, n_layer, n_head, head_dim, n_embd)\n    total_loss = 0.0\n    total_tokens = 0\n    kv_key_mats, kv_val_mats, kv_lens = alloc_kv_cache(n_layer, n_embd, block_size)\n    for tokens in val_tokens\n        n = min(block_size, length(tokens) - 1)\n        reset_kv_cache!(kv_lens)\n        for pos in 1:n\n            token_id = tokens[pos]\n            target_id = tokens[pos + 1]\n            # Outside @diff, Params behave like plain arrays\n            logits = gpt(token_id, pos, kv_key_mats, kv_val_mats, kv_lens, params, n_layer, n_head, head_dim)\n            probs = softmax_ag(logits)\n            p_val = Float64.(value(probs))\n            total_loss += -log(max(p_val[target_id], 1e-10))\n            total_tokens += 1\n        end\n    end\n    return total_loss / max(total_tokens, 1)\nend\n\nprintln(\"compute_val_loss defined (optimized: pre-allocated KV cache, typed params)\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# \u2500\u2500 Adam optimizer state (per-parameter matrices) \u2500\u2500\nlr, b1, b2, eps = 0.01, 0.85, 0.99, 1e-8\n\nadam_m = Dict{String, Matrix{Float32}}()\nadam_v = Dict{String, Matrix{Float32}}()\nfor k in param_keys\n    sz = size(value(state_dict[k]))\n    adam_m[k] = zeros(Float32, sz)\n    adam_v[k] = zeros(Float32, sz)\nend\n\nbest_val_loss = Inf\ntrain_loss_history = Float64[]\nval_loss_history = Float64[]\n\n# \u2500\u2500 Checkpoint paths \u2500\u2500\nLOCAL_CKPT = \"checkpoints\"\nmkpath(LOCAL_CKPT)\n\nfunction save_and_sync(path, sd, pk, uc, hp; kwargs...)\n    save_checkpoint(path, sd, pk, uc, hp; kwargs...)\n    hf_sync(path)\nend\n\n# \u2500\u2500 Initialize W&B logging (if API key is set) \u2500\u2500\nif haskey(ENV, \"WANDB_API_KEY\") && !isempty(ENV[\"WANDB_API_KEY\"])\n    wandb_init()\nend\n\n# \u2500\u2500 Optimized training loop (wrapped in function for type stability) \u2500\u2500\nfunction train_loop!(state_dict, params, param_keys, train_tokens, val_tokens,\n                     adam_m, adam_v, uchars, hyperparams;\n                     num_steps::Int, lr::Float64, b1::Float64, b2::Float64, eps::Float64,\n                     n_layer::Int, n_head::Int, head_dim::Int, n_embd::Int,\n                     block_size::Int, BOS::Int,\n                     best_val_loss::Float64=Inf,\n                     train_loss_history::Vector{Float64}=Float64[],\n                     val_loss_history::Vector{Float64}=Float64[],\n                     start_step::Int=1)\n\n    # Pre-allocate KV cache (reused across steps)\n    kv_key_mats, kv_val_mats, kv_lens = alloc_kv_cache(n_layer, n_embd, block_size)\n\n    hf_status = !isempty(get(ENV, \"HF_REPO\", \"\")) ? \"HF:$(ENV[\"HF_REPO\"])\" : ((@isdefined(HF_REPO_ID) && !isempty(HF_REPO_ID)) ? \"HF:$HF_REPO_ID\" : \"HF:(not configured)\")\n    println(\"--- training $num_steps steps (steps $start_step..$(start_step + num_steps - 1)) ---\")\n    println(\"    Local: checkpoints/  |  $hf_status\")\n    t_start = time()\n    last_save_time = time()\n    SAVE_INTERVAL = 600\n    completed_steps = start_step - 1\n\n    try\n        for step in start_step:(start_step + num_steps - 1)\n            completed_steps = step\n            tokens = train_tokens[mod1(step, length(train_tokens))]\n            n = min(block_size, length(tokens) - 1)\n\n            # Reset KV cache for new sequence\n            reset_kv_cache!(kv_lens)\n\n            # Single @diff tape for entire sequence (KV cache mutation is safe -- detached Float32)\n            tape = @diff begin\n                loss_sum = Float32(0)\n                for pos in 1:n\n                    token_id  = tokens[pos]\n                    target_id = tokens[pos + 1]\n                    logits = gpt(token_id, pos, kv_key_mats, kv_val_mats, kv_lens, params, n_layer, n_head, head_dim)\n                    probs = softmax_ag(logits)\n                    loss_sum = loss_sum + (-log(probs[target_id]))\n                end\n                loss_sum / Float32(n)\n            end\n\n            avg_loss = Float64(value(tape))\n            push!(train_loss_history, avg_loss)\n\n            # Extract gradients once and apply Adam update\n            lr_t = lr * (1 - (step - 1) / (start_step + num_steps - 1))\n            for k in param_keys\n                g = grad(tape, state_dict[k])\n                if g !== nothing\n                    g_dense = to_dense_grad(g)\n                    adam_m[k] .= Float32(b1) .* adam_m[k] .+ Float32(1 - b1) .* g_dense\n                    adam_v[k] .= Float32(b2) .* adam_v[k] .+ Float32(1 - b2) .* g_dense .^ 2\n                    m_hat = adam_m[k] ./ Float32(1 - b1^step)\n                    v_hat = adam_v[k] ./ Float32(1 - b2^step)\n                    value(state_dict[k]) .-= Float32(lr_t) .* m_hat ./ (sqrt.(v_hat) .+ Float32(eps))\n                end\n            end\n\n            # Validate + checkpoint every 50 steps\n            if step % 50 == 0\n                val_loss = compute_val_loss(val_tokens, params, BOS, block_size, n_layer, n_head, head_dim, n_embd)\n                push!(val_loss_history, val_loss)\n                elapsed = time() - t_start\n                wandb_log(; step=step, train_loss=avg_loss, val_loss=val_loss, lr=lr_t)\n\n                improved = \"\"\n                if val_loss < best_val_loss\n                    best_val_loss = val_loss\n                    save_and_sync(\"checkpoints/best_model.json\", state_dict, param_keys, uchars, hyperparams;\n                        adam_m=adam_m, adam_v=adam_v, step=step,\n                        lr=lr, b1=b1, b2=b2,\n                        best_val_loss=best_val_loss,\n                        train_losses=train_loss_history, val_losses=val_loss_history,\n                        total_steps=step, num_steps_target=start_step + num_steps - 1)\n                    improved = \" << new best!\"\n                end\n\n                @printf(\"step %4d / %4d | train %.4f | val %.4f | %.1fs%s\\n\",\n                        step, start_step + num_steps - 1, avg_loss, val_loss, elapsed, improved)\n            elseif step % 10 == 0\n                elapsed = time() - t_start\n                @printf(\"step %4d / %4d | train %.4f | %.1fs\\n\", step, start_step + num_steps - 1, avg_loss, elapsed)\n            end\n\n            # Periodic checkpoint every 100 steps\n            if step % 100 == 0\n                save_and_sync(\"checkpoints/checkpoint_latest.json\", state_dict, param_keys, uchars, hyperparams;\n                    adam_m=adam_m, adam_v=adam_v, step=step,\n                    lr=lr, b1=b1, b2=b2,\n                    best_val_loss=best_val_loss,\n                    train_losses=train_loss_history, val_losses=val_loss_history,\n                    total_steps=step, num_steps_target=start_step + num_steps - 1)\n                last_save_time = time()\n            end\n\n            # Time-based auto-save every 10 min\n            if time() - last_save_time > SAVE_INTERVAL\n                save_and_sync(\"checkpoints/checkpoint_latest.json\", state_dict, param_keys, uchars, hyperparams;\n                    adam_m=adam_m, adam_v=adam_v, step=step,\n                    lr=lr, b1=b1, b2=b2,\n                    best_val_loss=best_val_loss,\n                    train_losses=train_loss_history, val_losses=val_loss_history,\n                    total_steps=step, num_steps_target=start_step + num_steps - 1)\n                last_save_time = time()\n                println(\"  [auto-save at step $step]\")\n            end\n        end\n    catch e\n        if e isa InterruptException\n            println(\"\\n\\nTraining interrupted at step $completed_steps!\")\n        else\n            println(\"\\n\\nTraining error at step $completed_steps: $e\")\n        end\n        println(\"Saving emergency checkpoint...\")\n        save_and_sync(\"checkpoints/checkpoint_interrupted.json\", state_dict, param_keys, uchars, hyperparams;\n            adam_m=adam_m, adam_v=adam_v, step=completed_steps,\n            lr=lr, b1=b1, b2=b2,\n            best_val_loss=best_val_loss,\n            train_losses=train_loss_history, val_losses=val_loss_history,\n            total_steps=completed_steps, num_steps_target=start_step + num_steps - 1)\n        if !(e isa InterruptException)\n            rethrow(e)\n        end\n    end\n\n    elapsed = time() - t_start\n    @printf(\"\\ntraining complete in %.1f seconds\\n\", elapsed)\n\n    return best_val_loss, train_loss_history, val_loss_history, completed_steps\nend\n\n# \u2500\u2500 Run training \u2500\u2500\nNUM_EPOCHS = 3\nnum_steps = clamp(NUM_EPOCHS * length(train_tokens), 1000, 50000)\n\nbest_val_loss, train_loss_history, val_loss_history, final_step = train_loop!(\n    state_dict, params, param_keys, train_tokens, val_tokens,\n    adam_m, adam_v, uchars, hyperparams;\n    num_steps=num_steps, lr=lr, b1=b1, b2=b2, eps=eps,\n    n_layer=n_layer, n_head=n_head, head_dim=head_dim, n_embd=n_embd,\n    block_size=block_size, BOS=BOS,\n    best_val_loss=best_val_loss,\n    train_loss_history=train_loss_history,\n    val_loss_history=val_loss_history)\n\nwandb_finish()\n\n# Final save\nsave_and_sync(\"checkpoints/final_model.json\", state_dict, param_keys, uchars, hyperparams;\n    adam_m=adam_m, adam_v=adam_v, step=final_step,\n    lr=lr, b1=b1, b2=b2,\n    best_val_loss=best_val_loss,\n    train_losses=train_loss_history, val_losses=val_loss_history,\n    total_steps=final_step, num_steps_target=num_steps)\n\nprintln(\"\\nCheckpoints saved locally + synced to HF. Best val loss: $(@sprintf(\"%.4f\", best_val_loss))\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Inference \u2014 Hallucinated Philosophy\n",
    "\n",
    "Generate new philosophy-like text using temperature-controlled sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "function generate_text(params, uchars, BOS, n_layer, n_head, head_dim, n_embd, block_size;\n                       temperature=0.8, max_tokens=128)\n    kv_key_mats, kv_val_mats, kv_lens = alloc_kv_cache(n_layer, n_embd, block_size)\n    token_id = BOS\n    sample = Char[]\n    limit = min(max_tokens, block_size)\n    for pos in 1:limit\n        # No @diff needed for inference -- Params act like plain arrays outside @diff\n        logits = gpt(token_id, pos, kv_key_mats, kv_val_mats, kv_lens, params, n_layer, n_head, head_dim)\n        scaled = logits ./ temperature\n        probs = softmax_ag(scaled)\n        weights = Float64.(value(probs))\n\n        # Categorical sampling\n        r = rand()\n        cum = 0.0\n        token_id = 1\n        for (idx, w) in enumerate(weights)\n            cum += w\n            if r <= cum\n                token_id = idx\n                break\n            end\n        end\n        token_id == BOS && break\n        push!(sample, uchars[token_id])\n    end\n    return String(sample)\nend\n\nprintln(\"--- inference (hallucinated philosophy) ---\")\nfor i in 1:20\n    text = generate_text(params, uchars, BOS, n_layer, n_head, head_dim, n_embd, block_size; temperature=0.8)\n    @printf(\"sample %2d: %s\\n\", i, text)\nend"
  },
  {
   "cell_type": "markdown",
   "source": "---\n## 8a. Push Model to HuggingFace Hub\n\nPush your trained checkpoint to HuggingFace for persistence across Colab sessions.  \nSet `HF_REPO_ID` in the login cell above (e.g. `\"yourusername/microgpt-philosophy\"`).",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Push checkpoint to HuggingFace Hub\n# Make sure HF_REPO_ID is set in the login cell above\n\nif @isdefined(HF_REPO_ID) && !isempty(HF_REPO_ID)\n    # Create repo if it doesn't exist yet\n    hf_create_repo(HF_REPO_ID)\n\n    # Push best model checkpoint\n    if isfile(\"checkpoints/best_model.json\")\n        hf_push_checkpoint(HF_REPO_ID; checkpoint_path=\"checkpoints/best_model.json\")\n    else\n        println(\"No best_model.json found \u2014 train first!\")\n    end\n\n    # Also push final model if it exists\n    if isfile(\"checkpoints/final_model.json\")\n        hf_push(HF_REPO_ID, \"checkpoints/final_model.json\")\n    end\n\n    println(\"\\nDone! View your model at: https://huggingface.co/$HF_REPO_ID\")\nelse\n    println(\"Set HF_REPO_ID in the login cell (e.g. \\\"yourusername/microgpt-philosophy\\\")\")\nend",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n## 8b. Pull Checkpoint from HuggingFace Hub\n\nDownload a previously pushed checkpoint to resume training in a new Colab session.  \nRun this before the Resume Training cell below.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Pull checkpoint from HuggingFace to resume training\n# Make sure HF_REPO_ID is set in the login cell above\n\nif @isdefined(HF_REPO_ID) && !isempty(HF_REPO_ID)\n    mkpath(\"checkpoints\")\n    hf_pull(HF_REPO_ID, \"best_model.json\"; local_dir=\"checkpoints\")\n    println(\"\\nReady to resume from checkpoints/best_model.json\")\n    println(\"Run the 'Resume Training' cell below.\")\nelse\n    println(\"Set HF_REPO_ID in the login cell (e.g. \\\"yourusername/microgpt-philosophy\\\")\")\nend",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Resume Training from Checkpoint\n",
    "\n",
    "Load a saved checkpoint and continue training for more steps.  \n",
    "Skip this cell if you're training from scratch above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# \u2500\u2500 Load checkpoint \u2500\u2500\n# Change the path to load a different checkpoint\nRESUME_FROM = \"checkpoints/best_model.json\"\nEXTRA_STEPS = clamp(length(train_docs), 500, 25000)  # ~1 extra epoch\n\nif !isfile(RESUME_FROM)\n    # Try pulling from HuggingFace\n    if @isdefined(HF_REPO_ID) && !isempty(HF_REPO_ID)\n        println(\"Checkpoint not found locally, pulling from HuggingFace...\")\n        hf_pull(HF_REPO_ID, basename(RESUME_FROM); local_dir=\"checkpoints\")\n    end\n    isfile(RESUME_FROM) || error(\"Checkpoint not found: $RESUME_FROM\")\nend\n\nckpt = load_checkpoint(RESUME_FROM)\nstate_dict = ckpt.state_dict\nuchars = ckpt.uchars\nBOS = ckpt.BOS\nn_layer = ckpt.n_layer\nn_embd = ckpt.n_embd\nblock_size = ckpt.block_size\nn_head = ckpt.n_head\nhead_dim = ckpt.head_dim\n\nhyperparams = Dict{String,Any}(\n    \"n_layer\" => n_layer, \"n_embd\" => n_embd,\n    \"block_size\" => block_size, \"n_head\" => n_head\n)\n\n# Reconstruct dataset and split (ordered, not shuffled)\ndocs = copy(TRAINING_DATA)\nsplit_idx = max(1, Int(floor(0.9 * length(docs))))\ntrain_docs = docs[1:split_idx]\nval_docs = docs[split_idx+1:end]\nif isempty(val_docs)\n    val_docs = docs[max(1, end-4):end]\n    train_docs = docs[1:max(1, end-5)]\nend\n\n# Rebuild O(1) tokenizer and pre-tokenize\nchar_to_id = Dict{Char, Int}(ch => i for (i, ch) in enumerate(uchars))\ntrain_tokens = [tokenize_doc(doc, char_to_id, BOS) for doc in train_docs]\nval_tokens = [tokenize_doc(doc, char_to_id, BOS) for doc in val_docs]\n\nparam_keys = get_param_keys(n_layer)\n\n# Build type-stable params tuple\nparams = build_params(state_dict, n_layer)\n\n# Restore optimizer (Adam state per param key)\nlr = ckpt.lr; b1 = ckpt.b1; b2 = ckpt.b2; eps = 1e-8\nadam_m = if !isempty(ckpt.adam_m)\n    ckpt.adam_m\nelse\n    Dict{String, Matrix{Float32}}(k => zeros(Float32, size(value(state_dict[k]))) for k in param_keys)\nend\nadam_v = if !isempty(ckpt.adam_v)\n    ckpt.adam_v\nelse\n    Dict{String, Matrix{Float32}}(k => zeros(Float32, size(value(state_dict[k]))) for k in param_keys)\nend\n\nstart_step = ckpt.step + 1\nbest_val_loss = ckpt.best_val_loss\ntrain_loss_history = copy(ckpt.train_losses)\nval_loss_history = copy(ckpt.val_losses)\n\n# \u2500\u2500 Initialize W&B logging (if API key is set) \u2500\u2500\nif haskey(ENV, \"WANDB_API_KEY\") && !isempty(ENV[\"WANDB_API_KEY\"])\n    wandb_init()\nend\n\nprintln(\"\\nResuming from step $(ckpt.step) -> training $EXTRA_STEPS more steps\")\nprintln(\"Best val loss so far: $(round(best_val_loss, digits=4))\")\n\nbest_val_loss, train_loss_history, val_loss_history, final_step = train_loop!(\n    state_dict, params, param_keys, train_tokens, val_tokens,\n    adam_m, adam_v, uchars, hyperparams;\n    num_steps=EXTRA_STEPS, lr=lr, b1=b1, b2=b2, eps=Float64(eps),\n    n_layer=n_layer, n_head=n_head, head_dim=head_dim, n_embd=n_embd,\n    block_size=block_size, BOS=BOS,\n    best_val_loss=best_val_loss,\n    train_loss_history=train_loss_history,\n    val_loss_history=val_loss_history,\n    start_step=start_step)\n\nwandb_finish()\n\nsave_and_sync(\"checkpoints/final_model.json\", state_dict, param_keys, uchars, hyperparams;\n    adam_m=adam_m, adam_v=adam_v, step=final_step,\n    lr=lr, b1=b1, b2=b2,\n    best_val_loss=best_val_loss,\n    train_losses=train_loss_history, val_losses=val_loss_history,\n    total_steps=final_step, num_steps_target=start_step + EXTRA_STEPS - 1)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## 10. Download Checkpoint\n\nDownload the best model checkpoint to use with the inference server.\nIn Colab, use the Files panel (left sidebar) to download, or pull from HuggingFace Hub."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List saved checkpoints\n",
    "if isdir(\"checkpoints\")\n",
    "    files = readdir(\"checkpoints\")\n",
    "    println(\"Saved checkpoints:\")\n",
    "    for f in files\n",
    "        path = joinpath(\"checkpoints\", f)\n",
    "        size_kb = round(filesize(path) / 1024, digits=1)\n",
    "        println(\"  $path ($(size_kb) KB)\")\n",
    "    end\n",
    "else\n",
    "    println(\"No checkpoints directory found. Train first!\")\n",
    "end"
   ]
  }
 ]
}