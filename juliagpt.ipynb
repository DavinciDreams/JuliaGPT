{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "name": "JuliaGPT - Optimized GPT in Pure Julia",
   "include_colab_link": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": "<a href=\"https://colab.research.google.com/github/DavinciDreams/JuliaGPT/blob/main/juliagpt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# JuliaGPT â€” A Minimal GPT in Pure Julia\n\nFaithful port of Karpathy's JuliaGPT: the most atomic way to train a GPT.  \nArray-based autograd via AutoGrad.jl, transformer, Adam optimizer.  \nNo external dependencies beyond Julia stdlib + AutoGrad.jl.\n\n**Architecture** (following GPT-2 with simplifications):\n- AutoGrad.jl array-based automatic differentiation (`Param` wrapped matrices)\n- Single-layer transformer with multi-head attention\n- RMSNorm (not LayerNorm), no biases, ReLU (not GELU)\n- KV cache for natural causal masking\n- Adam optimizer with linear LR decay\n- Temperature-controlled generation\n- Best-model checkpointing with validation loss tracking\n\nBased on: https://gist.github.com/karpathy/8627fe009c40f57531cb18360106ce95"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Login & Setup\n",
    "\n",
    "This cell runs in **Python** to read your Colab secrets and set up credentials.\n",
    "\n",
    "1. Add secrets via the key icon (ðŸ”‘) in the left sidebar:\n",
    "   - `HF_TOKEN` â€” your HuggingFace access token\n",
    "   - `WANDB_KEY` â€” your Weights & Biases API key\n",
    "   - `HF_REPO` â€” your model repo (e.g. `LisaMegaWatts/JuliaGPT`)\n",
    "2. Run cells 0â€“1 (login + install Julia, ~3-5 min)\n",
    "3. **Runtime â†’ Change runtime type â†’ Julia 1.10**\n",
    "4. Continue with the remaining Julia cells"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# â”€â”€ Login to HF + W&B and fetch training data â”€â”€\n!pip install -q wandb huggingface_hub\n\nimport os, pathlib\n\n# â”€â”€ Read Colab secrets â”€â”€\nhf_token = \"\"\nwandb_key = \"\"\nhf_repo = \"\"\ntry:\n    from google.colab import userdata\n    try: hf_token = userdata.get(\"HF_TOKEN\")\n    except Exception: pass\n    try: wandb_key = userdata.get(\"WANDB_KEY\")\n    except Exception: pass\n    try: hf_repo = userdata.get(\"HF_REPO\")\n    except Exception: pass\nexcept ImportError:\n    pass\n\n# â”€â”€ Write tokens + repo to ~/.netrc (persists across kernel switch) â”€â”€\nnetrc_path = pathlib.Path.home() / \".netrc\"\nnetrc_lines = []\nif hf_token:\n    netrc_lines.extend([\"machine huggingface.co\", \"login hf\", \"password \" + hf_token, \"\"])\nif wandb_key:\n    netrc_lines.extend([\"machine api.wandb.ai\", \"login user\", \"password \" + wandb_key, \"\"])\nif hf_repo:\n    netrc_lines.extend([\"machine hf.repo\", \"login default\", \"password \" + hf_repo, \"\"])\nif netrc_lines:\n    netrc_path.write_text(chr(10).join(netrc_lines))\n    netrc_path.chmod(0o600)\n    print(f\"Credentials saved to {netrc_path}\")\nelse:\n    print(\"No secrets found. Add HF_TOKEN, WANDB_KEY, HF_REPO via key icon.\")\n\n# â”€â”€ Fetch training data from repo â”€â”€\nDATA_FILE = \"/content/training_data.txt\"\nif not os.path.exists(DATA_FILE):\n    print(\"Fetching training data...\")\n    !git clone --depth 1 --filter=blob:none --sparse https://github.com/DavinciDreams/JuliaGPT.git /content/_juliagpt 2>/dev/null || true\n    !cd /content/_juliagpt && git sparse-checkout set data 2>/dev/null || true\n    src = \"/content/_juliagpt/data/training_data.txt\"\n    if os.path.exists(src):\n        import shutil\n        shutil.copy(src, DATA_FILE)\n        print(f\"Training data: {DATA_FILE} ({os.path.getsize(DATA_FILE)/1024:.0f} KB)\")\n    else:\n        print(\"Warning: could not fetch training data from repo.\")\n        print(\"Upload training_data.txt manually to /content/\")\nelse:\n    print(f\"Training data present: {DATA_FILE}\")\n\n# â”€â”€ Mount Google Drive for persistent checkpoints â”€â”€\ntry:\n    from google.colab import drive\n    drive.mount(\"/content/drive\")\n    drive_dir = \"/content/drive/MyDrive/JuliaGPT\"\n    os.makedirs(drive_dir + \"/checkpoints\", exist_ok=True)\n    os.makedirs(drive_dir + \"/data\", exist_ok=True)\n    # Cache training data to Drive so it survives runtime restarts\n    if os.path.exists(DATA_FILE):\n        import shutil\n        shutil.copy(DATA_FILE, drive_dir + \"/data/training_data.txt\")\n    print(f\"Drive mounted: {drive_dir}/\")\nexcept Exception as e:\n    print(f\"Drive mount skipped: {e}\")\n\nprint(\"\\nDone! Now run the next cell to install Julia (~3-5 min).\")\nprint(\"Then: Runtime > Change runtime type > Julia 1.10\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1. Install Julia Kernel\n\nThis cell downloads and installs Julia + IJulia. **Takes ~3-5 minutes** on first run.\n\n**After it finishes:**\n1. Go to **Runtime â†’ Change runtime type**\n2. You may see both \"Julia\" and \"Julia 1.10\" â€” pick **Julia 1.10**\n3. Continue running the cells below"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "%%shell\nset -e\n\nJULIA_VERSION=\"1.10.5\"\nJULIA_MINOR=\"1.10\"\n\nif [ ! -d \"/usr/local/julia-${JULIA_VERSION}\" ]; then\n    echo \"Downloading Julia ${JULIA_VERSION}...\"\n    wget -q https://julialang-s3.julialang.org/bin/linux/x64/${JULIA_MINOR}/julia-${JULIA_VERSION}-linux-x86_64.tar.gz\n    tar xzf julia-${JULIA_VERSION}-linux-x86_64.tar.gz -C /usr/local/\n    rm julia-${JULIA_VERSION}-linux-x86_64.tar.gz\n    ln -sf /usr/local/julia-${JULIA_VERSION}/bin/julia /usr/local/bin/julia\n    echo \"Julia installed.\"\nelse\n    echo \"Julia already installed.\"\nfi\n\njulia -e '\n    using Pkg\n    Pkg.add(\"IJulia\")\n    Pkg.add(\"JSON3\")\n    Pkg.add(\"AutoGrad\")\n    using IJulia\n    installkernel(\"Julia\")\n'\n\necho \"\"\necho \"===========================================================\"\necho \"  Julia kernel installed!                                   \"\necho \"  Now: Runtime -> Change runtime type -> pick Julia 1.10       \"\necho \"  Then run the cells below.                              \"\necho \"===========================================================\"",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## 1b. W&B + HuggingFace Helpers (Julia)\n\nW&B logging uses a persistent Python subprocess fed JSON lines from Julia.  \nHuggingFace helpers use `huggingface-cli` to push/pull checkpoints.  \nCredentials were saved to disk by the login cell above.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# â”€â”€ Ensure pip-installed binaries (huggingface-cli, wandb) are on PATH â”€â”€\nfor p in [\"/usr/local/bin\", joinpath(homedir(), \".local/bin\"), \"/root/.local/bin\"]\n    if isdir(p) && !occursin(p, get(ENV, \"PATH\", \"\"))\n        ENV[\"PATH\"] = p * \":\" * get(ENV, \"PATH\", \"\")\n    end\nend\n\n# â”€â”€ Read credentials + config from ~/.netrc (written by Python login cell) â”€â”€\nfunction load_netrc_tokens()\n    netrc = expanduser(\"~/.netrc\")\n    tokens = Dict{String,String}()\n    if !isfile(netrc)\n        return tokens\n    end\n    lines = readlines(netrc)\n    current_machine = \"\"\n    for line in lines\n        line = strip(line)\n        m = match(r\"^machine\\s+(\\S+)\", line)\n        if m !== nothing\n            current_machine = m.captures[1]\n        end\n        m = match(r\"^password\\s+(\\S+)\", line)\n        if m !== nothing\n            tokens[current_machine] = m.captures[1]\n        end\n    end\n    return tokens\nend\n\nnetrc_tokens = load_netrc_tokens()\n\n# W&B\nwandb_key = get(netrc_tokens, \"api.wandb.ai\", \"\")\nif !isempty(wandb_key)\n    ENV[\"WANDB_API_KEY\"] = wandb_key\n    println(\"W&B API key: found\")\nelse\n    println(\"W&B API key: not found (run Python login cell first)\")\nend\n\n# HuggingFace token\nhf_token = get(netrc_tokens, \"huggingface.co\", \"\")\nif !isempty(hf_token)\n    ENV[\"HF_TOKEN\"] = hf_token\n    println(\"HF token: found\")\nelse\n    println(\"HF token: not found (run Python login cell first)\")\nend\n\n# HuggingFace repo ID\nHF_REPO_ID = get(netrc_tokens, \"hf.repo\", \"\")\nif !isempty(HF_REPO_ID)\n    println(\"HF repo: \", HF_REPO_ID)\nelse\n    println(\"HF repo: not set (add HF_REPO to Colab secrets or set HF_REPO_ID manually)\")\nend",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# W&B logging via persistent Python subprocess\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\nWANDB_PROJECT = \"microgpt-philosophy\"\nWANDB_RUN_ID = \"microgpt-\" * join(rand('a':'z', 6))\n\n# Write a tiny Python helper that reads JSON lines on stdin\nwrite(\"_wandb_log.py\", \"\"\"\nimport wandb, json, sys, os\nproject = os.environ.get(\"WANDB_PROJECT\", \"microgpt-philosophy\")\nrun_id = os.environ.get(\"WANDB_RUN_ID\", None)\nrun = wandb.init(project=project, id=run_id, resume=\"allow\",\n                 config={\"model\": \"microgpt\", \"architecture\": \"1-layer transformer\"})\nprint(f\"W&B run: {run.url}\", flush=True)\nfor line in sys.stdin:\n    line = line.strip()\n    if not line:\n        continue\n    try:\n        data = json.loads(line)\n        wandb.log(data)\n    except Exception as e:\n        print(f\"wandb log error: {e}\", file=sys.stderr, flush=True)\nwandb.finish()\n\"\"\")\n\nwandb_proc = nothing\n\nfunction wandb_init()\n    global wandb_proc, WANDB_PROJECT, WANDB_RUN_ID\n    if !haskey(ENV, \"WANDB_API_KEY\") || isempty(ENV[\"WANDB_API_KEY\"])\n        println(\"W&B: skipped (no API key)\")\n        return\n    end\n    ENV[\"WANDB_PROJECT\"] = WANDB_PROJECT\n    ENV[\"WANDB_RUN_ID\"] = WANDB_RUN_ID\n    wandb_proc = open(`python3 _wandb_log.py`, \"r+\")\n    println(\"W&B: initialized ($WANDB_PROJECT / $WANDB_RUN_ID)\")\nend\n\nfunction wandb_log(; kwargs...)\n    global wandb_proc\n    wandb_proc === nothing && return\n    metrics = Dict(string(k) => v for (k, v) in kwargs)\n    try\n        println(wandb_proc, JSON3.write(metrics))\n        flush(wandb_proc)\n    catch e\n        println(\"W&B log error: $e\")\n    end\nend\n\nfunction wandb_finish()\n    global wandb_proc\n    wandb_proc === nothing && return\n    try close(wandb_proc) catch end\n    wandb_proc = nothing\n    println(\"W&B: run finished\")\nend\n\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# HuggingFace Hub helpers\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\nfunction hf_push(repo_id::String, local_path::String; remote_path::String=\"\")\n    rp = isempty(remote_path) ? basename(local_path) : remote_path\n    run(`huggingface-cli upload $repo_id $local_path $rp`)\n    println(\"Pushed $local_path -> $repo_id/$rp\")\nend\n\nfunction hf_pull(repo_id::String, remote_path::String; local_dir::String=\"checkpoints\")\n    mkpath(local_dir)\n    run(`huggingface-cli download $repo_id $remote_path --local-dir $local_dir`)\n    println(\"Pulled $repo_id/$remote_path -> $local_dir/\")\nend\n\nfunction hf_push_checkpoint(repo_id::String; checkpoint_path::String=\"checkpoints/best_model.json\")\n    isfile(checkpoint_path) || error(\"Checkpoint not found: $checkpoint_path\")\n    hf_push(repo_id, checkpoint_path)\nend\n\nfunction hf_create_repo(repo_id::String)\n    try\n        run(`huggingface-cli repo create $repo_id --type model`)\n        println(\"Created HF repo: $repo_id\")\n    catch\n        println(\"HF repo already exists or creation skipped: $repo_id\")\n    end\nend\n\nprintln(\"W&B + HuggingFace helpers defined\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## 1. AutoGrad.jl Setup\n\nArray-based automatic differentiation using AutoGrad.jl.  \nParameters are wrapped with `Param()` for gradient tracking; `@diff` builds the computation tape and `grad()` extracts gradients."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "using Pkg\nPkg.add(\"JSON3\")\nPkg.add(\"AutoGrad\")\n\nusing Random\nusing Printf\nusing JSON3\nusing AutoGrad\nusing LinearAlgebra\n\nRandom.seed!(42)\n\nprintln(\"AutoGrad.jl loaded - array-based automatic differentiation\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# â”€â”€ Array-based neural network primitives (AutoGrad-compatible) â”€â”€\n# All operations use standard Julia array ops that AutoGrad can differentiate.\n# No in-place mutation inside @diff context.\n\n# ReLU for arrays (element-wise)\nrelu_ag(x) = max.(x, Float32(0))\n\n# RMSNorm for a vector\nfunction rmsnorm_ag(x)\n    n = length(x)\n    ms = sum(x .* x) / n\n    scale = (ms + Float32(1e-5)) ^ Float32(-0.5)\n    return x .* scale\nend\n\n# Softmax for a vector (numerically stable)\nfunction softmax_ag(logits)\n    mx = maximum(logits)\n    exps = exp.(logits .- mx)\n    s = sum(exps)\n    return exps ./ s\nend\n\nprintln(\"Array-based primitives defined (relu_ag, rmsnorm_ag, softmax_ag)\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# backward! is no longer needed -- AutoGrad.jl handles backpropagation via @diff + grad()\n# Usage:\n#   tape = @diff loss_expression    # builds computation tape\n#   g = grad(tape, some_param)      # extracts gradient for a Param\n#   loss_val = value(tape)          # extracts scalar loss value\nprintln(\"Using AutoGrad.jl for automatic differentiation (no manual backward! needed)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## 2. Dataset â€” Download & Load Training Data\n\nDownloads **54 classical philosophy texts** from Project Gutenberg and MIT Classics Archive, spanning:\n- **Plato** (12 works): Republic, Apology, Symposium, Phaedo, Crito, Meno, Phaedrus, Timaeus, Laws, Gorgias, Protagoras, Theaetetus\n- **Aristotle** (12 works): Categories, Ethics, Rhetoric, Physics, Metaphysics, Poetics, Politics, On the Soul, On the Heavens, Prior/Posterior Analytics, Topics, On Generation & Corruption\n- **Stoics** (4 works): Marcus Aurelius Meditations, Epictetus Discourses & Enchiridion, Seneca Moral Essays\n- **Roman** (4 works): Lucretius, Cicero (On Duties, Nature of Gods, On Friendship)\n- **Early Modern** (6 works): Descartes, Kant, Spinoza, Hobbes, Locke, Bacon\n- **Enlightenment/19th c.** (10 works): Hume, Rousseau, Nietzsche (x2), Mill (x2), Machiavelli, Emerson, Thoreau, Montaigne\n- **Other** (6 works): Boethius, Diogenes/Epicurus, Latin Grammar, Schopenhauer\n\nEach text is split into paragraphs to form the `TRAINING_DATA` document array.\nVocabulary is built dynamically from the data (a-z + punctuation + BOS)."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "using Downloads\n\nfunction download_and_clean(url::String, fn::String; is_gutenberg=true)\n    if !isfile(fn)\n        println(\"Downloading $fn ...\")\n        try\n            Downloads.download(url, fn)\n        catch e\n            @warn \"Download failed: $url -> $e\"\n            return \"\"\n        end\n    end\n    txt = read(fn, String)\n    if is_gutenberg\n        txt = replace(txt, r\"(?is)^.*?\\*{3}\\s*START OF (THE|THIS) PROJECT GUTENBERG.*?\\*{3}[\\r\\n]*\" => \"\")\n        txt = replace(txt, r\"(?is)\\*{3}\\s*END OF (THE|THIS) PROJECT GUTENBERG.*$\" => \"\")\n    end\n    txt = replace(txt, r\"\\r\\n\" => \"\\n\")\n    txt = replace(txt, r\"\\n{3,}\" => \"\\n\\n\")\n    return strip(txt)\nend\n\nsources = Dict(\n    # â”€â”€ Original sources (14) â”€â”€\n    \"grammar\"             => (\"https://www.gutenberg.org/files/15665/15665-0.txt\",        \"latin_grammar.txt\",         true),\n    \"categories\"          => (\"https://www.gutenberg.org/ebooks/2412.txt.utf-8\",          \"aristotle_categories.txt\",  true),\n    \"rhetoric\"            => (\"http://classics.mit.edu/Aristotle/rhetoric.mb.txt\",        \"aristotle_rhetoric.txt\",    false),\n    \"prior_analytics\"     => (\"http://classics.mit.edu/Aristotle/prior.mb.txt\",           \"prior_analytics.txt\",       false),\n    \"posterior_analytics\"  => (\"http://classics.mit.edu/Aristotle/posterior.mb.txt\",       \"posterior_analytics.txt\",   false),\n    \"topics\"              => (\"http://classics.mit.edu/Aristotle/topics.mb.txt\",          \"topics.txt\",                false),\n    \"boethius\"            => (\"https://www.gutenberg.org/files/14328/14328-0.txt\",        \"boethius_consolation.txt\",  true),\n    \"heavens\"             => (\"http://classics.mit.edu/Aristotle/heavens.mb.txt\",         \"aristotle_heavens.txt\",     false),\n    \"republic\"            => (\"https://www.gutenberg.org/files/1497/1497-0.txt\",          \"plato_republic.txt\",        true),\n    \"apology\"             => (\"https://www.gutenberg.org/files/1656/1656-0.txt\",          \"plato_apology.txt\",         true),\n    \"ethics\"              => (\"https://www.gutenberg.org/files/8438/8438-0.txt\",          \"aristotle_ethics.txt\",      true),\n    \"emerson\"             => (\"https://www.gutenberg.org/files/2944/2944-0.txt\",          \"emerson_essays.txt\",        true),\n    \"walden\"              => (\"https://www.gutenberg.org/files/205/205-0.txt\",            \"thoreau_walden.txt\",        true),\n    \"epicurus\"            => (\"https://www.gutenberg.org/files/57342/57342-0.txt\",        \"diogenes_epicurus.txt\",     true),\n    # â”€â”€ Plato (10 new) â”€â”€\n    \"plato_symposium\"     => (\"https://www.gutenberg.org/ebooks/1600.txt.utf-8\",          \"plato_symposium.txt\",       true),\n    \"plato_phaedo\"        => (\"https://www.gutenberg.org/ebooks/1658.txt.utf-8\",          \"plato_phaedo.txt\",          true),\n    \"plato_crito\"         => (\"https://www.gutenberg.org/ebooks/1657.txt.utf-8\",          \"plato_crito.txt\",           true),\n    \"plato_meno\"          => (\"https://www.gutenberg.org/ebooks/1643.txt.utf-8\",          \"plato_meno.txt\",            true),\n    \"plato_phaedrus\"      => (\"https://www.gutenberg.org/ebooks/1636.txt.utf-8\",          \"plato_phaedrus.txt\",        true),\n    \"plato_timaeus\"       => (\"https://www.gutenberg.org/ebooks/1572.txt.utf-8\",          \"plato_timaeus.txt\",         true),\n    \"plato_laws\"          => (\"https://www.gutenberg.org/ebooks/1750.txt.utf-8\",          \"plato_laws.txt\",            true),\n    \"plato_gorgias\"       => (\"https://www.gutenberg.org/ebooks/1672.txt.utf-8\",          \"plato_gorgias.txt\",         true),\n    \"plato_protagoras\"    => (\"https://www.gutenberg.org/ebooks/1591.txt.utf-8\",          \"plato_protagoras.txt\",      true),\n    \"plato_theaetetus\"    => (\"https://www.gutenberg.org/ebooks/1726.txt.utf-8\",          \"plato_theaetetus.txt\",      true),\n    # â”€â”€ Aristotle (6 new) â”€â”€\n    \"aristotle_physics\"   => (\"http://classics.mit.edu/Aristotle/physics.mb.txt\",         \"aristotle_physics.txt\",     false),\n    \"aristotle_metaphysics\" => (\"http://classics.mit.edu/Aristotle/metaphysics.mb.txt\",   \"aristotle_metaphysics.txt\", false),\n    \"aristotle_soul\"      => (\"http://classics.mit.edu/Aristotle/soul.mb.txt\",            \"aristotle_soul.txt\",        false),\n    \"aristotle_poetics\"   => (\"https://www.gutenberg.org/files/1974/1974.txt\",            \"aristotle_poetics.txt\",     true),\n    \"aristotle_politics\"  => (\"https://www.gutenberg.org/ebooks/6762.txt.utf-8\",          \"aristotle_politics.txt\",    true),\n    \"aristotle_generation\" => (\"http://classics.mit.edu/Aristotle/gener_corr.mb.txt\",     \"aristotle_generation.txt\",  false),\n    # â”€â”€ Stoics (4 new) â”€â”€\n    \"marcus_meditations\"  => (\"https://www.gutenberg.org/files/2680/2680-0.txt\",          \"marcus_meditations.txt\",    true),\n    \"epictetus_discourses\" => (\"https://www.gutenberg.org/ebooks/10661.txt.utf-8\",        \"epictetus_discourses.txt\",  true),\n    \"epictetus_enchiridion\" => (\"https://www.gutenberg.org/ebooks/45109.txt.utf-8\",       \"epictetus_enchiridion.txt\", true),\n    \"seneca_moral_essays\" => (\"https://www.gutenberg.org/ebooks/64576.txt.utf-8\",         \"seneca_moral_essays.txt\",   true),\n    # â”€â”€ Roman philosophers (4 new) â”€â”€\n    \"lucretius\"           => (\"https://www.gutenberg.org/ebooks/785.txt.utf-8\",           \"lucretius_nature.txt\",      true),\n    \"cicero_duties\"       => (\"https://www.gutenberg.org/ebooks/47001.txt.utf-8\",         \"cicero_duties.txt\",         true),\n    \"cicero_nature_gods\"  => (\"https://www.gutenberg.org/files/14988/14988.txt\",          \"cicero_nature_gods.txt\",    true),\n    \"cicero_friendship\"   => (\"https://www.gutenberg.org/ebooks/2808.txt.utf-8\",          \"cicero_friendship.txt\",     true),\n    # â”€â”€ Early Modern (6 new) â”€â”€\n    \"descartes_method\"    => (\"https://www.gutenberg.org/ebooks/59.txt.utf-8\",            \"descartes_method.txt\",      true),\n    \"descartes_meditations\" => (\"https://www.gutenberg.org/ebooks/70091.txt.utf-8\",       \"descartes_meditations.txt\", true),\n    \"kant_pure_reason\"    => (\"https://www.gutenberg.org/ebooks/4280.txt.utf-8\",          \"kant_pure_reason.txt\",      true),\n    \"spinoza_ethics\"      => (\"https://www.gutenberg.org/ebooks/3800.txt.utf-8\",          \"spinoza_ethics.txt\",        true),\n    \"hobbes_leviathan\"    => (\"https://www.gutenberg.org/ebooks/3207.txt.utf-8\",          \"hobbes_leviathan.txt\",      true),\n    \"locke_government\"    => (\"https://www.gutenberg.org/ebooks/7370.txt.utf-8\",          \"locke_government.txt\",      true),\n    # â”€â”€ Enlightenment & 19th century (8 new) â”€â”€\n    \"hume_understanding\"  => (\"https://www.gutenberg.org/ebooks/9662.txt.utf-8\",          \"hume_understanding.txt\",    true),\n    \"rousseau_social_contract\" => (\"https://www.gutenberg.org/files/46333/46333-0.txt\",   \"rousseau_social_contract.txt\", true),\n    \"nietzsche_beyond\"    => (\"https://www.gutenberg.org/ebooks/4363.txt.utf-8\",          \"nietzsche_beyond.txt\",      true),\n    \"nietzsche_zarathustra\" => (\"https://www.gutenberg.org/ebooks/1998.txt.utf-8\",        \"nietzsche_zarathustra.txt\", true),\n    \"mill_liberty\"        => (\"https://www.gutenberg.org/ebooks/34901.txt.utf-8\",         \"mill_liberty.txt\",          true),\n    \"mill_utilitarianism\" => (\"https://www.gutenberg.org/ebooks/11224.txt.utf-8\",         \"mill_utilitarianism.txt\",   true),\n    \"machiavelli_prince\"  => (\"https://www.gutenberg.org/files/57037/57037-0.txt\",        \"machiavelli_prince.txt\",    true),\n    \"bacon_essays\"        => (\"https://www.gutenberg.org/ebooks/575.txt.utf-8\",           \"bacon_essays.txt\",          true),\n    # â”€â”€ Essays (2 new) â”€â”€\n    \"montaigne_essays\"    => (\"https://www.gutenberg.org/ebooks/3600.txt.utf-8\",          \"montaigne_essays.txt\",      true),\n    \"schopenhauer_essays\" => (\"https://www.gutenberg.org/ebooks/11945.txt.utf-8\",         \"schopenhauer_essays.txt\",   true)\n)\n\n# Download all texts\ntexts = Dict{String,String}()\nfor (key, (url, fn, is_gut)) in sources\n    texts[key] = download_and_clean(url, fn; is_gutenberg=is_gut)\nend\nprintln(\"Downloaded $(length(texts)) texts.\")\n\n# Unicode normalization\nfor (_, (_, fn, _)) in sources\n    isfile(fn) || continue\n    txt = read(fn, String)\n    txt = replace(txt,\n        \"\\u201c\" => \"\\\"\", \"\\u201d\" => \"\\\"\",\n        \"\\u2018\" => \"'\", \"\\u2019\" => \"'\",\n        \"\\u2014\" => \"--\", \"\\u2013\" => \"-\",\n        \"\\u2026\" => \"...\", \"\\u00A0\" => \" \"\n    )\n    txt = replace(txt, r\"\\n{3,}\" => \"\\n\\n\")\n    open(fn, \"w\") do io; write(io, strip(txt)); end\nend\n\n# Split all texts into paragraphs -> TRAINING_DATA array\nTRAINING_DATA = String[]\nfor (_, (_, fn, _)) in sources\n    isfile(fn) || continue\n    txt = read(fn, String)\n    # Normalize to lowercase, keep a-z, space, period, newlines\n    txt = lowercase(txt)\n    txt = replace(txt, r\"[^a-z \\.\\n]\" => \" \")\n    txt = replace(txt, r\"  +\" => \" \")\n    # Split on blank lines into paragraphs\n    paragraphs = split(txt, r\"\\n\\n+\")\n    for p in paragraphs\n        cleaned = strip(replace(String(p), r\"\\n\" => \" \"))\n        cleaned = replace(cleaned, r\"  +\" => \" \")\n        # Only keep paragraphs with meaningful content (20+ chars)\n        if length(cleaned) >= 20\n            # Truncate very long paragraphs to block_size-friendly chunks (~512 chars)\n            while length(cleaned) > 512\n                # Find a sentence break near 512\n                cutoff = min(512, length(cleaned))\n                dot_pos = findlast('.', cleaned[1:cutoff])\n                if dot_pos !== nothing && dot_pos > 100\n                    push!(TRAINING_DATA, strip(cleaned[1:dot_pos]))\n                    cleaned = strip(cleaned[dot_pos+1:end])\n                else\n                    push!(TRAINING_DATA, strip(cleaned[1:cutoff]))\n                    cleaned = strip(cleaned[cutoff+1:end])\n                end\n            end\n            if length(cleaned) >= 20\n                push!(TRAINING_DATA, cleaned)\n            end\n        end\n    end\nend\n\nprintln(\"TRAINING_DATA: $(length(TRAINING_DATA)) paragraphs from 54 philosophy texts\")\nprintln(\"Total characters: $(sum(length, TRAINING_DATA))\")\nprintln(\"Sample: \\\"$(TRAINING_DATA[1][1:min(80, end)])...\\\"\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## 3. Neural Network Helpers\n\nWeight matrices are `Param(Matrix{Float32})` -- AutoGrad.jl tracks gradients through array operations automatically.  \nHelper functions for parameter initialization, key ordering, and checkpoint support."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Helper: deterministic parameter key ordering (for checkpoint serialization)\nfunction get_param_keys(n_layer::Int)\n    keys = [\"wte\", \"wpe\", \"lm_head\"]\n    for i in 0:n_layer-1\n        append!(keys, [\n            \"layer$i.attn_wq\", \"layer$i.attn_wk\", \"layer$i.attn_wv\", \"layer$i.attn_wo\",\n            \"layer$i.mlp_fc1\", \"layer$i.mlp_fc2\"\n        ])\n    end\n    return keys\nend\n\n# Helper: initialize a Param-wrapped weight matrix\nfunction init_param(nout::Int, nin::Int; std=0.08f0)\n    Param(randn(Float32, nout, nin) .* std)\nend\n\n# Helper: collect all Param objects from state_dict in deterministic order\nfunction collect_params(state_dict, param_keys)\n    ps = []\n    for key in param_keys\n        push!(ps, state_dict[key])\n    end\n    return ps\nend\n\nprintln(\"Neural network helpers defined (get_param_keys, init_param, collect_params)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## 4. GPT Forward Pass\n\nProcesses one token at a time with KV cache for causal masking.  \nAll operations use array matrix multiplications via AutoGrad.jl.  \nKV cache stores detached (plain Float32) vectors -- gradients flow only through the current token."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "function gpt(token_id::Int, pos_id::Int,\n             kv_keys::Vector{Vector{Vector{Float32}}},\n             kv_values::Vector{Vector{Vector{Float32}}},\n             state_dict,\n             n_layer::Int, n_head::Int, head_dim::Int)\n\n    d_model = n_head * head_dim\n\n    # Embedding lookup: row indexing into Param matrices\n    # wte is (vocab_size, d_model), wpe is (block_size, d_model)\n    tok_emb = state_dict[\"wte\"][token_id, :]\n    pos_emb = state_dict[\"wpe\"][pos_id, :]\n    x = tok_emb .+ pos_emb\n    x = rmsnorm_ag(x)\n\n    for li in 0:n_layer-1\n        x_res = x\n        x = rmsnorm_ag(x)\n\n        # Linear projections: W * x where W is (d_model, d_model)\n        q = state_dict[\"layer$(li).attn_wq\"] * x\n        k = state_dict[\"layer$(li).attn_wk\"] * x\n        v = state_dict[\"layer$(li).attn_wv\"] * x\n\n        # Detach K, V for cache storage (gradients don't flow through cached entries)\n        k_detached = Float32.(value(k))\n        v_detached = Float32.(value(v))\n        push!(kv_keys[li+1], k_detached)\n        push!(kv_values[li+1], v_detached)\n\n        # Build K and V matrices from cache: (d_model, n_cached)\n        n_cached = length(kv_keys[li+1])\n        K_mat = hcat(kv_keys[li+1]...)\n        V_mat = hcat(kv_values[li+1]...)\n\n        # Multi-head attention\n        head_outputs = Vector{Any}(undef, n_head)\n        for h in 0:n_head-1\n            hs = h * head_dim + 1\n            he = hs + head_dim - 1\n\n            q_h = q[hs:he]                   # (head_dim,) tracked\n            K_h = K_mat[hs:he, :]             # (head_dim, n_cached) plain Float32\n            V_h = V_mat[hs:he, :]             # (head_dim, n_cached) plain Float32\n\n            # Attention scores: K_h' * q_h / sqrt(head_dim)\n            # (n_cached, head_dim) * (head_dim,) = (n_cached,) -- tracked through q_h\n            scores = (K_h' * q_h) ./ Float32(sqrt(head_dim))\n            attn_w = softmax_ag(scores)\n\n            # Weighted sum: V_h * attn_w = (head_dim, n_cached) * (n_cached,) = (head_dim,)\n            head_outputs[h+1] = V_h * attn_w\n        end\n\n        x_attn = vcat(head_outputs...)    # (d_model,) tracked\n\n        # Output projection\n        x = state_dict[\"layer$(li).attn_wo\"] * x_attn\n        x = x .+ x_res\n\n        # MLP block\n        x_res = x\n        x = rmsnorm_ag(x)\n        x = state_dict[\"layer$(li).mlp_fc1\"] * x\n        x = relu_ag(x)\n        x = state_dict[\"layer$(li).mlp_fc2\"] * x\n        x = x .+ x_res\n    end\n\n    # LM head: (vocab_size, d_model) * (d_model,) = (vocab_size,)\n    logits = state_dict[\"lm_head\"] * x\n    return logits\nend\n\nprintln(\"GPT forward pass defined (array-based with AutoGrad.jl)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Checkpoint Save/Load\n",
    "\n",
    "Save and load model weights + optimizer state as JSON.  \n",
    "Checkpoints saved to Google Drive persist across Colab sessions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "function save_checkpoint(path::String, state_dict, param_keys, uchars, hyperparams;\n                         adam_m=nothing, adam_v=nothing, step::Int=0,\n                         lr::Float64=0.01, b1::Float64=0.85, b2::Float64=0.99,\n                         best_val_loss::Float64=Inf,\n                         train_losses::Vector{Float64}=Float64[],\n                         val_losses::Vector{Float64}=Float64[],\n                         total_steps::Int=0, num_steps_target::Int=0)\n\n    sd_data = Dict{String,Any}()\n    for k in param_keys\n        W = value(state_dict[k])  # unwrap Param -> Matrix{Float32}\n        sd_data[k] = [Float64.(W[i, :]) for i in 1:size(W, 1)]\n    end\n\n    # Serialize Adam state per param key\n    adam_m_data = Dict{String,Any}()\n    adam_v_data = Dict{String,Any}()\n    if adam_m !== nothing\n        for k in param_keys\n            if haskey(adam_m, k)\n                adam_m_data[k] = [Float64.(adam_m[k][i, :]) for i in 1:size(adam_m[k], 1)]\n                adam_v_data[k] = [Float64.(adam_v[k][i, :]) for i in 1:size(adam_v[k], 1)]\n            end\n        end\n    end\n\n    checkpoint = Dict{String,Any}(\n        \"format\" => \"autograd_v2\",\n        \"uchars\" => [string(c) for c in uchars],\n        \"hyperparams\" => hyperparams,\n        \"state_dict\" => sd_data,\n        \"optimizer\" => Dict{String,Any}(\n            \"adam_m\" => adam_m_data,\n            \"adam_v\" => adam_v_data,\n            \"step\" => step,\n            \"lr\" => lr,\n            \"beta1\" => b1,\n            \"beta2\" => b2\n        ),\n        \"training\" => Dict{String,Any}(\n            \"best_val_loss\" => best_val_loss,\n            \"train_losses\" => train_losses,\n            \"val_losses\" => val_losses,\n            \"total_steps_completed\" => total_steps,\n            \"num_steps_target\" => num_steps_target\n        )\n    )\n\n    mkpath(dirname(path))\n    open(path, \"w\") do f\n        JSON3.write(f, checkpoint)\n    end\n    vl_str = best_val_loss == Inf ? \"Inf\" : @sprintf(\"%.4f\", best_val_loss)\n    println(\"Checkpoint saved: $path (step $step, best_val_loss=$vl_str)\")\nend\n\nfunction load_checkpoint(path::String)\n    println(\"Loading checkpoint from $path ...\")\n    raw = JSON3.read(read(path, String))\n\n    uchars = [only(String(s)) for s in raw[\"uchars\"]]\n    BOS = length(uchars) + 1\n    vocab_size = BOS\n\n    hp = raw[\"hyperparams\"]\n    n_layer = Int(hp[\"n_layer\"])\n    n_embd = Int(hp[\"n_embd\"])\n    block_size = Int(hp[\"block_size\"])\n    n_head = Int(hp[\"n_head\"])\n    head_dim = n_embd Ã· n_head\n\n    # Detect format version\n    fmt = haskey(raw, \"format\") ? String(raw[\"format\"]) : \"v1\"\n\n    state_dict = Dict{String, Any}()\n    for (key, matrix_rows) in pairs(raw[\"state_dict\"])\n        rows = [Float32.(collect(row)) for row in matrix_rows]\n        W = vcat([reshape(r, 1, :) for r in rows]...)  # stack rows into Matrix\n        state_dict[string(key)] = Param(W)\n    end\n\n    # Load Adam state\n    opt_raw = raw[\"optimizer\"]\n    adam_m = Dict{String, Matrix{Float32}}()\n    adam_v = Dict{String, Matrix{Float32}}()\n\n    if fmt == \"autograd_v2\" && haskey(opt_raw, \"adam_m\")\n        am_raw = opt_raw[\"adam_m\"]\n        av_raw = opt_raw[\"adam_v\"]\n        if !isempty(am_raw)\n            for (key, matrix_rows) in pairs(am_raw)\n                rows = [Float32.(collect(row)) for row in matrix_rows]\n                adam_m[string(key)] = vcat([reshape(r, 1, :) for r in rows]...)\n            end\n            for (key, matrix_rows) in pairs(av_raw)\n                rows = [Float32.(collect(row)) for row in matrix_rows]\n                adam_v[string(key)] = vcat([reshape(r, 1, :) for r in rows]...)\n            end\n        end\n    end\n\n    step = Int(opt_raw[\"step\"])\n    lr = Float64(opt_raw[\"lr\"])\n    b1 = Float64(opt_raw[\"beta1\"])\n    b2 = Float64(opt_raw[\"beta2\"])\n\n    trn = raw[\"training\"]\n    best_val_loss = Float64(trn[\"best_val_loss\"])\n    train_losses = Float64.(collect(trn[\"train_losses\"]))\n    val_losses = Float64.(collect(trn[\"val_losses\"]))\n    total_steps = Int(trn[\"total_steps_completed\"])\n    num_steps_target = Int(trn[\"num_steps_target\"])\n\n    println(\"  vocab=$vocab_size, embd=$n_embd, layers=$n_layer, step=$step\")\n\n    return (;\n        state_dict, uchars, BOS, vocab_size,\n        n_layer, n_embd, block_size, n_head, head_dim,\n        adam_m, adam_v, step, lr, b1, b2,\n        best_val_loss, train_losses, val_losses,\n        total_steps, num_steps_target\n    )\nend\n\nprintln(\"Checkpoint save/load defined (autograd_v2 format)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Setup â€” Dataset, Tokenizer, Parameters\n",
    "\n",
    "Character-level tokenizer with a BOS token. 90/10 train/val split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# â”€â”€ Dataset with train/val split (ordered, not shuffled) â”€â”€\ndocs = copy(TRAINING_DATA)\nsplit_idx = max(1, Int(floor(0.9 * length(docs))))\ntrain_docs = docs[1:split_idx]\nval_docs = docs[split_idx+1:end]\nif isempty(val_docs)\n    val_docs = docs[max(1, end-4):end]\n    train_docs = docs[1:max(1, end-5)]\nend\nprintln(\"train: $(length(train_docs)) docs, val: $(length(val_docs)) docs\")\n\n# â”€â”€ Tokenizer â”€â”€\nuchars = sort(unique(join(docs)))\nBOS = length(uchars) + 1\nvocab_size = BOS\nprintln(\"vocab size: $vocab_size ($(length(uchars)) chars + BOS)\")\n\n# â”€â”€ Hyperparameters â”€â”€\nn_layer    = 1\nn_embd     = 16\nblock_size = 256\nn_head     = 4\nhead_dim   = n_embd Ã· n_head\n\nhyperparams = Dict{String,Any}(\n    \"n_layer\" => n_layer, \"n_embd\" => n_embd,\n    \"block_size\" => block_size, \"n_head\" => n_head\n)\n\n# â”€â”€ Initialize parameters as Param(Matrix{Float32}) â”€â”€\nstate_dict = Dict{String, Any}()\nstate_dict[\"wte\"]     = init_param(vocab_size, n_embd)   # (vocab_size, d_model)\nstate_dict[\"wpe\"]     = init_param(block_size, n_embd)   # (block_size, d_model)\nstate_dict[\"lm_head\"] = init_param(vocab_size, n_embd)   # (vocab_size, d_model)\nfor i in 0:n_layer-1\n    state_dict[\"layer$i.attn_wq\"]  = init_param(n_embd, n_embd)\n    state_dict[\"layer$i.attn_wk\"]  = init_param(n_embd, n_embd)\n    state_dict[\"layer$i.attn_wv\"]  = init_param(n_embd, n_embd)\n    state_dict[\"layer$i.attn_wo\"]  = init_param(n_embd, n_embd)\n    state_dict[\"layer$i.mlp_fc1\"]  = init_param(4 * n_embd, n_embd)\n    state_dict[\"layer$i.mlp_fc2\"]  = init_param(n_embd, 4 * n_embd)\nend\n\nparam_keys = get_param_keys(n_layer)\nall_params = collect_params(state_dict, param_keys)\ntotal_num_params = sum(length(value(p)) for p in all_params)\nprintln(\"num params: $total_num_params ($(length(all_params)) weight matrices)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Training Loop with Validation + Best-Model Checkpointing\n",
    "\n",
    "Adam optimizer with linear LR decay.  \n",
    "Validates every 50 steps, saves `best_model.json` when val loss improves.  \n",
    "Periodic checkpoints every 200 steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "function compute_val_loss(val_docs, uchars, BOS, block_size, state_dict, n_layer, n_head, head_dim)\n    total_loss = 0.0\n    total_tokens = 0\n    for doc in val_docs\n        tokens = vcat([BOS], [findfirst(==(ch), uchars) for ch in doc], [BOS])\n        n = min(block_size, length(tokens) - 1)\n        kv_keys = [Vector{Vector{Float32}}() for _ in 1:n_layer]\n        kv_vals = [Vector{Vector{Float32}}() for _ in 1:n_layer]\n        for pos in 1:n\n            token_id = tokens[pos]\n            target_id = tokens[pos + 1]\n            # Outside @diff, Params behave like plain arrays\n            logits = gpt(token_id, pos, kv_keys, kv_vals, state_dict, n_layer, n_head, head_dim)\n            probs = softmax_ag(logits)\n            p_val = Float64.(value(probs))\n            total_loss += -log(max(p_val[target_id], 1e-10))\n            total_tokens += 1\n        end\n    end\n    return total_loss / max(total_tokens, 1)\nend\n\nprintln(\"compute_val_loss defined\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# â”€â”€ Adam optimizer state (per-parameter matrices) â”€â”€\nlr, b1, b2, eps = 0.01, 0.85, 0.99, 1e-8\n\nadam_m = Dict{String, Matrix{Float32}}()\nadam_v = Dict{String, Matrix{Float32}}()\nfor k in param_keys\n    sz = size(value(state_dict[k]))\n    adam_m[k] = zeros(Float32, sz)\n    adam_v[k] = zeros(Float32, sz)\nend\n\nbest_val_loss = Inf\ntrain_loss_history = Float64[]\nval_loss_history = Float64[]\n\n# â”€â”€ Checkpoint paths (local + Drive for persistence) â”€â”€\nLOCAL_CKPT = \"checkpoints\"\nDRIVE_CKPT = \"/content/drive/MyDrive/JuliaGPT/checkpoints\"\nmkpath(LOCAL_CKPT)\n\nfunction save_to_drive(local_path::String)\n    if isdir(DRIVE_CKPT)\n        cp(local_path, joinpath(DRIVE_CKPT, basename(local_path)), force=true)\n    end\nend\n\nfunction save_and_sync(path, sd, pk, uc, hp; kwargs...)\n    save_checkpoint(path, sd, pk, uc, hp; kwargs...)\n    save_to_drive(path)\nend\n\n# â”€â”€ Initialize W&B logging (if API key is set) â”€â”€\nif haskey(ENV, \"WANDB_API_KEY\") && !isempty(ENV[\"WANDB_API_KEY\"])\n    wandb_init()\nend\n\n# â”€â”€ Training loop â”€â”€\nNUM_EPOCHS = 3\nnum_steps = clamp(NUM_EPOCHS * length(train_docs), 1000, 50000)\nprintln(\"--- training $num_steps steps (~$(NUM_EPOCHS) epochs over $(length(train_docs)) docs) ---\")\nprintln(\"    Local: $LOCAL_CKPT/  |  Drive: $(isdir(DRIVE_CKPT) ? DRIVE_CKPT : \"(not mounted)\")\")\nt_start = time()\nlast_save_time = time()\nSAVE_INTERVAL = 600  # auto-save every 10 min\ncompleted_steps = 0\n\ntry\n    for step in 1:num_steps\n        global completed_steps = step\n        doc = train_docs[mod1(step, length(train_docs))]\n        tokens = vcat([BOS], [findfirst(==(ch), uchars) for ch in doc], [BOS])\n        n = min(block_size, length(tokens) - 1)\n\n        # Zero gradient accumulators\n        grad_accum = Dict{String, Matrix{Float32}}()\n        for k in param_keys\n            grad_accum[k] = zeros(Float32, size(value(state_dict[k])))\n        end\n\n        kv_keys = [Vector{Vector{Float32}}() for _ in 1:n_layer]\n        kv_vals = [Vector{Vector{Float32}}() for _ in 1:n_layer]\n        total_loss_val = 0.0\n\n        # Forward pass: one @diff per token position (KV cache mutation between positions)\n        for pos in 1:n\n            token_id  = tokens[pos]\n            target_id = tokens[pos + 1]\n\n            tape = @diff begin\n                logits = gpt(token_id, pos, kv_keys, kv_vals, state_dict, n_layer, n_head, head_dim)\n                probs = softmax_ag(logits)\n                -log(probs[target_id])\n            end\n\n            total_loss_val += value(tape)\n\n            # Accumulate gradients across positions\n            for k in param_keys\n                g = grad(tape, state_dict[k])\n                if g !== nothing\n                    # AutoGrad may return Sparse for getindex grads; convert to dense\n                    g_dense = try\n                        Float32.(AutoGrad.full(g))\n                    catch\n                        Float32.(g)\n                    end\n                    grad_accum[k] .+= g_dense\n                end\n            end\n        end\n\n        avg_loss = total_loss_val / n\n        push!(train_loss_history, avg_loss)\n\n        # Average gradients and apply Adam update (once per document)\n        lr_t = lr * (1 - (step - 1) / num_steps)\n        for k in param_keys\n            g_avg = grad_accum[k] ./ Float32(n)\n            adam_m[k] .= Float32(b1) .* adam_m[k] .+ Float32(1 - b1) .* g_avg\n            adam_v[k] .= Float32(b2) .* adam_v[k] .+ Float32(1 - b2) .* g_avg .^ 2\n            m_hat = adam_m[k] ./ Float32(1 - b1^step)\n            v_hat = adam_v[k] ./ Float32(1 - b2^step)\n            # Update param value in-place (outside @diff context)\n            value(state_dict[k]) .-= Float32(lr_t) .* m_hat ./ (sqrt.(v_hat) .+ Float32(eps))\n        end\n\n        # Validate + checkpoint every 50 steps\n        if step % 50 == 0\n            val_loss = compute_val_loss(val_docs, uchars, BOS, block_size, state_dict, n_layer, n_head, head_dim)\n            push!(val_loss_history, val_loss)\n            elapsed = time() - t_start\n            wandb_log(; step=step, train_loss=avg_loss, val_loss=val_loss, lr=lr_t)\n\n            improved = \"\"\n            if val_loss < best_val_loss\n                best_val_loss = val_loss\n                save_and_sync(\"checkpoints/best_model.json\", state_dict, param_keys, uchars, hyperparams;\n                    adam_m=adam_m, adam_v=adam_v, step=step,\n                    lr=lr, b1=b1, b2=b2,\n                    best_val_loss=best_val_loss,\n                    train_losses=train_loss_history, val_losses=val_loss_history,\n                    total_steps=step, num_steps_target=num_steps)\n                improved = \" << new best!\"\n            end\n\n            @printf(\"step %4d / %4d | train %.4f | val %.4f | %.1fs%s\\n\",\n                    step, num_steps, avg_loss, val_loss, elapsed, improved)\n        elseif step % 10 == 0\n            elapsed = time() - t_start\n            @printf(\"step %4d / %4d | train %.4f | %.1fs\\n\", step, num_steps, avg_loss, elapsed)\n        end\n\n        # Periodic checkpoint every 100 steps -> Drive\n        if step % 100 == 0\n            save_and_sync(\"checkpoints/checkpoint_latest.json\", state_dict, param_keys, uchars, hyperparams;\n                adam_m=adam_m, adam_v=adam_v, step=step,\n                lr=lr, b1=b1, b2=b2,\n                best_val_loss=best_val_loss,\n                train_losses=train_loss_history, val_losses=val_loss_history,\n                total_steps=step, num_steps_target=num_steps)\n            last_save_time = time()\n        end\n\n        # Time-based auto-save every 10 min\n        if time() - last_save_time > SAVE_INTERVAL\n            save_and_sync(\"checkpoints/checkpoint_latest.json\", state_dict, param_keys, uchars, hyperparams;\n                adam_m=adam_m, adam_v=adam_v, step=step,\n                lr=lr, b1=b1, b2=b2,\n                best_val_loss=best_val_loss,\n                train_losses=train_loss_history, val_losses=val_loss_history,\n                total_steps=step, num_steps_target=num_steps)\n            last_save_time = time()\n            println(\"  [auto-save at step $step]\")\n        end\n    end\ncatch e\n    if e isa InterruptException\n        println(\"\\n\\nTraining interrupted at step $completed_steps!\")\n    else\n        println(\"\\n\\nTraining error at step $completed_steps: $e\")\n    end\n    println(\"Saving emergency checkpoint...\")\n    save_and_sync(\"checkpoints/checkpoint_interrupted.json\", state_dict, param_keys, uchars, hyperparams;\n        adam_m=adam_m, adam_v=adam_v, step=completed_steps,\n        lr=lr, b1=b1, b2=b2,\n        best_val_loss=best_val_loss,\n        train_losses=train_loss_history, val_losses=val_loss_history,\n        total_steps=completed_steps, num_steps_target=num_steps)\n    if !(e isa InterruptException)\n        rethrow(e)\n    end\nend\n\nelapsed = time() - t_start\n@printf(\"\\ntraining complete in %.1f seconds\\n\", elapsed)\nwandb_finish()\n\n# Final save -> local + Drive\nsave_and_sync(\"checkpoints/final_model.json\", state_dict, param_keys, uchars, hyperparams;\n    adam_m=adam_m, adam_v=adam_v, step=num_steps,\n    lr=lr, b1=b1, b2=b2,\n    best_val_loss=best_val_loss,\n    train_losses=train_loss_history, val_losses=val_loss_history,\n    total_steps=num_steps, num_steps_target=num_steps)\n\nprintln(\"\\nCheckpoints saved to local + Drive. Best val loss: $(@sprintf(\"%.4f\", best_val_loss))\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Inference â€” Hallucinated Philosophy\n",
    "\n",
    "Generate new philosophy-like text using temperature-controlled sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "function generate_text(state_dict, uchars, BOS, n_layer, n_head, head_dim, block_size;\n                       temperature=0.8, max_tokens=128)\n    kv_keys = [Vector{Vector{Float32}}() for _ in 1:n_layer]\n    kv_vals = [Vector{Vector{Float32}}() for _ in 1:n_layer]\n    token_id = BOS\n    sample = Char[]\n    limit = min(max_tokens, block_size)\n    for pos in 1:limit\n        # No @diff needed for inference -- Params act like plain arrays outside @diff\n        logits = gpt(token_id, pos, kv_keys, kv_vals, state_dict, n_layer, n_head, head_dim)\n        scaled = logits ./ temperature\n        probs = softmax_ag(scaled)\n        weights = Float64.(value(probs))\n\n        # Categorical sampling\n        r = rand()\n        cum = 0.0\n        token_id = 1\n        for (idx, w) in enumerate(weights)\n            cum += w\n            if r <= cum\n                token_id = idx\n                break\n            end\n        end\n        token_id == BOS && break\n        push!(sample, uchars[token_id])\n    end\n    return String(sample)\nend\n\nprintln(\"--- inference (hallucinated philosophy) ---\")\nfor i in 1:20\n    text = generate_text(state_dict, uchars, BOS, n_layer, n_head, head_dim, block_size; temperature=0.8)\n    @printf(\"sample %2d: %s\\n\", i, text)\nend"
  },
  {
   "cell_type": "markdown",
   "source": "---\n## 8a. Push Model to HuggingFace Hub\n\nPush your trained checkpoint to HuggingFace for persistence across Colab sessions.  \nSet `HF_REPO_ID` in the login cell above (e.g. `\"yourusername/microgpt-philosophy\"`).",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Push checkpoint to HuggingFace Hub\n# Make sure HF_REPO_ID is set in the login cell above\n\nif @isdefined(HF_REPO_ID) && !isempty(HF_REPO_ID)\n    # Create repo if it doesn't exist yet\n    hf_create_repo(HF_REPO_ID)\n\n    # Push best model checkpoint\n    if isfile(\"checkpoints/best_model.json\")\n        hf_push_checkpoint(HF_REPO_ID; checkpoint_path=\"checkpoints/best_model.json\")\n    else\n        println(\"No best_model.json found â€” train first!\")\n    end\n\n    # Also push final model if it exists\n    if isfile(\"checkpoints/final_model.json\")\n        hf_push(HF_REPO_ID, \"checkpoints/final_model.json\")\n    end\n\n    println(\"\\nDone! View your model at: https://huggingface.co/$HF_REPO_ID\")\nelse\n    println(\"Set HF_REPO_ID in the login cell (e.g. \\\"yourusername/microgpt-philosophy\\\")\")\nend",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n## 8b. Pull Checkpoint from HuggingFace Hub\n\nDownload a previously pushed checkpoint to resume training in a new Colab session.  \nRun this before the Resume Training cell below.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Pull checkpoint from HuggingFace to resume training\n# Make sure HF_REPO_ID is set in the login cell above\n\nif @isdefined(HF_REPO_ID) && !isempty(HF_REPO_ID)\n    mkpath(\"checkpoints\")\n    hf_pull(HF_REPO_ID, \"best_model.json\"; local_dir=\"checkpoints\")\n    println(\"\\nReady to resume from checkpoints/best_model.json\")\n    println(\"Run the 'Resume Training' cell below.\")\nelse\n    println(\"Set HF_REPO_ID in the login cell (e.g. \\\"yourusername/microgpt-philosophy\\\")\")\nend",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Resume Training from Checkpoint\n",
    "\n",
    "Load a saved checkpoint and continue training for more steps.  \n",
    "Skip this cell if you're training from scratch above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# â”€â”€ Load checkpoint â”€â”€\n# Change the path to load a different checkpoint\nRESUME_FROM = \"checkpoints/best_model.json\"\nEXTRA_STEPS = clamp(length(train_docs), 500, 25000)  # ~1 extra epoch\n\nckpt = load_checkpoint(RESUME_FROM)\nstate_dict = ckpt.state_dict\nuchars = ckpt.uchars\nBOS = ckpt.BOS\nn_layer = ckpt.n_layer\nn_embd = ckpt.n_embd\nblock_size = ckpt.block_size\nn_head = ckpt.n_head\nhead_dim = ckpt.head_dim\n\nhyperparams = Dict{String,Any}(\n    \"n_layer\" => n_layer, \"n_embd\" => n_embd,\n    \"block_size\" => block_size, \"n_head\" => n_head\n)\n\n# Reconstruct dataset and split (ordered, not shuffled)\ndocs = copy(TRAINING_DATA)\nsplit_idx = max(1, Int(floor(0.9 * length(docs))))\ntrain_docs = docs[1:split_idx]\nval_docs = docs[split_idx+1:end]\nif isempty(val_docs)\n    val_docs = docs[max(1, end-4):end]\n    train_docs = docs[1:max(1, end-5)]\nend\n\nparam_keys = get_param_keys(n_layer)\n\n# Restore optimizer (Adam state per param key)\nlr = ckpt.lr; b1 = ckpt.b1; b2 = ckpt.b2; eps = 1e-8\nadam_m = if !isempty(ckpt.adam_m)\n    ckpt.adam_m\nelse\n    Dict{String, Matrix{Float32}}(k => zeros(Float32, size(value(state_dict[k]))) for k in param_keys)\nend\nadam_v = if !isempty(ckpt.adam_v)\n    ckpt.adam_v\nelse\n    Dict{String, Matrix{Float32}}(k => zeros(Float32, size(value(state_dict[k]))) for k in param_keys)\nend\n\nstart_step = ckpt.step + 1\nend_step = ckpt.step + EXTRA_STEPS\nbest_val_loss = ckpt.best_val_loss\ntrain_loss_history = copy(ckpt.train_losses)\nval_loss_history = copy(ckpt.val_losses)\n\n# â”€â”€ Initialize W&B logging (if API key is set) â”€â”€\nif haskey(ENV, \"WANDB_API_KEY\") && !isempty(ENV[\"WANDB_API_KEY\"])\n    wandb_init()\nend\n\nprintln(\"\\nResuming from step $(ckpt.step) -> training to step $end_step\")\nprintln(\"Best val loss so far: $(round(best_val_loss, digits=4))\")\nt_start = time()\n\nfor step in start_step:end_step\n    doc = train_docs[mod1(step, length(train_docs))]\n    tokens = vcat([BOS], [findfirst(==(ch), uchars) for ch in doc], [BOS])\n    n = min(block_size, length(tokens) - 1)\n\n    # Zero gradient accumulators\n    grad_accum = Dict{String, Matrix{Float32}}()\n    for k in param_keys\n        grad_accum[k] = zeros(Float32, size(value(state_dict[k])))\n    end\n\n    kv_keys = [Vector{Vector{Float32}}() for _ in 1:n_layer]\n    kv_vals = [Vector{Vector{Float32}}() for _ in 1:n_layer]\n    total_loss_val = 0.0\n\n    for pos in 1:n\n        token_id  = tokens[pos]\n        target_id = tokens[pos + 1]\n\n        tape = @diff begin\n            logits = gpt(token_id, pos, kv_keys, kv_vals, state_dict, n_layer, n_head, head_dim)\n            probs = softmax_ag(logits)\n            -log(probs[target_id])\n        end\n\n        total_loss_val += value(tape)\n\n        for k in param_keys\n            g = grad(tape, state_dict[k])\n            if g !== nothing\n                g_dense = try\n                    Float32.(AutoGrad.full(g))\n                catch\n                    Float32.(g)\n                end\n                grad_accum[k] .+= g_dense\n            end\n        end\n    end\n\n    avg_loss = total_loss_val / n\n    push!(train_loss_history, avg_loss)\n\n    # Adam update\n    lr_t = lr * (1 - (step - 1) / end_step)\n    for k in param_keys\n        g_avg = grad_accum[k] ./ Float32(n)\n        adam_m[k] .= Float32(b1) .* adam_m[k] .+ Float32(1 - b1) .* g_avg\n        adam_v[k] .= Float32(b2) .* adam_v[k] .+ Float32(1 - b2) .* g_avg .^ 2\n        m_hat = adam_m[k] ./ Float32(1 - b1^step)\n        v_hat = adam_v[k] ./ Float32(1 - b2^step)\n        value(state_dict[k]) .-= Float32(lr_t) .* m_hat ./ (sqrt.(v_hat) .+ Float32(eps))\n    end\n\n    if step % 50 == 0\n        val_loss = compute_val_loss(val_docs, uchars, BOS, block_size, state_dict, n_layer, n_head, head_dim)\n        push!(val_loss_history, val_loss)\n        elapsed = time() - t_start\n        wandb_log(; step=step, train_loss=avg_loss, val_loss=val_loss, lr=lr_t)\n\n        improved = \"\"\n        if val_loss < best_val_loss\n            best_val_loss = val_loss\n            save_checkpoint(\"checkpoints/best_model.json\", state_dict, param_keys, uchars, hyperparams;\n                adam_m=adam_m, adam_v=adam_v, step=step,\n                lr=lr, b1=b1, b2=b2,\n                best_val_loss=best_val_loss,\n                train_losses=train_loss_history, val_losses=val_loss_history,\n                total_steps=step, num_steps_target=end_step)\n            improved = \" << new best!\"\n        end\n        @printf(\"step %4d / %4d | train %.4f | val %.4f | %.1fs%s\\n\",\n                step, end_step, avg_loss, val_loss, elapsed, improved)\n    elseif step % 10 == 0\n        elapsed = time() - t_start\n        @printf(\"step %4d / %4d | train %.4f | %.1fs\\n\", step, end_step, avg_loss, elapsed)\n    end\nend\n\nelapsed = time() - t_start\n@printf(\"\\nresume training complete in %.1f seconds\\n\", elapsed)\nwandb_finish()\n\nsave_checkpoint(\"checkpoints/final_model.json\", state_dict, param_keys, uchars, hyperparams;\n    adam_m=adam_m, adam_v=adam_v, step=end_step,\n    lr=lr, b1=b1, b2=b2,\n    best_val_loss=best_val_loss,\n    train_losses=train_loss_history, val_losses=val_loss_history,\n    total_steps=end_step, num_steps_target=end_step)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. Download Checkpoint\n",
    "\n",
    "Download the best model checkpoint to use with the inference server.  \n",
    "In Colab, use the Files panel (left sidebar) to download, or copy to Google Drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List saved checkpoints\n",
    "if isdir(\"checkpoints\")\n",
    "    files = readdir(\"checkpoints\")\n",
    "    println(\"Saved checkpoints:\")\n",
    "    for f in files\n",
    "        path = joinpath(\"checkpoints\", f)\n",
    "        size_kb = round(filesize(path) / 1024, digits=1)\n",
    "        println(\"  $path ($(size_kb) KB)\")\n",
    "    end\n",
    "else\n",
    "    println(\"No checkpoints directory found. Train first!\")\n",
    "end"
   ]
  }
 ]
}