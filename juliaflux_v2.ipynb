{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Julia",
   "language": "julia",
   "name": "julia"
  },
  "language_info": {
   "name": "julia"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "<a href=\"https://colab.research.google.com/github/DavinciDreams/JuliaGPT/blob/main/juliaflux_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# JuliaFlux v2 — SOTA Small Language Model\n",
    "\n",
    "GPU-accelerated small language model trained on classical texts using the trivium/quadrivium curriculum.\n",
    "Uses Flux.jl + CUDA.jl with modern LLaMA-style architecture.\n",
    "\n",
    "**Architecture (LLaMA-style):**\n",
    "- RMSNorm (replaces LayerNorm)\n",
    "- Rotary Position Embeddings (RoPE, replaces learned position embeddings)\n",
    "- SwiGLU activation (replaces GELU FFN)\n",
    "- Grouped Query Attention (GQA: 6 Q heads, 2 KV heads)\n",
    "- Weight-tied output projection\n",
    "- Gradient clipping, cosine LR with warmup\n",
    "- BPE tokenization with char-level fallback\n",
    "- Curriculum learning (trivium → quadrivium → philosophy)\n",
    "\n",
    "**100% Julia — no Python dependencies.**\n",
    "\n",
    "Based on: https://github.com/DavinciDreams/JuliaGPT"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 0. Setup — Credentials & Dependencies\n",
    "\n",
    "Set your API tokens in a `.env` file in the notebook directory, or set them directly in the cell below.\n",
    "\n",
    "**`.env` file format:**\n",
    "```\n",
    "HF_TOKEN=hf_yourTokenHere\n",
    "WANDB_API_KEY=your_wandb_key\n",
    "HF_REPO=YourUser/JuliaFluxGPT\n",
    "HF_DATA_REPO=YourUser/philosophy-corpus\n",
    "```\n",
    "\n",
    "On Colab: upload a `.env` file to the working directory, or uncomment the lines below."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# ══════════════════════════════════════════════════════════════════\n",
    "# Option 1: Set tokens directly (uncomment and fill in)\n",
    "# ══════════════════════════════════════════════════════════════════\n",
    "# ENV[\"HF_TOKEN\"]       = \"hf_...\"\n",
    "# ENV[\"WANDB_API_KEY\"]  = \"...\"\n",
    "# ENV[\"HF_REPO\"]        = \"YourUser/JuliaFluxGPT\"\n",
    "# ENV[\"HF_DATA_REPO\"]   = \"YourUser/philosophy-corpus\"\n",
    "\n",
    "# ══════════════════════════════════════════════════════════════════\n",
    "# Option 2: Load from .env file (recommended)\n",
    "# ══════════════════════════════════════════════════════════════════\n",
    "function load_dotenv(path=\".env\")\n",
    "    isfile(path) || return 0\n",
    "    n = 0\n",
    "    for line in eachline(path)\n",
    "        line = strip(line)\n",
    "        (isempty(line) || startswith(line, '#')) && continue\n",
    "        m = match(r\"^([A-Za-z_][A-Za-z0-9_]*)=(.*)\", line)\n",
    "        m === nothing && continue\n",
    "        key = m.captures[1]\n",
    "        val = strip(m.captures[2], ['\"', '\\'', ' '])\n",
    "        ENV[key] = val\n",
    "        n += 1\n",
    "    end\n",
    "    return n\n",
    "end\n",
    "\n",
    "n = load_dotenv()\n",
    "n > 0 && println(\"Loaded $n vars from .env\")\n",
    "\n",
    "# ── Validate ──\n",
    "HF_TOKEN      = get(ENV, \"HF_TOKEN\", \"\")\n",
    "HF_REPO_ID    = get(ENV, \"HF_REPO\", \"\")\n",
    "HF_DATA_REPO  = get(ENV, \"HF_DATA_REPO\", \"LisaMegaWatts/philosophy-corpus\")\n",
    "\n",
    "println(\"HF Token:  \", isempty(HF_TOKEN) ? \"NOT SET\" : \"$(HF_TOKEN[1:min(8,end)])...\")\n",
    "println(\"HF Repo:   \", isempty(HF_REPO_ID) ? \"NOT SET\" : HF_REPO_ID)\n",
    "println(\"HF Data:   \", HF_DATA_REPO)"
   ],
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "## 1. Install Julia Packages & Imports\n",
    "\n",
    "Installs Flux.jl ecosystem packages (first run takes a few minutes on Colab).\n",
    "CUDA is auto-detected for GPU acceleration. HuggingFaceApi.jl handles HF downloads natively."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "import Pkg\n",
    "Pkg.add([\"Flux\", \"Zygote\", \"Optimisers\", \"CUDA\", \"cuDNN\",\n",
    "         \"NNlib\", \"JLD2\", \"JSON3\", \"Downloads\", \"HTTP\",\n",
    "         \"HuggingFaceApi\"])\n",
    "\n",
    "using Flux\n",
    "using Zygote\n",
    "using Optimisers\n",
    "using CUDA\n",
    "using NNlib\n",
    "using Downloads\n",
    "using Statistics\n",
    "using Random\n",
    "using Printf\n",
    "using LinearAlgebra\n",
    "using JLD2\n",
    "using JSON3\n",
    "using HTTP\n",
    "using HuggingFaceApi\n",
    "\n",
    "Random.seed!(1337)\n",
    "\n",
    "device = CUDA.functional() ? gpu : cpu\n",
    "println(\"Device: \", device)\n",
    "println(\"CUDA functional: \", CUDA.functional())\n",
    "if CUDA.functional()\n",
    "    println(\"GPU: \", CUDA.name(CUDA.device()))\n",
    "    mem = CUDA.totalmem(CUDA.device())\n",
    "    println(\"VRAM: \", round(mem / 1024^3, digits=1), \" GB\")\n",
    "    println(\"CUDA version: \", CUDA.runtime_version())\n",
    "end"
   ],
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# ══════════════════════════════════════════════════════════════════\n# HuggingFace Hub helpers — pure Julia (no Python, no pip)\n# Uses HuggingFaceApi.jl for downloads, HTTP.jl for uploads\n# ══════════════════════════════════════════════════════════════════\n\n# ── Login note ──\n# HuggingFaceApi.login() is interactive (username/password), NOT token-based.\n# Pass auth_token=HF_TOKEN directly to hf_hub_download() instead.\nif !isempty(HF_TOKEN)\n    println(\"HuggingFace: token set (will pass auth_token to API calls)\")\nelse\n    println(\"HuggingFace: no token set (public repos only)\")\nend\n\n# ── Download files from HuggingFace ──\nfunction hf_download(repo_id::String, filename::String;\n                     local_dir::String=\".\", repo_type::String=\"dataset\")\n    local_path = joinpath(local_dir, filename)\n    isfile(local_path) && return local_path\n    mkpath(local_dir)\n    try\n        path = HuggingFaceApi.hf_hub_download(repo_id, filename;\n                    repo_type=repo_type, auth_token=HF_TOKEN)\n        cp(path, local_path; force=true)\n        println(\"  Downloaded: $filename ($(filesize(local_path)) bytes)\")\n    catch e\n        # Fallback: direct HTTP download\n        prefix = repo_type == \"dataset\" ? \"datasets/\" : \"\"\n        url = \"https://huggingface.co/$(prefix)$(repo_id)/resolve/main/$(filename)\"\n        headers = isempty(HF_TOKEN) ? Pair{String,String}[] : [\"Authorization\" => \"Bearer $HF_TOKEN\"]\n        Downloads.download(url, local_path; headers)\n        println(\"  Downloaded (HTTP fallback): $filename\")\n    end\n    return local_path\nend\n\n# ── Upload files to HuggingFace (via commit API) ──\nfunction hf_upload(repo_id::String, local_path::String;\n                   remote_path::String=\"\", repo_type::String=\"model\")\n    isempty(HF_TOKEN) && (@warn \"Cannot upload: no HF_TOKEN set\"; return)\n    rp = isempty(remote_path) ? basename(local_path) : remote_path\n    prefix = repo_type == \"model\" ? \"models\" : \"datasets\"\n    url = \"https://huggingface.co/api/$(prefix)/$(repo_id)/upload/main/$(rp)\"\n\n    data = read(local_path)\n    headers = [\n        \"Authorization\" => \"Bearer $HF_TOKEN\",\n        \"Content-Type\" => \"application/octet-stream\",\n    ]\n    try\n        HTTP.put(url, headers, data)\n        println(\"Pushed $local_path -> $repo_id/$rp\")\n    catch e\n        @warn \"Upload failed: $e\"\n    end\nend\n\nfunction hf_push(repo_id::String, local_path::String; remote_path::String=\"\")\n    hf_upload(repo_id, local_path; remote_path)\nend\n\nfunction hf_push_checkpoint(repo_id::String; checkpoint_path::String=\"checkpoints/best_model.jld2\")\n    isfile(checkpoint_path) || error(\"Checkpoint not found: $checkpoint_path\")\n    hf_push(repo_id, checkpoint_path)\nend\n\nfunction hf_create_repo(repo_id::String)\n    isempty(HF_TOKEN) && return\n    url = \"https://huggingface.co/api/repos/create\"\n    headers = [\n        \"Authorization\" => \"Bearer $HF_TOKEN\",\n        \"Content-Type\" => \"application/json\",\n    ]\n    body = JSON3.write(Dict(\"name\" => split(repo_id, \"/\")[end], \"type\" => \"model\", \"private\" => false))\n    try\n        HTTP.post(url, headers, body)\n        println(\"Created HF repo: $repo_id\")\n    catch\n        println(\"HF repo already exists or creation skipped: $repo_id\")\n    end\nend\n\nfunction hf_sync(local_path::String)\n    isempty(HF_REPO_ID) && return\n    try\n        hf_push(HF_REPO_ID, local_path)\n    catch e\n        println(\"  HF sync failed: $e\")\n    end\nend\n\n# ── Simple logging (no W&B, no Python) ──\nfunction wandb_init(); end\nfunction wandb_log(; kwargs...); end\nfunction wandb_finish(); end\n\nprintln(\"HuggingFace helpers defined (pure Julia)\")",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "## 2. Hyperparameters\n",
    "\n",
    "SOTA architecture defaults: RoPE, SwiGLU, GQA, RMSNorm."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# ── Model architecture (LLaMA-style) ──\n",
    "block_size     = 256       # context window\n",
    "n_embd         = 384       # embedding dim\n",
    "n_head         = 6         # Q attention heads\n",
    "n_kv_head      = 2         # KV heads for GQA (each KV head serves 3 Q heads)\n",
    "n_layer        = 6         # transformer layers\n",
    "dropout        = 0.1\n",
    "bias           = false\n",
    "rope_base      = 10000.0f0 # RoPE frequency base\n",
    "\n",
    "# ── Training ──\n",
    "batch_size     = 64\n",
    "learning_rate  = 3e-4\n",
    "max_iters      = 5000\n",
    "eval_interval  = 500\n",
    "eval_iters     = 100\n",
    "warmup_iters   = 200\n",
    "min_lr         = 1e-5\n",
    "max_grad_norm  = 1.0f0     # gradient clipping threshold\n",
    "\n",
    "# ── Curriculum learning ──\n",
    "curriculum_enabled = true\n",
    "curriculum_warmup  = 1000   # steps of trivium-only before mixing in harder texts\n",
    "\n",
    "println(\"Architecture: n_embd=$n_embd, n_layer=$n_layer, n_head=$n_head (Q), n_kv_head=$n_kv_head (KV)\")\n",
    "println(\"GQA ratio: $(n_head ÷ n_kv_head) Q heads per KV head\")\n",
    "println(\"Training: batch=$batch_size, lr=$learning_rate, iters=$max_iters, clip=$max_grad_norm\")\n",
    "println(\"Curriculum: enabled=$curriculum_enabled, warmup=$curriculum_warmup steps\")"
   ],
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "## 3. Dataset — Classical Curriculum\n",
    "\n",
    "Loads pre-cleaned philosophy corpus from the text pipeline, organized by\n",
    "the classical trivium/quadrivium curriculum:\n",
    "\n",
    "1. **Trivium** (language arts): grammar, rhetoric, logic\n",
    "2. **Quadrivium** (mathematical arts): arithmetic, geometry, music, astronomy\n",
    "3. **Philosophy**: ethics, metaphysics, politics\n",
    "\n",
    "**Data flow:**\n",
    "```\n",
    "text-pipeline/ → clean → chunk → train.txt + curriculum files\n",
    "                                → push to HuggingFace Dataset\n",
    "                                                    ↓\n",
    "juliaflux_v2.ipynb → hf_download() → BPE tokenize → curriculum batches → train\n",
    "```"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# ── Load pre-cleaned data from text pipeline ──\n",
    "DATA_DIR = \"data\"\n",
    "train_file = joinpath(DATA_DIR, \"train.txt\")\n",
    "val_file   = joinpath(DATA_DIR, \"val.txt\")\n",
    "\n",
    "# HF_DATA_REPO is set in the credentials cell above\n",
    "\n",
    "if !isfile(train_file) || !isfile(val_file)\n",
    "    println(\"Data not found locally, downloading from $HF_DATA_REPO ...\")\n",
    "    mkpath(DATA_DIR)\n",
    "    try\n",
    "        hf_download(HF_DATA_REPO, \"train.txt\"; local_dir=DATA_DIR, repo_type=\"dataset\")\n",
    "        hf_download(HF_DATA_REPO, \"val.txt\"; local_dir=DATA_DIR, repo_type=\"dataset\")\n",
    "    catch e\n",
    "        @warn \"Download failed: $e\"\n",
    "    end\n",
    "end\n",
    "\n",
    "if !isfile(train_file) || !isfile(val_file)\n",
    "    error(\"No data found in $DATA_DIR/! Push train.txt + val.txt to your HF dataset repo, or copy from text-pipeline/output/\")\n",
    "end\n",
    "\n",
    "train_text = read(train_file, String)\n",
    "val_text   = read(val_file, String)\n",
    "\n",
    "println(\"Data loaded from $DATA_DIR/ ($HF_DATA_REPO)\")\n",
    "println(\"  train.txt: $(length(train_text)) chars ($(count('\\n', train_text)) chunks)\")\n",
    "println(\"  val.txt:   $(length(val_text)) chars ($(count('\\n', val_text)) chunks)\")\n",
    "\n",
    "# ── Load curriculum phase files (optional) ──\n",
    "phase_data = Dict{String, String}()\n",
    "for phase in [\"trivium\", \"quadrivium\", \"philosophy\"]\n",
    "    phase_file = joinpath(DATA_DIR, \"train_$(phase).txt\")\n",
    "    if !isfile(phase_file)\n",
    "        try\n",
    "            hf_download(HF_DATA_REPO, \"train_$(phase).txt\"; local_dir=DATA_DIR, repo_type=\"dataset\")\n",
    "        catch; end\n",
    "    end\n",
    "    if isfile(phase_file)\n",
    "        phase_data[phase] = read(phase_file, String)\n",
    "        n_chunks = count('\\n', phase_data[phase])\n",
    "        println(\"  train_$(phase).txt: $(length(phase_data[phase])) chars ($n_chunks chunks)\")\n",
    "    end\n",
    "end\n",
    "\n",
    "if isempty(phase_data)\n",
    "    println(\"\\n  No curriculum phase files found — will use full corpus for all phases\")\n",
    "    curriculum_enabled = false\n",
    "end\n",
    "\n",
    "# ── Try to download tokenizer.json ──\n",
    "tokenizer_file = joinpath(DATA_DIR, \"tokenizer.json\")\n",
    "if !isfile(tokenizer_file)\n",
    "    try\n",
    "        hf_download(HF_DATA_REPO, \"tokenizer.json\"; local_dir=DATA_DIR, repo_type=\"dataset\")\n",
    "    catch\n",
    "        println(\"  No tokenizer.json available (will use char-level tokenizer)\")\n",
    "    end\n",
    "end"
   ],
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# ── Tokenizer: BPE with character-level fallback ──\n",
    "using JSON3\n",
    "\n",
    "TOKENIZER_PATH = joinpath(DATA_DIR, \"tokenizer.json\")\n",
    "USE_BPE = isfile(TOKENIZER_PATH)\n",
    "\n",
    "if USE_BPE\n",
    "    println(\"Loading BPE tokenizer from $TOKENIZER_PATH ...\")\n",
    "    tok_raw = read(TOKENIZER_PATH, String)\n",
    "    tok_json = JSON3.read(tok_raw)\n",
    "\n",
    "    # Parse vocabulary: token_string -> id (convert to 1-indexed for Julia)\n",
    "    bpe_vocab = Dict{String, Int}()\n",
    "    for (tok_str, id) in pairs(tok_json.model.vocab)\n",
    "        bpe_vocab[string(tok_str)] = Int(id) + 1\n",
    "    end\n",
    "\n",
    "    # Parse merges: ordered list of (a, b) pairs\n",
    "    bpe_merges = Vector{Tuple{String,String}}()\n",
    "    for merge_str in tok_json.model.merges\n",
    "        parts = split(string(merge_str), \" \", limit=2)\n",
    "        if length(parts) == 2\n",
    "            push!(bpe_merges, (String(parts[1]), String(parts[2])))\n",
    "        end\n",
    "    end\n",
    "\n",
    "    # Reverse vocab: id -> token_string\n",
    "    bpe_id_to_token = Dict{Int, String}(id => tok for (tok, id) in bpe_vocab)\n",
    "\n",
    "    global vocab_size = length(bpe_vocab)\n",
    "\n",
    "    # BPE encode: apply merges in priority order\n",
    "    function bpe_encode_word(word::Vector{String})\n",
    "        tokens = copy(word)\n",
    "        for (a, b) in bpe_merges\n",
    "            i = 1\n",
    "            while i < length(tokens)\n",
    "                if tokens[i] == a && tokens[i+1] == b\n",
    "                    tokens = vcat(tokens[1:i-1], [a * b], tokens[i+2:end])\n",
    "                else\n",
    "                    i += 1\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "        return tokens\n",
    "    end\n",
    "\n",
    "    function encode(s::String)\n",
    "        # Byte-level BPE: each byte is a starting token\n",
    "        chars = [string(c) for c in s]\n",
    "        tokens = bpe_encode_word(chars)\n",
    "        ids = Int[]\n",
    "        for tok in tokens\n",
    "            id = get(bpe_vocab, tok, nothing)\n",
    "            if id !== nothing\n",
    "                push!(ids, id)\n",
    "            end\n",
    "        end\n",
    "        return ids\n",
    "    end\n",
    "\n",
    "    function decode(ids::Vector{Int})\n",
    "        tokens = [get(bpe_id_to_token, id, \"\") for id in ids]\n",
    "        return join(tokens)\n",
    "    end\n",
    "\n",
    "    println(\"BPE tokenizer: vocab_size=$vocab_size, $(length(bpe_merges)) merges\")\n",
    "\n",
    "else\n",
    "    # ── Fallback: character-level tokenizer ──\n",
    "    println(\"No tokenizer.json found — using character-level tokenizer\")\n",
    "\n",
    "    full_text = train_text * \"\\n\" * val_text\n",
    "    chars = sort(unique(full_text))\n",
    "    filter!(c -> c != '\\n', chars)\n",
    "    global vocab_size = length(chars)\n",
    "\n",
    "    stoi = Dict(c => i for (i, c) in enumerate(chars))\n",
    "    itos = Dict(i => c for (i, c) in enumerate(chars))\n",
    "\n",
    "    encode(s::String) = [stoi[c] for c in s if haskey(stoi, c)]\n",
    "    decode(ids::Vector{Int}) = join(itos[i] for i in ids)\n",
    "\n",
    "    println(\"Char-level tokenizer: vocab_size=$vocab_size -> [$(join(chars))]\")\n",
    "end\n",
    "\n",
    "# ── Encode training and validation data ──\n",
    "train_clean = replace(strip(train_text), '\\n' => ' ')\n",
    "val_clean   = replace(strip(val_text), '\\n' => ' ')\n",
    "\n",
    "global train_data = encode(train_clean)\n",
    "global val_data   = encode(val_clean)\n",
    "\n",
    "# ── Encode curriculum phase data ──\n",
    "global phase_encoded = Dict{String, Vector{Int}}()\n",
    "for (phase, text) in phase_data\n",
    "    clean = replace(strip(text), '\\n' => ' ')\n",
    "    phase_encoded[phase] = encode(clean)\n",
    "    println(\"  Phase $phase: $(length(phase_encoded[phase])) tokens\")\n",
    "end\n",
    "\n",
    "println(\"\\nTrain: $(length(train_data)) tokens\")\n",
    "println(\"Val:   $(length(val_data)) tokens\")\n",
    "println(\"Total: $(length(train_data) + length(val_data)) tokens\")"
   ],
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "## 4. Model Architecture\n",
    "\n",
    "LLaMA-style transformer with SOTA components.\n",
    "All structs defined in one cell (Julia limitation: structs cannot be redefined).\n",
    "\n",
    "| Component | Old (v1) | New (v2) |\n",
    "|-----------|----------|----------|\n",
    "| Normalization | LayerNorm | **RMSNorm** |\n",
    "| Position encoding | Learned absolute | **RoPE** (rotary) |\n",
    "| FFN activation | GELU (2 matrices) | **SwiGLU** (3 matrices) |\n",
    "| Attention | Standard MHA | **GQA** (grouped query) |\n",
    "| Output head | Separate Dense | **Weight-tied** with embedding |\n",
    "| Gradient | No clipping | **ClipNorm(1.0)** |"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# ── Curriculum learning state ──\n",
    "global curriculum_step = 0\n",
    "\n",
    "function get_batch(split=\"train\")\n",
    "    global curriculum_step\n",
    "\n",
    "    if split == \"val\"\n",
    "        d = val_data\n",
    "    elseif curriculum_enabled && !isempty(phase_encoded)\n",
    "        # Curriculum: start with trivium, progressively add harder material\n",
    "        progress = min(curriculum_step / curriculum_warmup, 1.0)\n",
    "\n",
    "        if progress < 0.33 && haskey(phase_encoded, \"trivium\") && !isempty(phase_encoded[\"trivium\"])\n",
    "            d = phase_encoded[\"trivium\"]\n",
    "        elseif progress < 0.66\n",
    "            # Mix trivium + quadrivium\n",
    "            sources = Vector{Int}[]\n",
    "            haskey(phase_encoded, \"trivium\") && push!(sources, phase_encoded[\"trivium\"])\n",
    "            haskey(phase_encoded, \"quadrivium\") && push!(sources, phase_encoded[\"quadrivium\"])\n",
    "            d = isempty(sources) ? train_data : vcat(sources...)\n",
    "        else\n",
    "            d = train_data  # full corpus\n",
    "        end\n",
    "    else\n",
    "        d = train_data\n",
    "    end\n",
    "\n",
    "    # Ensure data is long enough\n",
    "    if length(d) <= block_size + 1\n",
    "        d = train_data\n",
    "    end\n",
    "\n",
    "    ix = rand(1:length(d) - block_size, batch_size)\n",
    "    x = hcat([d[i:i+block_size-1] for i in ix]...)\n",
    "    y = hcat([d[i+1:i+block_size] for i in ix]...)\n",
    "    x = permutedims(x)   # (B, T)\n",
    "    y = permutedims(y)\n",
    "    x = x |> device\n",
    "    y = y |> device\n",
    "    return x, y\n",
    "end"
   ],
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# ══════════════════════════════════════════════════════════════════\n",
    "# ALL MODEL STRUCTS IN ONE CELL (Julia structs cannot be redefined)\n",
    "# Same LLaMA-style architecture as juliaflux_v2.ipynb\n",
    "# ══════════════════════════════════════════════════════════════════\n",
    "\n",
    "using NNlib: batched_mul\n",
    "\n",
    "const CAUSAL_MASK = triu(fill(typemin(Float32), block_size, block_size), 1)\n",
    "const CAUSAL_MASK_GPU = CUDA.functional() ? cu(CAUSAL_MASK) : CAUSAL_MASK\n",
    "\n",
    "const HEAD_DIM = n_embd ÷ n_head\n",
    "\n",
    "function precompute_rope_freqs(head_dim::Int, max_seq_len::Int; base::Float32 = 10000.0f0)\n",
    "    half_dim = head_dim ÷ 2\n",
    "    freqs = Float32[1.0f0 / (base ^ (Float32(2 * (i - 1)) / Float32(head_dim))) for i in 1:half_dim]\n",
    "    positions = Float32.(collect(0:max_seq_len-1))\n",
    "    angles = freqs * positions'\n",
    "    return cos.(angles), sin.(angles)\n",
    "end\n",
    "\n",
    "const ROPE_COS, ROPE_SIN = precompute_rope_freqs(HEAD_DIM, block_size; base=rope_base)\n",
    "const ROPE_COS_GPU = CUDA.functional() ? cu(ROPE_COS) : ROPE_COS\n",
    "const ROPE_SIN_GPU = CUDA.functional() ? cu(ROPE_SIN) : ROPE_SIN\n",
    "\n",
    "function apply_rope(x, cos_f, sin_f, T::Int)\n",
    "    d = size(x, 1) ÷ 2\n",
    "    x1 = x[1:d, :, :]\n",
    "    x2 = x[d+1:2d, :, :]\n",
    "    c = cos_f[:, 1:T]\n",
    "    s = sin_f[:, 1:T]\n",
    "    return vcat(x1 .* c .- x2 .* s, x1 .* s .+ x2 .* c)\n",
    "end\n",
    "\n",
    "struct RMSNorm{W <: AbstractVector}\n",
    "    weight::W\n",
    "    eps::Float32\n",
    "end\n",
    "\n",
    "Flux.@layer RMSNorm\n",
    "\n",
    "function RMSNorm(dim::Int; eps::Float32 = 1.0f-6)\n",
    "    RMSNorm(ones(Float32, dim), eps)\n",
    "end\n",
    "\n",
    "function (rn::RMSNorm)(x)\n",
    "    rms = sqrt.(mean(x .^ 2, dims=1) .+ rn.eps)\n",
    "    return (x ./ rms) .* rn.weight\n",
    "end\n",
    "\n",
    "struct SwiGLUFFN\n",
    "    w_gate::Dense\n",
    "    w_up::Dense\n",
    "    w_down::Dense\n",
    "    drop::Dropout\n",
    "end\n",
    "\n",
    "Flux.@layer SwiGLUFFN\n",
    "\n",
    "function SwiGLUFFN(n_embd::Int; bias=false, dropout=0.0)\n",
    "    raw_inner = Int(floor(4 * n_embd * 2 / 3))\n",
    "    inner_dim = max(64, 64 * div(raw_inner + 32, 64))\n",
    "    SwiGLUFFN(\n",
    "        Dense(n_embd => inner_dim; bias),\n",
    "        Dense(n_embd => inner_dim; bias),\n",
    "        Dense(inner_dim => n_embd; bias),\n",
    "        Dropout(dropout)\n",
    "    )\n",
    "end\n",
    "\n",
    "function (ff::SwiGLUFFN)(x)\n",
    "    ff.drop(ff.w_down(NNlib.swish(ff.w_gate(x)) .* ff.w_up(x)))\n",
    "end\n",
    "\n",
    "struct CausalSelfAttention\n",
    "    wq::Dense\n",
    "    wkv::Dense\n",
    "    proj::Dense\n",
    "    n_head::Int\n",
    "    n_kv_head::Int\n",
    "end\n",
    "\n",
    "Flux.@layer CausalSelfAttention trainable=(wq, wkv, proj)\n",
    "\n",
    "function CausalSelfAttention(n_embd::Int, n_head::Int, n_kv_head::Int; bias=false)\n",
    "    head_dim = n_embd ÷ n_head\n",
    "    kv_dim = head_dim * n_kv_head\n",
    "    CausalSelfAttention(\n",
    "        Dense(n_embd => n_embd; bias),\n",
    "        Dense(n_embd => 2 * kv_dim; bias),\n",
    "        Dense(n_embd => n_embd; bias),\n",
    "        n_head,\n",
    "        n_kv_head\n",
    "    )\n",
    "end\n",
    "\n",
    "function (attn::CausalSelfAttention)(x)\n",
    "    C, T, B = size(x)\n",
    "    nh = attn.n_head\n",
    "    nkv = attn.n_kv_head\n",
    "    hs = C ÷ nh\n",
    "    kv_dim = hs * nkv\n",
    "    groups = nh ÷ nkv\n",
    "\n",
    "    q = attn.wq(x)\n",
    "    kv = attn.wkv(x)\n",
    "    k = kv[1:kv_dim, :, :]\n",
    "    v = kv[kv_dim+1:2*kv_dim, :, :]\n",
    "\n",
    "    q = reshape(permutedims(reshape(q, hs, nh, T, B), (1, 3, 2, 4)), hs, T, nh * B)\n",
    "    k = reshape(permutedims(reshape(k, hs, nkv, T, B), (1, 3, 2, 4)), hs, T, nkv * B)\n",
    "    v = reshape(permutedims(reshape(v, hs, nkv, T, B), (1, 3, 2, 4)), hs, T, nkv * B)\n",
    "\n",
    "    cos_f = x isa CuArray ? ROPE_COS_GPU : ROPE_COS\n",
    "    sin_f = x isa CuArray ? ROPE_SIN_GPU : ROPE_SIN\n",
    "    q = apply_rope(q, cos_f, sin_f, T)\n",
    "    k = apply_rope(k, cos_f, sin_f, T)\n",
    "\n",
    "    if groups > 1\n",
    "        k_4d = reshape(k, hs, T, nkv, B)\n",
    "        v_4d = reshape(v, hs, T, nkv, B)\n",
    "        k_rep = repeat(reshape(k_4d, hs, T, nkv, 1, B), 1, 1, 1, groups, 1)\n",
    "        v_rep = repeat(reshape(v_4d, hs, T, nkv, 1, B), 1, 1, 1, groups, 1)\n",
    "        k = reshape(permutedims(k_rep, (1, 2, 4, 3, 5)), hs, T, nh * B)\n",
    "        v = reshape(permutedims(v_rep, (1, 2, 4, 3, 5)), hs, T, nh * B)\n",
    "    end\n",
    "\n",
    "    scale = Float32(1 / sqrt(hs))\n",
    "    wei = batched_mul(permutedims(q, (2, 1, 3)), k) .* scale\n",
    "\n",
    "    mask = x isa CuArray ? CAUSAL_MASK_GPU[1:T, 1:T] : CAUSAL_MASK[1:T, 1:T]\n",
    "    wei = wei .+ mask\n",
    "    wei = softmax(wei; dims=2)\n",
    "\n",
    "    out = batched_mul(v, permutedims(wei, (2, 1, 3)))\n",
    "    out = reshape(permutedims(reshape(out, hs, T, nh, B), (1, 3, 2, 4)), C, T, B)\n",
    "\n",
    "    attn.proj(out)\n",
    "end\n",
    "\n",
    "struct TransformerBlock\n",
    "    ln1::RMSNorm\n",
    "    attn::CausalSelfAttention\n",
    "    ln2::RMSNorm\n",
    "    ffwd::SwiGLUFFN\n",
    "end\n",
    "\n",
    "Flux.@layer TransformerBlock\n",
    "\n",
    "function TransformerBlock(n_embd::Int, n_head::Int, n_kv_head::Int; dropout=0.0)\n",
    "    TransformerBlock(\n",
    "        RMSNorm(n_embd),\n",
    "        CausalSelfAttention(n_embd, n_head, n_kv_head),\n",
    "        RMSNorm(n_embd),\n",
    "        SwiGLUFFN(n_embd; dropout)\n",
    "    )\n",
    "end\n",
    "\n",
    "function (block::TransformerBlock)(x)\n",
    "    x = x .+ block.attn(block.ln1(x))\n",
    "    x = x .+ block.ffwd(block.ln2(x))\n",
    "    x\n",
    "end\n",
    "\n",
    "struct TiedDense{W <: AbstractMatrix}\n",
    "    weight_ref::W\n",
    "end\n",
    "\n",
    "Flux.@layer TiedDense trainable=()\n",
    "\n",
    "function (td::TiedDense)(x)\n",
    "    C, T, B = size(x)\n",
    "    W = td.weight_ref\n",
    "    x_flat = reshape(x, C, T * B)\n",
    "    out = W' * x_flat\n",
    "    reshape(out, size(W, 2), T, B)\n",
    "end\n",
    "\n",
    "struct GPT\n",
    "    wte::Embedding\n",
    "    drop::Dropout\n",
    "    blocks::Chain\n",
    "    ln_f::RMSNorm\n",
    "    lm_head::TiedDense\n",
    "end\n",
    "\n",
    "Flux.@layer GPT\n",
    "\n",
    "function GPT(; vocab_size, n_embd, block_size, n_layer, n_head, n_kv_head, dropout=0.1)\n",
    "    wte = Embedding(vocab_size => n_embd)\n",
    "    GPT(\n",
    "        wte,\n",
    "        Dropout(dropout),\n",
    "        Chain([TransformerBlock(n_embd, n_head, n_kv_head; dropout) for _ in 1:n_layer]...),\n",
    "        RMSNorm(n_embd),\n",
    "        TiedDense(wte.weight)\n",
    "    )\n",
    "end\n",
    "\n",
    "function (m::GPT)(idx)\n",
    "    B, T = size(idx)\n",
    "    tok = permutedims(m.wte(idx), (1, 3, 2))\n",
    "    x = m.drop(tok)\n",
    "    x = m.blocks(x)\n",
    "    x = m.ln_f(x)\n",
    "    m.lm_head(x)\n",
    "end\n",
    "\n",
    "println(\"Student model structs defined (same architecture as juliaflux_v2)\")\n",
    "println(\"SOTA: RoPE, SwiGLU, GQA ($(n_head)Q/$(n_kv_head)KV), RMSNorm, weight tying\")"
   ],
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "model = GPT(;\n",
    "    vocab_size = vocab_size,\n",
    "    n_embd     = n_embd,\n",
    "    block_size = block_size,\n",
    "    n_layer    = n_layer,\n",
    "    n_head     = n_head,\n",
    "    n_kv_head  = n_kv_head,\n",
    "    dropout    = dropout\n",
    ") |> device\n",
    "\n",
    "n_params = sum(length, Flux.trainables(model))\n",
    "println(\"Model created on $device\")\n",
    "println(\"Parameters: $(n_params) ($(round(n_params/1e6, digits=2))M)\")\n",
    "println(\"  Weight tying saves $(vocab_size * n_embd) params = $(round(vocab_size * n_embd / 1e3, digits=1))K\")\n",
    "\n",
    "if CUDA.functional()\n",
    "    println(\"GPU memory: $(round(CUDA.used_memory() / 1024^2, digits=1)) MB\")\n",
    "end\n",
    "\n",
    "# Smoke test\n",
    "x_test, y_test = get_batch(\"train\")\n",
    "logits_test = model(x_test)\n",
    "println(\"Forward pass OK — logits: $(size(logits_test))\")\n",
    "@assert size(logits_test, 1) == vocab_size\n",
    "@assert size(logits_test, 2) == block_size\n",
    "@assert size(logits_test, 3) == batch_size"
   ],
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "## 5. Checkpoint Save/Load"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "LOCAL_CKPT = \"checkpoints\"\n",
    "mkpath(LOCAL_CKPT)\n",
    "\n",
    "function save_checkpoint(path::String, model, opt_state;\n",
    "                          step::Int=0, best_val_loss::Float64=Inf,\n",
    "                          train_losses::Vector{Float64}=Float64[],\n",
    "                          val_losses::Vector{Float64}=Float64[])\n",
    "    mkpath(dirname(path))\n",
    "    model_cpu = cpu(model)\n",
    "    opt_cpu = cpu(opt_state)\n",
    "    JLD2.jldsave(path;\n",
    "        model_state = Flux.state(model_cpu),\n",
    "        opt_state = opt_cpu,\n",
    "        step = step,\n",
    "        best_val_loss = best_val_loss,\n",
    "        train_losses = train_losses,\n",
    "        val_losses = val_losses,\n",
    "        hyperparams = Dict(\n",
    "            \"vocab_size\" => vocab_size,\n",
    "            \"n_embd\" => n_embd,\n",
    "            \"block_size\" => block_size,\n",
    "            \"n_layer\" => n_layer,\n",
    "            \"n_head\" => n_head,\n",
    "            \"n_kv_head\" => n_kv_head,\n",
    "            \"dropout\" => dropout,\n",
    "            \"use_bpe\" => USE_BPE,\n",
    "            \"rope_base\" => rope_base\n",
    "        )\n",
    "    )\n",
    "    vl_str = best_val_loss == Inf ? \"Inf\" : @sprintf(\"%.4f\", best_val_loss)\n",
    "    println(\"Checkpoint saved: $path (step $step, best_val_loss=$vl_str)\")\n",
    "end\n",
    "\n",
    "function save_and_sync(path, model, opt_state; kwargs...)\n",
    "    save_checkpoint(path, model, opt_state; kwargs...)\n",
    "    hf_sync(path)\n",
    "end\n",
    "\n",
    "function load_checkpoint(path::String, device_fn)\n",
    "    println(\"Loading checkpoint from $path ...\")\n",
    "    data = JLD2.load(path)\n",
    "\n",
    "    hp = data[\"hyperparams\"]\n",
    "    m = GPT(;\n",
    "        vocab_size = hp[\"vocab_size\"],\n",
    "        n_embd     = hp[\"n_embd\"],\n",
    "        block_size = hp[\"block_size\"],\n",
    "        n_layer    = hp[\"n_layer\"],\n",
    "        n_head     = hp[\"n_head\"],\n",
    "        n_kv_head  = get(hp, \"n_kv_head\", hp[\"n_head\"]),\n",
    "        dropout    = get(hp, \"dropout\", 0.1)\n",
    "    )\n",
    "    Flux.loadmodel!(m, data[\"model_state\"])\n",
    "    m = m |> device_fn\n",
    "\n",
    "    opt = data[\"opt_state\"]\n",
    "\n",
    "    println(\"  step=$(data[\\\"step\\\"]), best_val=$(round(data[\\\"best_val_loss\\\"], digits=4))\")\n",
    "    return (;\n",
    "        model = m,\n",
    "        opt_state = opt |> device_fn,\n",
    "        step = data[\"step\"],\n",
    "        best_val_loss = data[\"best_val_loss\"],\n",
    "        train_losses = get(data, \"train_losses\", Float64[]),\n",
    "        val_losses = get(data, \"val_losses\", Float64[])\n",
    "    )\n",
    "end\n",
    "\n",
    "println(\"Checkpoint save/load defined (JLD2 + HuggingFace sync)\")"
   ],
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "## 6. Training Loop\n",
    "\n",
    "Adam optimizer with cosine LR + warmup + gradient clipping.\n",
    "Reports both loss and perplexity. Curriculum learning advances automatically."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "using Printf\n",
    "\n",
    "# ── Generate text helper (defined here so training loop can call it) ──\n",
    "function generate_text(model, max_tokens=200; temperature=0.8, prompt=\"\")\n",
    "    model_eval = Flux.testmode!(deepcopy(model))\n",
    "    if !isempty(prompt)\n",
    "        prompt_ids = encode(prompt)\n",
    "        idx = reshape(prompt_ids, 1, :) |> device\n",
    "    else\n",
    "        idx = reshape([rand(1:vocab_size)], 1, 1) |> device\n",
    "    end\n",
    "    generated = Int[]\n",
    "    for _ in 1:max_tokens\n",
    "        idx_cond = idx[:, max(1, end-block_size+1):end]\n",
    "        logits = model_eval(idx_cond)\n",
    "        logits_last = logits[:, end, 1]\n",
    "        probs = softmax(logits_last ./ Float32(temperature))\n",
    "        probs_cpu = Float64.(cpu(probs))\n",
    "        r = rand()\n",
    "        cum = 0.0\n",
    "        next_id = 1\n",
    "        for (i, p) in enumerate(probs_cpu)\n",
    "            cum += p\n",
    "            if r <= cum\n",
    "                next_id = i\n",
    "                break\n",
    "            end\n",
    "        end\n",
    "        push!(generated, next_id)\n",
    "        next_token = reshape([next_id], 1, 1) |> device\n",
    "        idx = hcat(idx, next_token)\n",
    "    end\n",
    "    return decode(generated)\n",
    "end\n",
    "\n",
    "function estimate_loss(model, n_iters=eval_iters)\n",
    "    model_eval = Flux.testmode!(deepcopy(model))\n",
    "    losses = Dict{String, Float64}()\n",
    "    for split in [\"train\", \"val\"]\n",
    "        total = 0.0\n",
    "        for _ in 1:n_iters\n",
    "            x, y = get_batch(split)\n",
    "            logits = model_eval(x)\n",
    "            y_flat = reshape(y, :)\n",
    "            logits_flat = reshape(logits, vocab_size, :)\n",
    "            onehot = Flux.onehotbatch(y_flat, 1:vocab_size) |> device\n",
    "            loss = Flux.logitcrossentropy(logits_flat, onehot)\n",
    "            total += loss\n",
    "        end\n",
    "        losses[split] = total / n_iters\n",
    "    end\n",
    "    losses[\"train_ppl\"] = exp(losses[\"train\"])\n",
    "    losses[\"val_ppl\"] = exp(losses[\"val\"])\n",
    "    return losses\n",
    "end\n",
    "\n",
    "function compute_diversity(text::String)\n",
    "    words = split(text)\n",
    "    isempty(words) && return (distinct1=0.0, distinct2=0.0, rep_rate=0.0)\n",
    "    distinct1 = length(Set(words)) / length(words)\n",
    "    bigrams = [words[i] * \" \" * words[i+1] for i in 1:length(words)-1]\n",
    "    distinct2 = isempty(bigrams) ? 0.0 : length(Set(bigrams)) / length(bigrams)\n",
    "    if length(words) >= 3\n",
    "        trigrams = [join(words[i:i+2], \" \") for i in 1:length(words)-2]\n",
    "        rep_rate = 1.0 - length(Set(trigrams)) / length(trigrams)\n",
    "    else\n",
    "        rep_rate = 0.0\n",
    "    end\n",
    "    return (distinct1=round(distinct1, digits=3), distinct2=round(distinct2, digits=3), rep_rate=round(rep_rate, digits=3))\n",
    "end\n",
    "\n",
    "function get_lr(iter)\n",
    "    if iter < warmup_iters\n",
    "        return learning_rate * iter / warmup_iters\n",
    "    end\n",
    "    decay_ratio = (iter - warmup_iters) / (max_iters - warmup_iters)\n",
    "    coeff = 0.5 * (1.0 + cos(Float64(pi) * decay_ratio))\n",
    "    return min_lr + coeff * (learning_rate - min_lr)\n",
    "end\n",
    "\n",
    "# ── Optimizer with gradient clipping ──\n",
    "opt_state = Flux.setup(\n",
    "    OptimiserChain(ClipNorm(max_grad_norm), Adam(learning_rate)),\n",
    "    model\n",
    ")\n",
    "\n",
    "best_val = Inf\n",
    "train_loss_history = Float64[]\n",
    "val_loss_history = Float64[]\n",
    "\n",
    "if haskey(ENV, \"WANDB_API_KEY\") && !isempty(ENV[\"WANDB_API_KEY\"])\n",
    "    wandb_init()\n",
    "end\n",
    "\n",
    "SAVE_INTERVAL = 600\n",
    "last_save_time = time()\n",
    "completed_iter = 0\n",
    "\n",
    "println(\"Training for $max_iters steps (curriculum=$(curriculum_enabled))...\")\n",
    "t_start = time()\n",
    "\n",
    "try\n",
    "    for iter in 1:max_iters\n",
    "        global completed_iter = iter\n",
    "\n",
    "        # Advance curriculum\n",
    "        if curriculum_enabled\n",
    "            global curriculum_step = iter\n",
    "        end\n",
    "\n",
    "        lr_t = get_lr(iter)\n",
    "        Flux.adjust!(opt_state, lr_t)\n",
    "\n",
    "        x, y = get_batch(\"train\")\n",
    "        loss, grads = Flux.withgradient(model) do m\n",
    "            logits = m(x)\n",
    "            y_flat = reshape(y, :)\n",
    "            logits_flat = reshape(logits, vocab_size, :)\n",
    "            onehot = Flux.onehotbatch(y_flat, 1:vocab_size) |> device\n",
    "            Flux.logitcrossentropy(logits_flat, onehot)\n",
    "        end\n",
    "        Flux.update!(opt_state, model, grads[1])\n",
    "        push!(train_loss_history, Float64(loss))\n",
    "\n",
    "        if iter % 100 == 0 && CUDA.functional()\n",
    "            GC.gc(false)\n",
    "        end\n",
    "\n",
    "        if iter % eval_interval == 0 || iter == 1\n",
    "            losses = estimate_loss(model)\n",
    "            push!(val_loss_history, losses[\"val\"])\n",
    "            elapsed = round(time() - t_start, digits=1)\n",
    "            wandb_log(; step=iter, train_loss=losses[\"train\"], val_loss=losses[\"val\"],\n",
    "                       train_ppl=losses[\"train_ppl\"], val_ppl=losses[\"val_ppl\"], lr=lr_t)\n",
    "\n",
    "            improved = \"\"\n",
    "            if losses[\"val\"] < best_val\n",
    "                best_val = losses[\"val\"]\n",
    "                save_and_sync(\"checkpoints/best_model.jld2\", model, opt_state;\n",
    "                    step=iter, best_val_loss=best_val,\n",
    "                    train_losses=train_loss_history, val_losses=val_loss_history)\n",
    "                improved = \" << best!\"\n",
    "            end\n",
    "\n",
    "            phase_str = curriculum_enabled ? \" [$(curriculum_step < curriculum_warmup * 0.33 ? \"trivium\" : curriculum_step < curriculum_warmup * 0.66 ? \"tri+quad\" : \"full\")]\" : \"\"\n",
    "            @printf(\"step %5d | train %.4f (ppl %.1f) | val %.4f (ppl %.1f) | lr %.2e | %.1fs%s%s\\n\",\n",
    "                    iter, losses[\"train\"], losses[\"train_ppl\"],\n",
    "                    losses[\"val\"], losses[\"val_ppl\"], lr_t, elapsed, phase_str, improved)\n",
    "\n",
    "            # Diversity check every 5th eval\n",
    "            if iter % (eval_interval * 5) == 0\n",
    "                sample = generate_text(model, 200; temperature=0.8)\n",
    "                div = compute_diversity(sample)\n",
    "                @printf(\"  diversity: D1=%.3f D2=%.3f rep=%.3f\\n\", div.distinct1, div.distinct2, div.rep_rate)\n",
    "                wandb_log(; step=iter, distinct1=div.distinct1, distinct2=div.distinct2, rep_rate=div.rep_rate)\n",
    "            end\n",
    "        end\n",
    "\n",
    "        if iter % 1000 == 0\n",
    "            save_and_sync(\"checkpoints/checkpoint_latest.jld2\", model, opt_state;\n",
    "                step=iter, best_val_loss=best_val,\n",
    "                train_losses=train_loss_history, val_losses=val_loss_history)\n",
    "            last_save_time = time()\n",
    "        end\n",
    "\n",
    "        if time() - last_save_time > SAVE_INTERVAL\n",
    "            save_and_sync(\"checkpoints/checkpoint_latest.jld2\", model, opt_state;\n",
    "                step=iter, best_val_loss=best_val,\n",
    "                train_losses=train_loss_history, val_losses=val_loss_history)\n",
    "            last_save_time = time()\n",
    "            println(\"  [auto-save at step $iter]\")\n",
    "        end\n",
    "    end\n",
    "catch e\n",
    "    if e isa InterruptException\n",
    "        println(\"\\n\\nInterrupted at step $completed_iter!\")\n",
    "    else\n",
    "        println(\"\\n\\nError at step $completed_iter: $e\")\n",
    "    end\n",
    "    save_and_sync(\"checkpoints/checkpoint_interrupted.jld2\", model, opt_state;\n",
    "        step=completed_iter, best_val_loss=best_val,\n",
    "        train_losses=train_loss_history, val_losses=val_loss_history)\n",
    "    e isa InterruptException || rethrow(e)\n",
    "end\n",
    "\n",
    "elapsed = round(time() - t_start, digits=1)\n",
    "println(\"\\nTraining complete in $(elapsed)s. Best val loss: $(round(best_val, digits=4)) (ppl $(round(exp(best_val), digits=1)))\")\n",
    "wandb_finish()\n",
    "\n",
    "save_and_sync(\"checkpoints/final_model.jld2\", model, opt_state;\n",
    "    step=max_iters, best_val_loss=best_val,\n",
    "    train_losses=train_loss_history, val_losses=val_loss_history)"
   ],
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "## 7. Inference — Generate Text\n",
    "\n",
    "Temperature-controlled sampling with optional prompt."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "println(\"--- Generated Philosophy ---\")\n",
    "for i in 1:5\n",
    "    text = generate_text(model, 300; temperature=0.8)\n",
    "    @printf(\"\\nSample %d:\\n%s\\n\", i, text[1:min(end, 500)])\n",
    "    println(\"---\")\n",
    "end"
   ],
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "## 7a. Push Model to HuggingFace Hub"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "if @isdefined(HF_REPO_ID) && !isempty(HF_REPO_ID)\n",
    "    hf_create_repo(HF_REPO_ID)\n",
    "\n",
    "    if isfile(\"checkpoints/best_model.jld2\")\n",
    "        hf_push_checkpoint(HF_REPO_ID; checkpoint_path=\"checkpoints/best_model.jld2\")\n",
    "    else\n",
    "        println(\"No best_model.jld2 found -- train first!\")\n",
    "    end\n",
    "\n",
    "    if isfile(\"checkpoints/final_model.jld2\")\n",
    "        hf_push(HF_REPO_ID, \"checkpoints/final_model.jld2\")\n",
    "    end\n",
    "\n",
    "    println(\"\\nDone! View your model at: https://huggingface.co/$HF_REPO_ID\")\n",
    "else\n",
    "    println(\"Set HF_REPO_ID in the login cell (e.g. \\\"yourusername/juliaflux-philosophy\\\")\")\n",
    "end"
   ],
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "## 7b. Pull Checkpoint from HuggingFace Hub"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "if @isdefined(HF_REPO_ID) && !isempty(HF_REPO_ID)\n",
    "    mkpath(\"checkpoints\")\n",
    "    hf_pull(HF_REPO_ID, \"best_model.jld2\"; local_dir=\"checkpoints\")\n",
    "    println(\"\\nReady to resume from checkpoints/best_model.jld2\")\n",
    "    println(\"Run the 'Resume Training' cell below.\")\n",
    "else\n",
    "    println(\"Set HF_REPO_ID in the login cell (e.g. \\\"yourusername/juliaflux-philosophy\\\")\")\n",
    "end"
   ],
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "## 8. Resume Training from Checkpoint"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "RESUME_FROM = \"checkpoints/best_model.jld2\"\n",
    "EXTRA_ITERS = 2000\n",
    "\n",
    "if !isfile(RESUME_FROM)\n",
    "    if @isdefined(HF_REPO_ID) && !isempty(HF_REPO_ID)\n",
    "        println(\"Checkpoint not found locally, pulling from HuggingFace...\")\n",
    "        hf_pull(HF_REPO_ID, basename(RESUME_FROM); local_dir=\"checkpoints\")\n",
    "    end\n",
    "    isfile(RESUME_FROM) || error(\"Checkpoint not found: $RESUME_FROM\")\n",
    "end\n",
    "\n",
    "ckpt = load_checkpoint(RESUME_FROM, device)\n",
    "model = ckpt.model\n",
    "opt_state = ckpt.opt_state\n",
    "start_iter = ckpt.step + 1\n",
    "best_val = ckpt.best_val_loss\n",
    "train_loss_history = copy(ckpt.train_losses)\n",
    "val_loss_history = copy(ckpt.val_losses)\n",
    "end_iter = ckpt.step + EXTRA_ITERS\n",
    "\n",
    "if haskey(ENV, \"WANDB_API_KEY\") && !isempty(ENV[\"WANDB_API_KEY\"])\n",
    "    wandb_init()\n",
    "end\n",
    "\n",
    "println(\"\\nResuming from step $(ckpt.step) -> training to step $end_iter\")\n",
    "println(\"Best val loss so far: $(round(best_val, digits=4))\")\n",
    "t_start = time()\n",
    "last_save_time = time()\n",
    "\n",
    "try\n",
    "    for iter in start_iter:end_iter\n",
    "        global completed_iter = iter\n",
    "\n",
    "        if curriculum_enabled\n",
    "            global curriculum_step = iter\n",
    "        end\n",
    "\n",
    "        lr_t = get_lr(min(iter, max_iters))\n",
    "        Flux.adjust!(opt_state, lr_t)\n",
    "\n",
    "        x, y = get_batch(\"train\")\n",
    "        loss, grads = Flux.withgradient(model) do m\n",
    "            logits = m(x)\n",
    "            y_flat = reshape(y, :)\n",
    "            logits_flat = reshape(logits, vocab_size, :)\n",
    "            onehot = Flux.onehotbatch(y_flat, 1:vocab_size) |> device\n",
    "            Flux.logitcrossentropy(logits_flat, onehot)\n",
    "        end\n",
    "        Flux.update!(opt_state, model, grads[1])\n",
    "        push!(train_loss_history, Float64(loss))\n",
    "\n",
    "        if iter % 100 == 0 && CUDA.functional()\n",
    "            GC.gc(false)\n",
    "        end\n",
    "\n",
    "        if iter % eval_interval == 0\n",
    "            losses = estimate_loss(model)\n",
    "            push!(val_loss_history, losses[\"val\"])\n",
    "            elapsed = round(time() - t_start, digits=1)\n",
    "            wandb_log(; step=iter, train_loss=losses[\"train\"], val_loss=losses[\"val\"],\n",
    "                       train_ppl=losses[\"train_ppl\"], val_ppl=losses[\"val_ppl\"], lr=lr_t)\n",
    "\n",
    "            improved = \"\"\n",
    "            if losses[\"val\"] < best_val\n",
    "                best_val = losses[\"val\"]\n",
    "                save_and_sync(\"checkpoints/best_model.jld2\", model, opt_state;\n",
    "                    step=iter, best_val_loss=best_val,\n",
    "                    train_losses=train_loss_history, val_losses=val_loss_history)\n",
    "                improved = \" << best!\"\n",
    "            end\n",
    "\n",
    "            @printf(\"step %5d / %5d | train %.4f (ppl %.1f) | val %.4f (ppl %.1f) | lr %.2e | %.1fs%s\\n\",\n",
    "                    iter, end_iter, losses[\"train\"], losses[\"train_ppl\"],\n",
    "                    losses[\"val\"], losses[\"val_ppl\"], lr_t, elapsed, improved)\n",
    "        end\n",
    "\n",
    "        if iter % 1000 == 0\n",
    "            save_and_sync(\"checkpoints/checkpoint_latest.jld2\", model, opt_state;\n",
    "                step=iter, best_val_loss=best_val,\n",
    "                train_losses=train_loss_history, val_losses=val_loss_history)\n",
    "            last_save_time = time()\n",
    "        end\n",
    "\n",
    "        if time() - last_save_time > SAVE_INTERVAL\n",
    "            save_and_sync(\"checkpoints/checkpoint_latest.jld2\", model, opt_state;\n",
    "                step=iter, best_val_loss=best_val,\n",
    "                train_losses=train_loss_history, val_losses=val_loss_history)\n",
    "            last_save_time = time()\n",
    "            println(\"  [auto-save at step $iter]\")\n",
    "        end\n",
    "    end\n",
    "catch e\n",
    "    if e isa InterruptException\n",
    "        println(\"\\n\\nTraining interrupted at step $completed_iter!\")\n",
    "    else\n",
    "        println(\"\\n\\nTraining error at step $completed_iter: $e\")\n",
    "    end\n",
    "    save_and_sync(\"checkpoints/checkpoint_interrupted.jld2\", model, opt_state;\n",
    "        step=completed_iter, best_val_loss=best_val,\n",
    "        train_losses=train_loss_history, val_losses=val_loss_history)\n",
    "    e isa InterruptException || rethrow(e)\n",
    "end\n",
    "\n",
    "elapsed = round(time() - t_start, digits=1)\n",
    "@printf(\"\\nResume training complete in %.1fs\\n\", elapsed)\n",
    "wandb_finish()\n",
    "\n",
    "save_and_sync(\"checkpoints/final_model.jld2\", model, opt_state;\n",
    "    step=end_iter, best_val_loss=best_val,\n",
    "    train_losses=train_loss_history, val_losses=val_loss_history)"
   ],
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "## 9. Download Checkpoint"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "if isdir(\"checkpoints\")\n",
    "    files = readdir(\"checkpoints\")\n",
    "    println(\"Saved checkpoints:\")\n",
    "    for f in files\n",
    "        path = joinpath(\"checkpoints\", f)\n",
    "        size_kb = round(filesize(path) / 1024, digits=1)\n",
    "        println(\"  $path ($(size_kb) KB)\")\n",
    "    end\n",
    "else\n",
    "    println(\"No checkpoints directory found. Train first!\")\n",
    "end"
   ],
   "metadata": {},
   "outputs": [],
   "execution_count": null
  }
 ]
}