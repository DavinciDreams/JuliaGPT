{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.10",
   "language": "julia",
   "name": "julia-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.5"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<a href=\"https://colab.research.google.com/github/DavinciDreams/JuliaGPT/blob/main/juliaflux_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# JuliaFlux v2 — SOTA Small Language Model\n\nGPU-accelerated small language model trained on classical texts using the trivium/quadrivium curriculum.\nUses Flux.jl + CUDA.jl with modern LLaMA-style architecture.\n\n**Architecture (LLaMA-style):**\n- RMSNorm (replaces LayerNorm)\n- Rotary Position Embeddings (RoPE, replaces learned position embeddings)\n- SwiGLU activation (replaces GELU FFN)\n- Grouped Query Attention (GQA: 6 Q heads, 2 KV heads)\n- Weight-tied output projection\n- Gradient clipping, cosine LR with warmup\n- BPE tokenization with char-level fallback\n- Curriculum learning (trivium → quadrivium → philosophy)\n\nBased on: https://github.com/DavinciDreams/JuliaGPT"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 0. Login & Setup\n\nThis cell runs in **Python** to read your Colab secrets and save them for Julia.\n\n1. Add secrets via the key icon in the left sidebar:\n   - `HF_TOKEN` — your HuggingFace access token\n   - `WANDB_KEY` — your Weights & Biases API key\n   - `HF_REPO` — your model repo (e.g. `LisaMegaWatts/JuliaFluxGPT`)\n   - `HF_DATA_REPO` — your dataset repo (e.g. `LisaMegaWatts/philosophy-corpus`)\n2. Run cells 0-1 (login + install Julia, ~3-5 min)\n3. **Runtime > Change runtime type > Julia 1.10**\n4. Continue with the remaining Julia cells"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": "# ── Minimal Python setup: install tools + save Colab secrets for Julia ──\n!pip install -q wandb huggingface_hub datasets tokenizers\n\nimport json, pathlib, os\n\nsecrets = {}\ntry:\n    from google.colab import userdata\n    for key in (\"HF_TOKEN\", \"WANDB_KEY\", \"HF_REPO\", \"HF_DATA_REPO\"):\n        try: secrets[key] = userdata.get(key)\n        except Exception: pass\nexcept ImportError:\n    pass\n\nsecrets_path = pathlib.Path.home() / \".julia_secrets.json\"\nsecrets_path.write_text(json.dumps(secrets))\nsecrets_path.chmod(0o600)\n\nfound = [k for k in secrets if secrets[k]]\nprint(f\"Secrets saved: {', '.join(found) if found else 'none found'}\")\n\n# ── Download pre-cleaned dataset from HuggingFace ──\nDEFAULT_DATA_REPO = \"LisaMegaWatts/philosophy-corpus\"\ndata_repo = secrets.get(\"HF_DATA_REPO\", \"\") or DEFAULT_DATA_REPO\ndata_dir = pathlib.Path(\"data\")\ntrain_file = data_dir / \"train.txt\"\nval_file = data_dir / \"val.txt\"\n\nif not (train_file.exists() and val_file.exists()):\n    print(f\"\\nDownloading dataset from HuggingFace: {data_repo}\")\n    data_dir.mkdir(exist_ok=True)\n    if secrets.get(\"HF_TOKEN\"):\n        os.environ[\"HF_TOKEN\"] = secrets[\"HF_TOKEN\"]\n    try:\n        from datasets import load_dataset\n        ds = load_dataset(data_repo)\n        with open(train_file, \"w\") as f:\n            for row in ds[\"train\"]:\n                f.write(row[\"text\"] + \"\\n\")\n        split_name = \"validation\" if \"validation\" in ds else \"val\"\n        with open(val_file, \"w\") as f:\n            for row in ds[split_name]:\n                f.write(row[\"text\"] + \"\\n\")\n        print(f\"  train.txt: {train_file.stat().st_size:,} bytes\")\n        print(f\"  val.txt:   {val_file.stat().st_size:,} bytes\")\n    except Exception as e:\n        print(f\"  Dataset download failed: {e}\")\nelse:\n    print(f\"\\nDataset already downloaded: {data_dir}/\")\n\n# ── Download tokenizer.json if available ──\ntokenizer_file = data_dir / \"tokenizer.json\"\nif not tokenizer_file.exists():\n    try:\n        from huggingface_hub import hf_hub_download\n        hf_hub_download(repo_id=data_repo, filename=\"tokenizer.json\",\n                       local_dir=str(data_dir), repo_type=\"dataset\")\n        print(f\"  tokenizer.json downloaded\")\n    except Exception:\n        print(\"  No tokenizer.json on HuggingFace (will use char-level tokenizer)\")\n\n# ── Download curriculum phase files if available ──\nfor phase in [\"trivium\", \"quadrivium\", \"philosophy\"]:\n    phase_file = data_dir / f\"train_{phase}.txt\"\n    if not phase_file.exists():\n        try:\n            from huggingface_hub import hf_hub_download\n            hf_hub_download(repo_id=data_repo, filename=f\"train_{phase}.txt\",\n                           local_dir=str(data_dir), repo_type=\"dataset\")\n            print(f\"  train_{phase}.txt downloaded\")\n        except Exception:\n            pass\n\nprint(\"\\nDone! Now run the next cell to install Julia (~3-5 min).\")\nprint(\"Then: Runtime > Change runtime type > Julia 1.10\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1. Install Julia Kernel\nThis cell downloads and installs Julia + IJulia + Flux packages. **Takes ~5-10 minutes** on first run.\n\n**After it finishes:**\n1. Go to **Runtime > Change runtime type**\n2. Pick **Julia 1.10**\n3. Continue running the cells below"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "shell"
   },
   "outputs": [],
   "source": "%%shell\nset -e\n\nJULIA_VERSION=\"1.10.5\"\nJULIA_MINOR=\"1.10\"\n\nif [ ! -d \"/usr/local/julia-${JULIA_VERSION}\" ]; then\n    echo \"Downloading Julia ${JULIA_VERSION}...\"\n    wget -q https://julialang-s3.julialang.org/bin/linux/x64/${JULIA_MINOR}/julia-${JULIA_VERSION}-linux-x86_64.tar.gz\n    tar xzf julia-${JULIA_VERSION}-linux-x86_64.tar.gz -C /usr/local/\n    rm julia-${JULIA_VERSION}-linux-x86_64.tar.gz\n    ln -sf /usr/local/julia-${JULIA_VERSION}/bin/julia /usr/local/bin/julia\n    echo \"Julia installed.\"\nelse\n    echo \"Julia already installed.\"\nfi\n\njulia -e '\n    using Pkg\n    Pkg.add(\"IJulia\")\n    Pkg.add([\"Flux\", \"Zygote\", \"Optimisers\", \"CUDA\", \"cuDNN\",\n             \"Downloads\", \"Statistics\", \"Random\", \"Printf\",\n             \"LinearAlgebra\", \"JLD2\", \"NNlib\", \"JSON3\"])\n    using IJulia\n    installkernel(\"Julia\")\n'\n\necho \"\"\necho \"===========================================================\"\necho \"  Julia kernel installed!                                   \"\necho \"  Now: Runtime -> Change runtime type -> pick Julia 1.10   \"\necho \"  Then run the cells below.                                 \"\necho \"===========================================================\""
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1b. Load Credentials + W&B / HuggingFace Helpers (Julia)\n\nReads tokens from `~/.julia_secrets.json` (written by the Python setup cell)."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "julia"
   },
   "outputs": [],
   "source": "using JSON3\n\n# ── Ensure pip-installed binaries (huggingface-cli, wandb) are on PATH ──\nfor p in [\"/usr/local/bin\", joinpath(homedir(), \".local/bin\"), \"/root/.local/bin\"]\n    if isdir(p) && !occursin(p, get(ENV, \"PATH\", \"\"))\n        ENV[\"PATH\"] = p * \":\" * get(ENV, \"PATH\", \"\")\n    end\nend\n\n# ── Read credentials from ~/.julia_secrets.json (written by Python setup cell) ──\nfunction load_secrets()\n    path = expanduser(\"~/.julia_secrets.json\")\n    if !isfile(path)\n        @warn \"No secrets file found at $path — run the Python setup cell first\"\n        return Dict{String,String}()\n    end\n    raw = JSON3.read(read(path, String))\n    return Dict{String,String}(string(k) => string(v) for (k, v) in pairs(raw) if !isempty(string(v)))\nend\n\nsecrets = load_secrets()\n\n# W&B\nif haskey(secrets, \"WANDB_KEY\")\n    ENV[\"WANDB_API_KEY\"] = secrets[\"WANDB_KEY\"]\n    println(\"W&B API key: found\")\nelse\n    println(\"W&B API key: not found (add WANDB_KEY to Colab secrets)\")\nend\n\n# HuggingFace token\nif haskey(secrets, \"HF_TOKEN\")\n    ENV[\"HF_TOKEN\"] = secrets[\"HF_TOKEN\"]\n    println(\"HF token: found\")\nelse\n    println(\"HF token: not found (add HF_TOKEN to Colab secrets)\")\nend\n\n# HuggingFace repo ID\nHF_REPO_ID = get(secrets, \"HF_REPO\", \"\")\nif !isempty(HF_REPO_ID)\n    println(\"HF repo: \", HF_REPO_ID)\nelse\n    println(\"HF repo: not set (add HF_REPO to Colab secrets or set HF_REPO_ID manually)\")\nend"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "julia"
   },
   "outputs": [],
   "source": "WANDB_PROJECT = \"juliaflux-v2-philosophy\"\nWANDB_RUN_ID = \"juliaflux-\" * join(rand('a':'z', 6))\n\n# Write a tiny Python helper that reads JSON lines on stdin\nwrite(\"_wandb_log.py\", \"\"\"\nimport wandb, json, sys, os\nproject = os.environ.get(\"WANDB_PROJECT\", \"juliaflux-v2-philosophy\")\nrun_id = os.environ.get(\"WANDB_RUN_ID\", None)\nrun = wandb.init(project=project, id=run_id, resume=\"allow\",\n                 config={\"model\": \"juliaflux-gpt\", \"architecture\": \"llama-style transformer\"})\nprint(f\"W&B run: {run.url}\", flush=True)\nfor line in sys.stdin:\n    line = line.strip()\n    if not line:\n        continue\n    try:\n        data = json.loads(line)\n        wandb.log(data)\n    except Exception as e:\n        print(f\"wandb log error: {e}\", file=sys.stderr, flush=True)\nwandb.finish()\n\"\"\")\n\nwandb_proc = nothing\n\nfunction wandb_init()\n    global wandb_proc, WANDB_PROJECT, WANDB_RUN_ID\n    if !haskey(ENV, \"WANDB_API_KEY\") || isempty(ENV[\"WANDB_API_KEY\"])\n        println(\"W&B: skipped (no API key)\")\n        return\n    end\n    ENV[\"WANDB_PROJECT\"] = WANDB_PROJECT\n    ENV[\"WANDB_RUN_ID\"] = WANDB_RUN_ID\n    wandb_proc = open(`python3 _wandb_log.py`, \"r+\")\n    println(\"W&B: initialized ($WANDB_PROJECT / $WANDB_RUN_ID)\")\nend\n\nfunction wandb_log(; kwargs...)\n    global wandb_proc\n    wandb_proc === nothing && return\n    metrics = Dict(string(k) => v for (k, v) in kwargs)\n    try\n        println(wandb_proc, JSON3.write(metrics))\n        flush(wandb_proc)\n    catch e\n        println(\"W&B log error: $e\")\n    end\nend\n\nfunction wandb_finish()\n    global wandb_proc\n    wandb_proc === nothing && return\n    try close(wandb_proc) catch end\n    wandb_proc = nothing\n    println(\"W&B: run finished\")\nend\n\n# ═══════════════════════════════════════════════════════════════\n# HuggingFace Hub helpers\n# ═══════════════════════════════════════════════════════════════\n\nfunction hf_push(repo_id::String, local_path::String; remote_path::String=\"\")\n    rp = isempty(remote_path) ? basename(local_path) : remote_path\n    run(`huggingface-cli upload $repo_id $local_path $rp`)\n    println(\"Pushed $local_path -> $repo_id/$rp\")\nend\n\nfunction hf_pull(repo_id::String, remote_path::String; local_dir::String=\"checkpoints\")\n    mkpath(local_dir)\n    run(`huggingface-cli download $repo_id $remote_path --local-dir $local_dir`)\n    println(\"Pulled $repo_id/$remote_path -> $local_dir/\")\nend\n\nfunction hf_push_checkpoint(repo_id::String; checkpoint_path::String=\"checkpoints/best_model.jld2\")\n    isfile(checkpoint_path) || error(\"Checkpoint not found: $checkpoint_path\")\n    hf_push(repo_id, checkpoint_path)\nend\n\nfunction hf_create_repo(repo_id::String)\n    try\n        run(`huggingface-cli repo create $repo_id --type model`)\n        println(\"Created HF repo: $repo_id\")\n    catch\n        println(\"HF repo already exists or creation skipped: $repo_id\")\n    end\nend\n\nfunction hf_sync(local_path::String)\n    if !@isdefined(HF_REPO_ID) || isempty(HF_REPO_ID)\n        return\n    end\n    try\n        hf_push(HF_REPO_ID, local_path)\n    catch e\n        println(\"  HF sync failed: $e\")\n    end\nend\n\nprintln(\"W&B + HuggingFace helpers defined\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## 2. Imports & Setup\nLoad Flux.jl ecosystem packages. CUDA is auto-detected for GPU acceleration."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "julia"
   },
   "outputs": [],
   "source": "using Flux\nusing Zygote\nusing Optimisers\nusing CUDA\nusing NNlib\nusing Downloads\nusing Statistics\nusing Random\nusing Printf\nusing LinearAlgebra\nusing JLD2\n\nRandom.seed!(1337)\n\ndevice = CUDA.functional() ? gpu : cpu\nprintln(\"Device: \", device)\nprintln(\"CUDA functional: \", CUDA.functional())\nif CUDA.functional()\n    println(\"GPU: \", CUDA.name(CUDA.device()))\n    mem = CUDA.totalmem(CUDA.device())\n    println(\"VRAM: \", round(mem / 1024^3, digits=1), \" GB\")\n    println(\"CUDA version: \", CUDA.runtime_version())\nend"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## 3. Hyperparameters\n\nSOTA architecture defaults: RoPE, SwiGLU, GQA, RMSNorm."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "julia"
   },
   "outputs": [],
   "source": "# ── Model architecture (LLaMA-style) ──\nblock_size     = 256       # context window\nn_embd         = 384       # embedding dim\nn_head         = 6         # Q attention heads\nn_kv_head      = 2         # KV heads for GQA (each KV head serves 3 Q heads)\nn_layer        = 6         # transformer layers\ndropout        = 0.1\nbias           = false\nrope_base      = 10000.0f0 # RoPE frequency base\n\n# ── Training ──\nbatch_size     = 64\nlearning_rate  = 3e-4\nmax_iters      = 5000\neval_interval  = 500\neval_iters     = 100\nwarmup_iters   = 200\nmin_lr         = 1e-5\nmax_grad_norm  = 1.0f0     # gradient clipping threshold\n\n# ── Curriculum learning ──\ncurriculum_enabled = true\ncurriculum_warmup  = 1000   # steps of trivium-only before mixing in harder texts\n\nprintln(\"Architecture: n_embd=$n_embd, n_layer=$n_layer, n_head=$n_head (Q), n_kv_head=$n_kv_head (KV)\")\nprintln(\"GQA ratio: $(n_head ÷ n_kv_head) Q heads per KV head\")\nprintln(\"Training: batch=$batch_size, lr=$learning_rate, iters=$max_iters, clip=$max_grad_norm\")\nprintln(\"Curriculum: enabled=$curriculum_enabled, warmup=$curriculum_warmup steps\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## 4. Dataset — Classical Curriculum\n\nLoads pre-cleaned philosophy corpus from the text pipeline, organized by\nthe classical trivium/quadrivium curriculum:\n\n1. **Trivium** (language arts): grammar, rhetoric, logic\n2. **Quadrivium** (mathematical arts): arithmetic, geometry, music, astronomy\n3. **Philosophy**: ethics, metaphysics, politics\n\nThe curriculum learning approach trains on simpler texts first (trivium),\nthen progressively adds harder material — inspired by \"Textbooks Are All You Need\"\n(Microsoft Phi research).\n\n**Data flow:**\n```\ntext-pipeline/ → clean → chunk → train.txt + train_trivium.txt + train_quadrivium.txt + train_philosophy.txt\n                                → push to HuggingFace\n                                                    ↓\njuliaflux_v2.ipynb → pulls data/ → BPE tokenize → curriculum batches → train\n```"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "julia"
   },
   "outputs": [],
   "source": "# ── Load pre-cleaned data from text pipeline ──\nDATA_DIR = \"data\"\ntrain_file = joinpath(DATA_DIR, \"train.txt\")\nval_file   = joinpath(DATA_DIR, \"val.txt\")\n\nDEFAULT_DATA_REPO = \"LisaMegaWatts/philosophy-corpus\"\nHF_DATA_REPO = let r = get(secrets, \"HF_DATA_REPO\", \"\"); isempty(r) ? DEFAULT_DATA_REPO : r end\n\n# Julia-side fallback: try huggingface-cli if Python download missed\nif !isfile(train_file) || !isfile(val_file)\n    println(\"Data not found locally, trying huggingface-cli download...\")\n    mkpath(DATA_DIR)\n    try\n        run(`huggingface-cli download $HF_DATA_REPO train.txt val.txt --local-dir $DATA_DIR`)\n    catch e\n        @warn \"Download failed: $e\"\n    end\nend\n\nif !isfile(train_file) || !isfile(val_file)\n    error(\"No data found in $DATA_DIR/. Re-run the Python setup cell.\")\nend\n\ntrain_text = read(train_file, String)\nval_text   = read(val_file, String)\n\nprintln(\"Data loaded from $DATA_DIR/ ($HF_DATA_REPO)\")\nprintln(\"  train.txt: $(length(train_text)) chars ($(count('\\n', train_text)) chunks)\")\nprintln(\"  val.txt:   $(length(val_text)) chars ($(count('\\n', val_text)) chunks)\")\n\n# ── Load curriculum phase files (optional) ──\nphase_data = Dict{String, String}()\nfor phase in [\"trivium\", \"quadrivium\", \"philosophy\"]\n    phase_file = joinpath(DATA_DIR, \"train_$(phase).txt\")\n    if isfile(phase_file)\n        phase_data[phase] = read(phase_file, String)\n        n_chunks = count('\\n', phase_data[phase])\n        println(\"  train_$(phase).txt: $(length(phase_data[phase])) chars ($n_chunks chunks)\")\n    end\nend\n\nif isempty(phase_data)\n    println(\"\\n  No curriculum phase files found — will use full corpus for all phases\")\n    curriculum_enabled = false\nend\n\n# ── Try to download tokenizer.json ──\ntokenizer_file = joinpath(DATA_DIR, \"tokenizer.json\")\nif !isfile(tokenizer_file)\n    try\n        run(`huggingface-cli download $HF_DATA_REPO tokenizer.json --local-dir $DATA_DIR`)\n        println(\"  Downloaded tokenizer.json\")\n    catch\n        println(\"  No tokenizer.json available (will use char-level tokenizer)\")\n    end\nend"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "julia"
   },
   "outputs": [],
   "source": "# ── Tokenizer: BPE with character-level fallback ──\nusing JSON3\n\nTOKENIZER_PATH = joinpath(DATA_DIR, \"tokenizer.json\")\nUSE_BPE = isfile(TOKENIZER_PATH)\n\nif USE_BPE\n    println(\"Loading BPE tokenizer from $TOKENIZER_PATH ...\")\n    tok_raw = read(TOKENIZER_PATH, String)\n    tok_json = JSON3.read(tok_raw)\n\n    # Parse vocabulary: token_string -> id (convert to 1-indexed for Julia)\n    bpe_vocab = Dict{String, Int}()\n    for (tok_str, id) in pairs(tok_json.model.vocab)\n        bpe_vocab[string(tok_str)] = Int(id) + 1\n    end\n\n    # Parse merges: ordered list of (a, b) pairs\n    bpe_merges = Vector{Tuple{String,String}}()\n    for merge_str in tok_json.model.merges\n        parts = split(string(merge_str), \" \", limit=2)\n        if length(parts) == 2\n            push!(bpe_merges, (String(parts[1]), String(parts[2])))\n        end\n    end\n\n    # Reverse vocab: id -> token_string\n    bpe_id_to_token = Dict{Int, String}(id => tok for (tok, id) in bpe_vocab)\n\n    global vocab_size = length(bpe_vocab)\n\n    # BPE encode: apply merges in priority order\n    function bpe_encode_word(word::Vector{String})\n        tokens = copy(word)\n        for (a, b) in bpe_merges\n            i = 1\n            while i < length(tokens)\n                if tokens[i] == a && tokens[i+1] == b\n                    tokens = vcat(tokens[1:i-1], [a * b], tokens[i+2:end])\n                else\n                    i += 1\n                end\n            end\n        end\n        return tokens\n    end\n\n    function encode(s::String)\n        # Byte-level BPE: each byte is a starting token\n        chars = [string(c) for c in s]\n        tokens = bpe_encode_word(chars)\n        ids = Int[]\n        for tok in tokens\n            id = get(bpe_vocab, tok, nothing)\n            if id !== nothing\n                push!(ids, id)\n            end\n        end\n        return ids\n    end\n\n    function decode(ids::Vector{Int})\n        tokens = [get(bpe_id_to_token, id, \"\") for id in ids]\n        return join(tokens)\n    end\n\n    println(\"BPE tokenizer: vocab_size=$vocab_size, $(length(bpe_merges)) merges\")\n\nelse\n    # ── Fallback: character-level tokenizer ──\n    println(\"No tokenizer.json found — using character-level tokenizer\")\n\n    full_text = train_text * \"\\n\" * val_text\n    chars = sort(unique(full_text))\n    filter!(c -> c != '\\n', chars)\n    global vocab_size = length(chars)\n\n    stoi = Dict(c => i for (i, c) in enumerate(chars))\n    itos = Dict(i => c for (i, c) in enumerate(chars))\n\n    encode(s::String) = [stoi[c] for c in s if haskey(stoi, c)]\n    decode(ids::Vector{Int}) = join(itos[i] for i in ids)\n\n    println(\"Char-level tokenizer: vocab_size=$vocab_size -> [$(join(chars))]\")\nend\n\n# ── Encode training and validation data ──\ntrain_clean = replace(strip(train_text), '\\n' => ' ')\nval_clean   = replace(strip(val_text), '\\n' => ' ')\n\nglobal train_data = encode(train_clean)\nglobal val_data   = encode(val_clean)\n\n# ── Encode curriculum phase data ──\nglobal phase_encoded = Dict{String, Vector{Int}}()\nfor (phase, text) in phase_data\n    clean = replace(strip(text), '\\n' => ' ')\n    phase_encoded[phase] = encode(clean)\n    println(\"  Phase $phase: $(length(phase_encoded[phase])) tokens\")\nend\n\nprintln(\"\\nTrain: $(length(train_data)) tokens\")\nprintln(\"Val:   $(length(val_data)) tokens\")\nprintln(\"Total: $(length(train_data) + length(val_data)) tokens\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## 5. Model Architecture\n\nLLaMA-style transformer with SOTA components.\nAll structs defined in one cell (Julia limitation: structs cannot be redefined).\n\n| Component | Old (v1) | New (v2) |\n|-----------|----------|----------|\n| Normalization | LayerNorm | **RMSNorm** |\n| Position encoding | Learned absolute | **RoPE** (rotary) |\n| FFN activation | GELU (2 matrices) | **SwiGLU** (3 matrices) |\n| Attention | Standard MHA | **GQA** (grouped query) |\n| Output head | Separate Dense | **Weight-tied** with embedding |\n| Gradient | No clipping | **ClipNorm(1.0)** |"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "julia"
   },
   "outputs": [],
   "source": "# ── Curriculum learning state ──\nglobal curriculum_step = 0\n\nfunction get_batch(split=\"train\")\n    global curriculum_step\n\n    if split == \"val\"\n        d = val_data\n    elseif curriculum_enabled && !isempty(phase_encoded)\n        # Curriculum: start with trivium, progressively add harder material\n        progress = min(curriculum_step / curriculum_warmup, 1.0)\n\n        if progress < 0.33 && haskey(phase_encoded, \"trivium\") && !isempty(phase_encoded[\"trivium\"])\n            d = phase_encoded[\"trivium\"]\n        elseif progress < 0.66\n            # Mix trivium + quadrivium\n            sources = Vector{Int}[]\n            haskey(phase_encoded, \"trivium\") && push!(sources, phase_encoded[\"trivium\"])\n            haskey(phase_encoded, \"quadrivium\") && push!(sources, phase_encoded[\"quadrivium\"])\n            d = isempty(sources) ? train_data : vcat(sources...)\n        else\n            d = train_data  # full corpus\n        end\n    else\n        d = train_data\n    end\n\n    # Ensure data is long enough\n    if length(d) <= block_size + 1\n        d = train_data\n    end\n\n    ix = rand(1:length(d) - block_size, batch_size)\n    x = hcat([d[i:i+block_size-1] for i in ix]...)\n    y = hcat([d[i+1:i+block_size] for i in ix]...)\n    x = permutedims(x)   # (B, T)\n    y = permutedims(y)\n    x = x |> device\n    y = y |> device\n    return x, y\nend"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "julia"
   },
   "outputs": [],
   "source": "# ══════════════════════════════════════════════════════════════════\n# ALL MODEL STRUCTS IN ONE CELL (Julia structs cannot be redefined)\n# Architecture: RMSNorm + RoPE + SwiGLU + GQA + Weight Tying\n# ══════════════════════════════════════════════════════════════════\n\nusing NNlib: batched_mul\n\n# ── Pre-compute causal mask ──\nconst CAUSAL_MASK = triu(fill(typemin(Float32), block_size, block_size), 1)\nconst CAUSAL_MASK_GPU = CUDA.functional() ? cu(CAUSAL_MASK) : CAUSAL_MASK\n\n# ──────────────────────────────────────────────────────────────────\n# RoPE: Rotary Position Embeddings\n# ──────────────────────────────────────────────────────────────────\n\nconst HEAD_DIM = n_embd ÷ n_head\n\nfunction precompute_rope_freqs(head_dim::Int, max_seq_len::Int; base::Float32 = 10000.0f0)\n    half_dim = head_dim ÷ 2\n    freqs = Float32[1.0f0 / (base ^ (Float32(2 * (i - 1)) / Float32(head_dim))) for i in 1:half_dim]\n    positions = Float32.(collect(0:max_seq_len-1))\n    angles = freqs * positions'\n    return cos.(angles), sin.(angles)\nend\n\nconst ROPE_COS, ROPE_SIN = precompute_rope_freqs(HEAD_DIM, block_size; base=rope_base)\nconst ROPE_COS_GPU = CUDA.functional() ? cu(ROPE_COS) : ROPE_COS\nconst ROPE_SIN_GPU = CUDA.functional() ? cu(ROPE_SIN) : ROPE_SIN\n\nfunction apply_rope(x, cos_f, sin_f, T::Int)\n    d = size(x, 1) ÷ 2\n    x1 = x[1:d, :, :]\n    x2 = x[d+1:2d, :, :]\n    c = cos_f[:, 1:T]\n    s = sin_f[:, 1:T]\n    return vcat(x1 .* c .- x2 .* s, x1 .* s .+ x2 .* c)\nend\n\n# ──────────────────────────────────────────────────────────────────\n# RMSNorm (replaces LayerNorm)\n# ──────────────────────────────────────────────────────────────────\n\nstruct RMSNorm{W <: AbstractVector}\n    weight::W\n    eps::Float32\nend\n\nFlux.@layer RMSNorm\n\nfunction RMSNorm(dim::Int; eps::Float32 = 1.0f-6)\n    RMSNorm(ones(Float32, dim), eps)\nend\n\nfunction (rn::RMSNorm)(x)\n    rms = sqrt.(mean(x .^ 2, dims=1) .+ rn.eps)\n    return (x ./ rms) .* rn.weight\nend\n\n# ──────────────────────────────────────────────────────────────────\n# SwiGLU Feed-Forward Network (replaces GELU FFN)\n# ──────────────────────────────────────────────────────────────────\n\nstruct SwiGLUFFN\n    w_gate::Dense\n    w_up::Dense\n    w_down::Dense\n    drop::Dropout\nend\n\nFlux.@layer SwiGLUFFN\n\nfunction SwiGLUFFN(n_embd::Int; bias=false, dropout=0.0)\n    raw_inner = Int(floor(4 * n_embd * 2 / 3))\n    inner_dim = max(64, 64 * div(raw_inner + 32, 64))\n    SwiGLUFFN(\n        Dense(n_embd => inner_dim; bias),\n        Dense(n_embd => inner_dim; bias),\n        Dense(inner_dim => n_embd; bias),\n        Dropout(dropout)\n    )\nend\n\nfunction (ff::SwiGLUFFN)(x)\n    ff.drop(ff.w_down(NNlib.swish(ff.w_gate(x)) .* ff.w_up(x)))\nend\n\n# ──────────────────────────────────────────────────────────────────\n# GQA-capable Causal Self-Attention\n# ──────────────────────────────────────────────────────────────────\n\nstruct CausalSelfAttention\n    wq::Dense\n    wkv::Dense\n    proj::Dense\n    n_head::Int\n    n_kv_head::Int\nend\n\nFlux.@layer CausalSelfAttention trainable=(wq, wkv, proj)\n\nfunction CausalSelfAttention(n_embd::Int, n_head::Int, n_kv_head::Int; bias=false)\n    head_dim = n_embd ÷ n_head\n    kv_dim = head_dim * n_kv_head\n    CausalSelfAttention(\n        Dense(n_embd => n_embd; bias),\n        Dense(n_embd => 2 * kv_dim; bias),\n        Dense(n_embd => n_embd; bias),\n        n_head,\n        n_kv_head\n    )\nend\n\nfunction (attn::CausalSelfAttention)(x)\n    C, T, B = size(x)\n    nh = attn.n_head\n    nkv = attn.n_kv_head\n    hs = C ÷ nh\n    kv_dim = hs * nkv\n    groups = nh ÷ nkv\n\n    q = attn.wq(x)\n    kv = attn.wkv(x)\n    k = kv[1:kv_dim, :, :]\n    v = kv[kv_dim+1:2*kv_dim, :, :]\n\n    # Reshape to per-head tensors\n    q = reshape(permutedims(reshape(q, hs, nh, T, B), (1, 3, 2, 4)), hs, T, nh * B)\n    k = reshape(permutedims(reshape(k, hs, nkv, T, B), (1, 3, 2, 4)), hs, T, nkv * B)\n    v = reshape(permutedims(reshape(v, hs, nkv, T, B), (1, 3, 2, 4)), hs, T, nkv * B)\n\n    # Apply RoPE to Q and K\n    cos_f = x isa CuArray ? ROPE_COS_GPU : ROPE_COS\n    sin_f = x isa CuArray ? ROPE_SIN_GPU : ROPE_SIN\n    q = apply_rope(q, cos_f, sin_f, T)\n    k = apply_rope(k, cos_f, sin_f, T)\n\n    # GQA: repeat KV heads to match Q heads\n    if groups > 1\n        k_4d = reshape(k, hs, T, nkv, B)\n        v_4d = reshape(v, hs, T, nkv, B)\n        k_rep = repeat(reshape(k_4d, hs, T, nkv, 1, B), 1, 1, 1, groups, 1)\n        v_rep = repeat(reshape(v_4d, hs, T, nkv, 1, B), 1, 1, 1, groups, 1)\n        k = reshape(permutedims(k_rep, (1, 2, 4, 3, 5)), hs, T, nh * B)\n        v = reshape(permutedims(v_rep, (1, 2, 4, 3, 5)), hs, T, nh * B)\n    end\n\n    # Attention scores\n    scale = Float32(1 / sqrt(hs))\n    wei = batched_mul(permutedims(q, (2, 1, 3)), k) .* scale\n\n    mask = x isa CuArray ? CAUSAL_MASK_GPU[1:T, 1:T] : CAUSAL_MASK[1:T, 1:T]\n    wei = wei .+ mask\n    wei = softmax(wei; dims=2)\n\n    out = batched_mul(v, permutedims(wei, (2, 1, 3)))\n    out = reshape(permutedims(reshape(out, hs, T, nh, B), (1, 3, 2, 4)), C, T, B)\n\n    attn.proj(out)\nend\n\n# ──────────────────────────────────────────────────────────────────\n# TransformerBlock (pre-norm residual with RMSNorm)\n# ──────────────────────────────────────────────────────────────────\n\nstruct TransformerBlock\n    ln1::RMSNorm\n    attn::CausalSelfAttention\n    ln2::RMSNorm\n    ffwd::SwiGLUFFN\nend\n\nFlux.@layer TransformerBlock\n\nfunction TransformerBlock(n_embd::Int, n_head::Int, n_kv_head::Int; dropout=0.0)\n    TransformerBlock(\n        RMSNorm(n_embd),\n        CausalSelfAttention(n_embd, n_head, n_kv_head),\n        RMSNorm(n_embd),\n        SwiGLUFFN(n_embd; dropout)\n    )\nend\n\nfunction (block::TransformerBlock)(x)\n    x = x .+ block.attn(block.ln1(x))\n    x = x .+ block.ffwd(block.ln2(x))\n    x\nend\n\n# ──────────────────────────────────────────────────────────────────\n# TiedDense: weight-tied output projection\n# ──────────────────────────────────────────────────────────────────\n\nstruct TiedDense{W <: AbstractMatrix}\n    weight_ref::W\nend\n\nFlux.@layer TiedDense trainable=()\n\nfunction (td::TiedDense)(x)\n    C, T, B = size(x)\n    W = td.weight_ref\n    x_flat = reshape(x, C, T * B)\n    out = W' * x_flat\n    reshape(out, size(W, 2), T, B)\nend\n\n# ──────────────────────────────────────────────────────────────────\n# GPT Model (LLaMA-style: no wpe, RoPE, weight-tied lm_head)\n# ──────────────────────────────────────────────────────────────────\n\nstruct GPT\n    wte::Embedding\n    drop::Dropout\n    blocks::Chain\n    ln_f::RMSNorm\n    lm_head::TiedDense\nend\n\nFlux.@layer GPT\n\nfunction GPT(; vocab_size, n_embd, block_size, n_layer, n_head, n_kv_head, dropout=0.1)\n    wte = Embedding(vocab_size => n_embd)\n    GPT(\n        wte,\n        Dropout(dropout),\n        Chain([TransformerBlock(n_embd, n_head, n_kv_head; dropout) for _ in 1:n_layer]...),\n        RMSNorm(n_embd),\n        TiedDense(wte.weight)\n    )\nend\n\nfunction (m::GPT)(idx)\n    B, T = size(idx)\n    tok = permutedims(m.wte(idx), (1, 3, 2))\n    x = m.drop(tok)\n    x = m.blocks(x)\n    x = m.ln_f(x)\n    m.lm_head(x)\nend\n\nprintln(\"Model structs defined: RMSNorm, SwiGLUFFN, CausalSelfAttention (GQA), TransformerBlock, TiedDense, GPT\")\nprintln(\"SOTA: RoPE (head_dim=$(HEAD_DIM)), SwiGLU, GQA ($(n_head)Q/$(n_kv_head)KV), RMSNorm, weight tying\")\nprintln(\"Pre-computed: causal mask $(size(CAUSAL_MASK)), RoPE tables $(size(ROPE_COS))\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "julia"
   },
   "outputs": [],
   "source": "model = GPT(;\n    vocab_size = vocab_size,\n    n_embd     = n_embd,\n    block_size = block_size,\n    n_layer    = n_layer,\n    n_head     = n_head,\n    n_kv_head  = n_kv_head,\n    dropout    = dropout\n) |> device\n\nn_params = sum(length, Flux.trainables(model))\nprintln(\"Model created on $device\")\nprintln(\"Parameters: $(n_params) ($(round(n_params/1e6, digits=2))M)\")\nprintln(\"  Weight tying saves $(vocab_size * n_embd) params = $(round(vocab_size * n_embd / 1e3, digits=1))K\")\n\nif CUDA.functional()\n    println(\"GPU memory: $(round(CUDA.used_memory() / 1024^2, digits=1)) MB\")\nend\n\n# Smoke test\nx_test, y_test = get_batch(\"train\")\nlogits_test = model(x_test)\nprintln(\"Forward pass OK — logits: $(size(logits_test))\")\n@assert size(logits_test, 1) == vocab_size\n@assert size(logits_test, 2) == block_size\n@assert size(logits_test, 3) == batch_size"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## 6. Checkpoint Save/Load"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "julia"
   },
   "outputs": [],
   "source": "LOCAL_CKPT = \"checkpoints\"\nmkpath(LOCAL_CKPT)\n\nfunction save_checkpoint(path::String, model, opt_state;\n                          step::Int=0, best_val_loss::Float64=Inf,\n                          train_losses::Vector{Float64}=Float64[],\n                          val_losses::Vector{Float64}=Float64[])\n    mkpath(dirname(path))\n    model_cpu = cpu(model)\n    opt_cpu = cpu(opt_state)\n    JLD2.jldsave(path;\n        model_state = Flux.state(model_cpu),\n        opt_state = opt_cpu,\n        step = step,\n        best_val_loss = best_val_loss,\n        train_losses = train_losses,\n        val_losses = val_losses,\n        hyperparams = Dict(\n            \"vocab_size\" => vocab_size,\n            \"n_embd\" => n_embd,\n            \"block_size\" => block_size,\n            \"n_layer\" => n_layer,\n            \"n_head\" => n_head,\n            \"n_kv_head\" => n_kv_head,\n            \"dropout\" => dropout,\n            \"use_bpe\" => USE_BPE,\n            \"rope_base\" => rope_base\n        )\n    )\n    vl_str = best_val_loss == Inf ? \"Inf\" : @sprintf(\"%.4f\", best_val_loss)\n    println(\"Checkpoint saved: $path (step $step, best_val_loss=$vl_str)\")\nend\n\nfunction save_and_sync(path, model, opt_state; kwargs...)\n    save_checkpoint(path, model, opt_state; kwargs...)\n    hf_sync(path)\nend\n\nfunction load_checkpoint(path::String, device_fn)\n    println(\"Loading checkpoint from $path ...\")\n    data = JLD2.load(path)\n\n    hp = data[\"hyperparams\"]\n    m = GPT(;\n        vocab_size = hp[\"vocab_size\"],\n        n_embd     = hp[\"n_embd\"],\n        block_size = hp[\"block_size\"],\n        n_layer    = hp[\"n_layer\"],\n        n_head     = hp[\"n_head\"],\n        n_kv_head  = get(hp, \"n_kv_head\", hp[\"n_head\"]),\n        dropout    = get(hp, \"dropout\", 0.1)\n    )\n    Flux.loadmodel!(m, data[\"model_state\"])\n    m = m |> device_fn\n\n    opt = data[\"opt_state\"]\n\n    println(\"  step=$(data[\\\"step\\\"]), best_val=$(round(data[\\\"best_val_loss\\\"], digits=4))\")\n    return (;\n        model = m,\n        opt_state = opt |> device_fn,\n        step = data[\"step\"],\n        best_val_loss = data[\"best_val_loss\"],\n        train_losses = get(data, \"train_losses\", Float64[]),\n        val_losses = get(data, \"val_losses\", Float64[])\n    )\nend\n\nprintln(\"Checkpoint save/load defined (JLD2 + HuggingFace sync)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## 7. Training Loop\n\nAdam optimizer with cosine LR + warmup + gradient clipping.\nReports both loss and perplexity. Curriculum learning advances automatically."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "julia"
   },
   "outputs": [],
   "source": "using Printf\n\n# ── Generate text helper (defined here so training loop can call it) ──\nfunction generate_text(model, max_tokens=200; temperature=0.8, prompt=\"\")\n    model_eval = Flux.testmode!(deepcopy(model))\n    if !isempty(prompt)\n        prompt_ids = encode(prompt)\n        idx = reshape(prompt_ids, 1, :) |> device\n    else\n        idx = reshape([rand(1:vocab_size)], 1, 1) |> device\n    end\n    generated = Int[]\n    for _ in 1:max_tokens\n        idx_cond = idx[:, max(1, end-block_size+1):end]\n        logits = model_eval(idx_cond)\n        logits_last = logits[:, end, 1]\n        probs = softmax(logits_last ./ Float32(temperature))\n        probs_cpu = Float64.(cpu(probs))\n        r = rand()\n        cum = 0.0\n        next_id = 1\n        for (i, p) in enumerate(probs_cpu)\n            cum += p\n            if r <= cum\n                next_id = i\n                break\n            end\n        end\n        push!(generated, next_id)\n        next_token = reshape([next_id], 1, 1) |> device\n        idx = hcat(idx, next_token)\n    end\n    return decode(generated)\nend\n\nfunction estimate_loss(model, n_iters=eval_iters)\n    model_eval = Flux.testmode!(deepcopy(model))\n    losses = Dict{String, Float64}()\n    for split in [\"train\", \"val\"]\n        total = 0.0\n        for _ in 1:n_iters\n            x, y = get_batch(split)\n            logits = model_eval(x)\n            y_flat = reshape(y, :)\n            logits_flat = reshape(logits, vocab_size, :)\n            onehot = Flux.onehotbatch(y_flat, 1:vocab_size) |> device\n            loss = Flux.logitcrossentropy(logits_flat, onehot)\n            total += loss\n        end\n        losses[split] = total / n_iters\n    end\n    losses[\"train_ppl\"] = exp(losses[\"train\"])\n    losses[\"val_ppl\"] = exp(losses[\"val\"])\n    return losses\nend\n\nfunction compute_diversity(text::String)\n    words = split(text)\n    isempty(words) && return (distinct1=0.0, distinct2=0.0, rep_rate=0.0)\n    distinct1 = length(Set(words)) / length(words)\n    bigrams = [words[i] * \" \" * words[i+1] for i in 1:length(words)-1]\n    distinct2 = isempty(bigrams) ? 0.0 : length(Set(bigrams)) / length(bigrams)\n    if length(words) >= 3\n        trigrams = [join(words[i:i+2], \" \") for i in 1:length(words)-2]\n        rep_rate = 1.0 - length(Set(trigrams)) / length(trigrams)\n    else\n        rep_rate = 0.0\n    end\n    return (distinct1=round(distinct1, digits=3), distinct2=round(distinct2, digits=3), rep_rate=round(rep_rate, digits=3))\nend\n\nfunction get_lr(iter)\n    if iter < warmup_iters\n        return learning_rate * iter / warmup_iters\n    end\n    decay_ratio = (iter - warmup_iters) / (max_iters - warmup_iters)\n    coeff = 0.5 * (1.0 + cos(Float64(pi) * decay_ratio))\n    return min_lr + coeff * (learning_rate - min_lr)\nend\n\n# ── Optimizer with gradient clipping ──\nopt_state = Flux.setup(\n    OptimiserChain(ClipNorm(max_grad_norm), Adam(learning_rate)),\n    model\n)\n\nbest_val = Inf\ntrain_loss_history = Float64[]\nval_loss_history = Float64[]\n\nif haskey(ENV, \"WANDB_API_KEY\") && !isempty(ENV[\"WANDB_API_KEY\"])\n    wandb_init()\nend\n\nSAVE_INTERVAL = 600\nlast_save_time = time()\ncompleted_iter = 0\n\nprintln(\"Training for $max_iters steps (curriculum=$(curriculum_enabled))...\")\nt_start = time()\n\ntry\n    for iter in 1:max_iters\n        global completed_iter = iter\n\n        # Advance curriculum\n        if curriculum_enabled\n            global curriculum_step = iter\n        end\n\n        lr_t = get_lr(iter)\n        Flux.adjust!(opt_state, lr_t)\n\n        x, y = get_batch(\"train\")\n        loss, grads = Flux.withgradient(model) do m\n            logits = m(x)\n            y_flat = reshape(y, :)\n            logits_flat = reshape(logits, vocab_size, :)\n            onehot = Flux.onehotbatch(y_flat, 1:vocab_size) |> device\n            Flux.logitcrossentropy(logits_flat, onehot)\n        end\n        Flux.update!(opt_state, model, grads[1])\n        push!(train_loss_history, Float64(loss))\n\n        if iter % 100 == 0 && CUDA.functional()\n            GC.gc(false)\n        end\n\n        if iter % eval_interval == 0 || iter == 1\n            losses = estimate_loss(model)\n            push!(val_loss_history, losses[\"val\"])\n            elapsed = round(time() - t_start, digits=1)\n            wandb_log(; step=iter, train_loss=losses[\"train\"], val_loss=losses[\"val\"],\n                       train_ppl=losses[\"train_ppl\"], val_ppl=losses[\"val_ppl\"], lr=lr_t)\n\n            improved = \"\"\n            if losses[\"val\"] < best_val\n                best_val = losses[\"val\"]\n                save_and_sync(\"checkpoints/best_model.jld2\", model, opt_state;\n                    step=iter, best_val_loss=best_val,\n                    train_losses=train_loss_history, val_losses=val_loss_history)\n                improved = \" << best!\"\n            end\n\n            phase_str = curriculum_enabled ? \" [$(curriculum_step < curriculum_warmup * 0.33 ? \"trivium\" : curriculum_step < curriculum_warmup * 0.66 ? \"tri+quad\" : \"full\")]\" : \"\"\n            @printf(\"step %5d | train %.4f (ppl %.1f) | val %.4f (ppl %.1f) | lr %.2e | %.1fs%s%s\\n\",\n                    iter, losses[\"train\"], losses[\"train_ppl\"],\n                    losses[\"val\"], losses[\"val_ppl\"], lr_t, elapsed, phase_str, improved)\n\n            # Diversity check every 5th eval\n            if iter % (eval_interval * 5) == 0\n                sample = generate_text(model, 200; temperature=0.8)\n                div = compute_diversity(sample)\n                @printf(\"  diversity: D1=%.3f D2=%.3f rep=%.3f\\n\", div.distinct1, div.distinct2, div.rep_rate)\n                wandb_log(; step=iter, distinct1=div.distinct1, distinct2=div.distinct2, rep_rate=div.rep_rate)\n            end\n        end\n\n        if iter % 1000 == 0\n            save_and_sync(\"checkpoints/checkpoint_latest.jld2\", model, opt_state;\n                step=iter, best_val_loss=best_val,\n                train_losses=train_loss_history, val_losses=val_loss_history)\n            last_save_time = time()\n        end\n\n        if time() - last_save_time > SAVE_INTERVAL\n            save_and_sync(\"checkpoints/checkpoint_latest.jld2\", model, opt_state;\n                step=iter, best_val_loss=best_val,\n                train_losses=train_loss_history, val_losses=val_loss_history)\n            last_save_time = time()\n            println(\"  [auto-save at step $iter]\")\n        end\n    end\ncatch e\n    if e isa InterruptException\n        println(\"\\n\\nInterrupted at step $completed_iter!\")\n    else\n        println(\"\\n\\nError at step $completed_iter: $e\")\n    end\n    save_and_sync(\"checkpoints/checkpoint_interrupted.jld2\", model, opt_state;\n        step=completed_iter, best_val_loss=best_val,\n        train_losses=train_loss_history, val_losses=val_loss_history)\n    e isa InterruptException || rethrow(e)\nend\n\nelapsed = round(time() - t_start, digits=1)\nprintln(\"\\nTraining complete in $(elapsed)s. Best val loss: $(round(best_val, digits=4)) (ppl $(round(exp(best_val), digits=1)))\")\nwandb_finish()\n\nsave_and_sync(\"checkpoints/final_model.jld2\", model, opt_state;\n    step=max_iters, best_val_loss=best_val,\n    train_losses=train_loss_history, val_losses=val_loss_history)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## 8. Inference — Generate Text\n\nTemperature-controlled sampling with optional prompt."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "julia"
   },
   "outputs": [],
   "source": "println(\"--- Generated Philosophy ---\")\nfor i in 1:5\n    text = generate_text(model, 300; temperature=0.8)\n    @printf(\"\\nSample %d:\\n%s\\n\", i, text[1:min(end, 500)])\n    println(\"---\")\nend"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## 8a. Push Model to HuggingFace Hub\nPush your trained checkpoint to HuggingFace for persistence across Colab sessions.\nSet `HF_REPO_ID` in the login cell above."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "julia"
   },
   "outputs": [],
   "source": "if @isdefined(HF_REPO_ID) && !isempty(HF_REPO_ID)\n    hf_create_repo(HF_REPO_ID)\n\n    if isfile(\"checkpoints/best_model.jld2\")\n        hf_push_checkpoint(HF_REPO_ID; checkpoint_path=\"checkpoints/best_model.jld2\")\n    else\n        println(\"No best_model.jld2 found -- train first!\")\n    end\n\n    if isfile(\"checkpoints/final_model.jld2\")\n        hf_push(HF_REPO_ID, \"checkpoints/final_model.jld2\")\n    end\n\n    println(\"\\nDone! View your model at: https://huggingface.co/$HF_REPO_ID\")\nelse\n    println(\"Set HF_REPO_ID in the login cell (e.g. \\\"yourusername/juliaflux-philosophy\\\")\")\nend"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## 8b. Pull Checkpoint from HuggingFace Hub\nDownload a previously pushed checkpoint to resume training in a new Colab session."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "julia"
   },
   "outputs": [],
   "source": "if @isdefined(HF_REPO_ID) && !isempty(HF_REPO_ID)\n    mkpath(\"checkpoints\")\n    hf_pull(HF_REPO_ID, \"best_model.jld2\"; local_dir=\"checkpoints\")\n    println(\"\\nReady to resume from checkpoints/best_model.jld2\")\n    println(\"Run the 'Resume Training' cell below.\")\nelse\n    println(\"Set HF_REPO_ID in the login cell (e.g. \\\"yourusername/juliaflux-philosophy\\\")\")\nend"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## 9. Resume Training from Checkpoint\nLoad a saved checkpoint and continue training for more steps.\nSkip this cell if you're training from scratch above."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "julia"
   },
   "outputs": [],
   "source": "RESUME_FROM = \"checkpoints/best_model.jld2\"\nEXTRA_ITERS = 2000\n\nif !isfile(RESUME_FROM)\n    if @isdefined(HF_REPO_ID) && !isempty(HF_REPO_ID)\n        println(\"Checkpoint not found locally, pulling from HuggingFace...\")\n        hf_pull(HF_REPO_ID, basename(RESUME_FROM); local_dir=\"checkpoints\")\n    end\n    isfile(RESUME_FROM) || error(\"Checkpoint not found: $RESUME_FROM\")\nend\n\nckpt = load_checkpoint(RESUME_FROM, device)\nmodel = ckpt.model\nopt_state = ckpt.opt_state\nstart_iter = ckpt.step + 1\nbest_val = ckpt.best_val_loss\ntrain_loss_history = copy(ckpt.train_losses)\nval_loss_history = copy(ckpt.val_losses)\nend_iter = ckpt.step + EXTRA_ITERS\n\nif haskey(ENV, \"WANDB_API_KEY\") && !isempty(ENV[\"WANDB_API_KEY\"])\n    wandb_init()\nend\n\nprintln(\"\\nResuming from step $(ckpt.step) -> training to step $end_iter\")\nprintln(\"Best val loss so far: $(round(best_val, digits=4))\")\nt_start = time()\nlast_save_time = time()\n\ntry\n    for iter in start_iter:end_iter\n        global completed_iter = iter\n\n        if curriculum_enabled\n            global curriculum_step = iter\n        end\n\n        lr_t = get_lr(min(iter, max_iters))\n        Flux.adjust!(opt_state, lr_t)\n\n        x, y = get_batch(\"train\")\n        loss, grads = Flux.withgradient(model) do m\n            logits = m(x)\n            y_flat = reshape(y, :)\n            logits_flat = reshape(logits, vocab_size, :)\n            onehot = Flux.onehotbatch(y_flat, 1:vocab_size) |> device\n            Flux.logitcrossentropy(logits_flat, onehot)\n        end\n        Flux.update!(opt_state, model, grads[1])\n        push!(train_loss_history, Float64(loss))\n\n        if iter % 100 == 0 && CUDA.functional()\n            GC.gc(false)\n        end\n\n        if iter % eval_interval == 0\n            losses = estimate_loss(model)\n            push!(val_loss_history, losses[\"val\"])\n            elapsed = round(time() - t_start, digits=1)\n            wandb_log(; step=iter, train_loss=losses[\"train\"], val_loss=losses[\"val\"],\n                       train_ppl=losses[\"train_ppl\"], val_ppl=losses[\"val_ppl\"], lr=lr_t)\n\n            improved = \"\"\n            if losses[\"val\"] < best_val\n                best_val = losses[\"val\"]\n                save_and_sync(\"checkpoints/best_model.jld2\", model, opt_state;\n                    step=iter, best_val_loss=best_val,\n                    train_losses=train_loss_history, val_losses=val_loss_history)\n                improved = \" << best!\"\n            end\n\n            @printf(\"step %5d / %5d | train %.4f (ppl %.1f) | val %.4f (ppl %.1f) | lr %.2e | %.1fs%s\\n\",\n                    iter, end_iter, losses[\"train\"], losses[\"train_ppl\"],\n                    losses[\"val\"], losses[\"val_ppl\"], lr_t, elapsed, improved)\n        end\n\n        if iter % 1000 == 0\n            save_and_sync(\"checkpoints/checkpoint_latest.jld2\", model, opt_state;\n                step=iter, best_val_loss=best_val,\n                train_losses=train_loss_history, val_losses=val_loss_history)\n            last_save_time = time()\n        end\n\n        if time() - last_save_time > SAVE_INTERVAL\n            save_and_sync(\"checkpoints/checkpoint_latest.jld2\", model, opt_state;\n                step=iter, best_val_loss=best_val,\n                train_losses=train_loss_history, val_losses=val_loss_history)\n            last_save_time = time()\n            println(\"  [auto-save at step $iter]\")\n        end\n    end\ncatch e\n    if e isa InterruptException\n        println(\"\\n\\nTraining interrupted at step $completed_iter!\")\n    else\n        println(\"\\n\\nTraining error at step $completed_iter: $e\")\n    end\n    save_and_sync(\"checkpoints/checkpoint_interrupted.jld2\", model, opt_state;\n        step=completed_iter, best_val_loss=best_val,\n        train_losses=train_loss_history, val_losses=val_loss_history)\n    e isa InterruptException || rethrow(e)\nend\n\nelapsed = round(time() - t_start, digits=1)\n@printf(\"\\nResume training complete in %.1fs\\n\", elapsed)\nwandb_finish()\n\nsave_and_sync(\"checkpoints/final_model.jld2\", model, opt_state;\n    step=end_iter, best_val_loss=best_val,\n    train_losses=train_loss_history, val_losses=val_loss_history)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## 10. Download Checkpoint\n\nDownload the best model checkpoint to use elsewhere.\nIn Colab, use the Files panel (left sidebar) to download, or pull from HuggingFace Hub."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "julia"
   },
   "outputs": [],
   "source": "if isdir(\"checkpoints\")\n    files = readdir(\"checkpoints\")\n    println(\"Saved checkpoints:\")\n    for f in files\n        path = joinpath(\"checkpoints\", f)\n        size_kb = round(filesize(path) / 1024, digits=1)\n        println(\"  $path ($(size_kb) KB)\")\n    end\nelse\n    println(\"No checkpoints directory found. Train first!\")\nend"
  }
 ]
}