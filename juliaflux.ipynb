{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "name": "julia",
   "display_name": "Julia"
  },
  "language_info": {
   "name": "julia"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/DavinciDreams/JuliaGPT/blob/main/juliaflux.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JuliaFlux GPT — A Flux.jl Transformer for Philosophy",
    "",
    "GPU-accelerated character-level GPT trained on classical philosophy texts.",
    "Uses Flux.jl + Zygote.jl + CUDA.jl for automatic differentiation and GPU compute.",
    "",
    "**Architecture** (following GPT-2 with simplifications):",
    "- Multi-head causal self-attention with batched matrix ops",
    "- LayerNorm, GELU activations, residual connections",
    "- Cosine LR schedule with warmup",
    "- JLD2 checkpoint persistence (local + Google Drive)",
    "- W&B logging + HuggingFace Hub integration",
    "",
    "Based on: https://github.com/DavinciDreams/JuliaGPT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Login & Setup",
    "",
    "This cell runs in **Python** to read your Colab secrets and set up credentials.",
    "",
    "1. Add secrets via the key icon in the left sidebar:",
    "   - `HF_TOKEN` — your HuggingFace access token",
    "   - `WANDB_KEY` — your Weights & Biases API key",
    "   - `HF_REPO` — your model repo (e.g. `LisaMegaWatts/JuliaFluxGPT`)",
    "2. Run cells 0-1 (login + install Julia, ~3-5 min)",
    "3. **Runtime > Change runtime type > Julia 1.10**",
    "4. Continue with the remaining Julia cells"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ── Login to HF + W&B and fetch training data ──\n",
    "!pip install -q wandb huggingface_hub\n",
    "\n",
    "import os, pathlib\n",
    "\n",
    "# ── Read Colab secrets ──\n",
    "hf_token = \"\"\n",
    "wandb_key = \"\"\n",
    "hf_repo = \"\"\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    try: hf_token = userdata.get(\"HF_TOKEN\")\n",
    "    except Exception: pass\n",
    "    try: wandb_key = userdata.get(\"WANDB_KEY\")\n",
    "    except Exception: pass\n",
    "    try: hf_repo = userdata.get(\"HF_REPO\")\n",
    "    except Exception: pass\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "# ── Write tokens + repo to ~/.netrc (persists across kernel switch) ──\n",
    "netrc_path = pathlib.Path.home() / \".netrc\"\n",
    "netrc_lines = []\n",
    "if hf_token:\n",
    "    netrc_lines.extend([\"machine huggingface.co\", \"login hf\", \"password \" + hf_token, \"\"])\n",
    "if wandb_key:\n",
    "    netrc_lines.extend([\"machine api.wandb.ai\", \"login user\", \"password \" + wandb_key, \"\"])\n",
    "if hf_repo:\n",
    "    netrc_lines.extend([\"machine hf.repo\", \"login default\", \"password \" + hf_repo, \"\"])\n",
    "if netrc_lines:\n",
    "    netrc_path.write_text(chr(10).join(netrc_lines))\n",
    "    netrc_path.chmod(0o600)\n",
    "    print(f\"Credentials saved to {netrc_path}\")\n",
    "else:\n",
    "    print(\"No secrets found. Add HF_TOKEN, WANDB_KEY, HF_REPO via key icon.\")\n",
    "\n",
    "# ── Mount Google Drive for persistent checkpoints ──\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount(\"/content/drive\")\n",
    "    drive_dir = \"/content/drive/MyDrive/JuliaFluxGPT\"\n",
    "    os.makedirs(drive_dir + \"/checkpoints\", exist_ok=True)\n",
    "    print(f\"Drive mounted: {drive_dir}/\")\n",
    "except Exception as e:\n",
    "    print(f\"Drive mount skipped: {e}\")\n",
    "\n",
    "print(\"\\nDone! Now run the next cell to install Julia (~3-5 min).\")\n",
    "print(\"Then: Runtime > Change runtime type > Julia 1.10\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Julia Kernel",
    "",
    "This cell downloads and installs Julia + IJulia + Flux packages. **Takes ~5-10 minutes** on first run.",
    "",
    "**After it finishes:**",
    "1. Go to **Runtime > Change runtime type**",
    "2. Pick **Julia 1.10**",
    "3. Continue running the cells below"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "%%shell\n",
    "set -e\n",
    "\n",
    "JULIA_VERSION=\"1.10.5\"\n",
    "JULIA_MINOR=\"1.10\"\n",
    "\n",
    "if [ ! -d \"/usr/local/julia-${JULIA_VERSION}\" ]; then\n",
    "    echo \"Downloading Julia ${JULIA_VERSION}...\"\n",
    "    wget -q https://julialang-s3.julialang.org/bin/linux/x64/${JULIA_MINOR}/julia-${JULIA_VERSION}-linux-x86_64.tar.gz\n",
    "    tar xzf julia-${JULIA_VERSION}-linux-x86_64.tar.gz -C /usr/local/\n",
    "    rm julia-${JULIA_VERSION}-linux-x86_64.tar.gz\n",
    "    ln -sf /usr/local/julia-${JULIA_VERSION}/bin/julia /usr/local/bin/julia\n",
    "    echo \"Julia installed.\"\n",
    "else\n",
    "    echo \"Julia already installed.\"\n",
    "fi\n",
    "\n",
    "julia -e '\n",
    "    using Pkg\n",
    "    Pkg.add(\"IJulia\")\n",
    "    Pkg.add([\"Flux\", \"Zygote\", \"Optimisers\", \"CUDA\", \"cuDNN\",\n",
    "             \"Downloads\", \"Statistics\", \"Random\", \"Printf\",\n",
    "             \"LinearAlgebra\", \"JLD2\", \"NNlib\", \"JSON3\"])\n",
    "    using IJulia\n",
    "    installkernel(\"Julia\")\n",
    "'\n",
    "\n",
    "echo \"\"\n",
    "echo \"===========================================================\"\n",
    "echo \"  Julia kernel installed!                                   \"\n",
    "echo \"  Now: Runtime -> Change runtime type -> pick Julia 1.10       \"\n",
    "echo \"  Then run the cells below.                              \"\n",
    "echo \"===========================================================\" \n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1b. W&B + HuggingFace Helpers (Julia)",
    "",
    "W&B logging uses a persistent Python subprocess fed JSON lines from Julia.",
    "HuggingFace helpers use `huggingface-cli` to push/pull checkpoints.",
    "Credentials were saved to disk by the login cell above."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ── Ensure pip-installed binaries (huggingface-cli, wandb) are on PATH ──\n",
    "for p in [\"/usr/local/bin\", joinpath(homedir(), \".local/bin\"), \"/root/.local/bin\"]\n",
    "    if isdir(p) && !occursin(p, get(ENV, \"PATH\", \"\"))\n",
    "        ENV[\"PATH\"] = p * \":\" * get(ENV, \"PATH\", \"\")\n",
    "    end\n",
    "end\n",
    "\n",
    "# ── Read credentials + config from ~/.netrc (written by Python login cell) ──\n",
    "function load_netrc_tokens()\n",
    "    netrc = expanduser(\"~/.netrc\")\n",
    "    tokens = Dict{String,String}()\n",
    "    if !isfile(netrc)\n",
    "        return tokens\n",
    "    end\n",
    "    lines = readlines(netrc)\n",
    "    current_machine = \"\"\n",
    "    for line in lines\n",
    "        line = strip(line)\n",
    "        m = match(r\"^machine\\s+(\\S+)\", line)\n",
    "        if m !== nothing\n",
    "            current_machine = m.captures[1]\n",
    "        end\n",
    "        m = match(r\"^password\\s+(\\S+)\", line)\n",
    "        if m !== nothing\n",
    "            tokens[current_machine] = m.captures[1]\n",
    "        end\n",
    "    end\n",
    "    return tokens\n",
    "end\n",
    "\n",
    "netrc_tokens = load_netrc_tokens()\n",
    "\n",
    "# W&B\n",
    "wandb_key = get(netrc_tokens, \"api.wandb.ai\", \"\")\n",
    "if !isempty(wandb_key)\n",
    "    ENV[\"WANDB_API_KEY\"] = wandb_key\n",
    "    println(\"W&B API key: found\")\n",
    "else\n",
    "    println(\"W&B API key: not found (run Python login cell first)\")\n",
    "end\n",
    "\n",
    "# HuggingFace token\n",
    "hf_token = get(netrc_tokens, \"huggingface.co\", \"\")\n",
    "if !isempty(hf_token)\n",
    "    ENV[\"HF_TOKEN\"] = hf_token\n",
    "    println(\"HF token: found\")\n",
    "else\n",
    "    println(\"HF token: not found (run Python login cell first)\")\n",
    "end\n",
    "\n",
    "# HuggingFace repo ID\n",
    "HF_REPO_ID = get(netrc_tokens, \"hf.repo\", \"\")\n",
    "if !isempty(HF_REPO_ID)\n",
    "    println(\"HF repo: \", HF_REPO_ID)\n",
    "else\n",
    "    println(\"HF repo: not set (add HF_REPO to Colab secrets or set HF_REPO_ID manually)\")\n",
    "end\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "using JSON3\n",
    "\n",
    "WANDB_PROJECT = \"juliaflux-philosophy\"\n",
    "WANDB_RUN_ID = \"juliaflux-\" * join(rand('a':'z', 6))\n",
    "\n",
    "# Write a tiny Python helper that reads JSON lines on stdin\n",
    "write(\"_wandb_log.py\", \"\"\"\n",
    "import wandb, json, sys, os\n",
    "project = os.environ.get(\"WANDB_PROJECT\", \"juliaflux-philosophy\")\n",
    "run_id = os.environ.get(\"WANDB_RUN_ID\", None)\n",
    "run = wandb.init(project=project, id=run_id, resume=\"allow\",\n",
    "                 config={\"model\": \"juliaflux-gpt\", \"architecture\": \"multi-layer transformer\"})\n",
    "print(f\"W&B run: {run.url}\", flush=True)\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "    if not line:\n",
    "        continue\n",
    "    try:\n",
    "        data = json.loads(line)\n",
    "        wandb.log(data)\n",
    "    except Exception as e:\n",
    "        print(f\"wandb log error: {e}\", file=sys.stderr, flush=True)\n",
    "wandb.finish()\n",
    "\"\"\")\n",
    "\n",
    "wandb_proc = nothing\n",
    "\n",
    "function wandb_init()\n",
    "    global wandb_proc, WANDB_PROJECT, WANDB_RUN_ID\n",
    "    if !haskey(ENV, \"WANDB_API_KEY\") || isempty(ENV[\"WANDB_API_KEY\"])\n",
    "        println(\"W&B: skipped (no API key)\")\n",
    "        return\n",
    "    end\n",
    "    ENV[\"WANDB_PROJECT\"] = WANDB_PROJECT\n",
    "    ENV[\"WANDB_RUN_ID\"] = WANDB_RUN_ID\n",
    "    wandb_proc = open(`python3 _wandb_log.py`, \"r+\")\n",
    "    println(\"W&B: initialized ($WANDB_PROJECT / $WANDB_RUN_ID)\")\n",
    "end\n",
    "\n",
    "function wandb_log(; kwargs...)\n",
    "    global wandb_proc\n",
    "    wandb_proc === nothing && return\n",
    "    metrics = Dict(string(k) => v for (k, v) in kwargs)\n",
    "    try\n",
    "        println(wandb_proc, JSON3.write(metrics))\n",
    "        flush(wandb_proc)\n",
    "    catch e\n",
    "        println(\"W&B log error: $e\")\n",
    "    end\n",
    "end\n",
    "\n",
    "function wandb_finish()\n",
    "    global wandb_proc\n",
    "    wandb_proc === nothing && return\n",
    "    try close(wandb_proc) catch end\n",
    "    wandb_proc = nothing\n",
    "    println(\"W&B: run finished\")\n",
    "end\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "# HuggingFace Hub helpers\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "\n",
    "function hf_push(repo_id::String, local_path::String; remote_path::String=\"\")\n",
    "    rp = isempty(remote_path) ? basename(local_path) : remote_path\n",
    "    run(`huggingface-cli upload $repo_id $local_path $rp`)\n",
    "    println(\"Pushed $local_path -> $repo_id/$rp\")\n",
    "end\n",
    "\n",
    "function hf_pull(repo_id::String, remote_path::String; local_dir::String=\"checkpoints\")\n",
    "    mkpath(local_dir)\n",
    "    run(`huggingface-cli download $repo_id $remote_path --local-dir $local_dir`)\n",
    "    println(\"Pulled $repo_id/$remote_path -> $local_dir/\")\n",
    "end\n",
    "\n",
    "function hf_push_checkpoint(repo_id::String; checkpoint_path::String=\"checkpoints/best_model.jld2\")\n",
    "    isfile(checkpoint_path) || error(\"Checkpoint not found: $checkpoint_path\")\n",
    "    hf_push(repo_id, checkpoint_path)\n",
    "end\n",
    "\n",
    "function hf_create_repo(repo_id::String)\n",
    "    try\n",
    "        run(`huggingface-cli repo create $repo_id --type model`)\n",
    "        println(\"Created HF repo: $repo_id\")\n",
    "    catch\n",
    "        println(\"HF repo already exists or creation skipped: $repo_id\")\n",
    "    end\n",
    "end\n",
    "\n",
    "println(\"W&B + HuggingFace helpers defined\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---",
    "## 2. Imports & Setup",
    "",
    "Load Flux.jl ecosystem packages. CUDA is auto-detected for GPU acceleration."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "using Flux\n",
    "using Zygote\n",
    "using Optimisers\n",
    "using CUDA\n",
    "using NNlib\n",
    "using Downloads\n",
    "using Statistics\n",
    "using Random\n",
    "using Printf\n",
    "using LinearAlgebra\n",
    "using JLD2\n",
    "\n",
    "Random.seed!(1337)\n",
    "\n",
    "device = CUDA.functional() ? gpu : cpu\n",
    "println(\"Device: \", device)\n",
    "println(\"CUDA functional: \", CUDA.functional())\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---",
    "## 3. Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "block_size     = 128\n",
    "n_embd         = 256\n",
    "n_head         = 4\n",
    "n_layer        = 4\n",
    "dropout        = 0.1\n",
    "bias           = false\n",
    "\n",
    "batch_size     = 32\n",
    "learning_rate  = 4e-4\n",
    "max_iters      = 5000\n",
    "eval_interval  = 500\n",
    "eval_iters     = 100\n",
    "warmup_iters   = 200\n",
    "min_lr         = 1e-5\n",
    "\n",
    "println(\"n_embd = $n_embd, n_layer = $n_layer, n_head = $n_head\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---",
    "## 4. Dataset — Download & Tokenize",
    "",
    "Downloads classical philosophy texts from Project Gutenberg and MIT Classics.",
    "Character-level tokenizer with 90/10 train/val split."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "function download_and_clean(url::String, fn::String; is_gutenberg=true)\n    if !isfile(fn)\n        println(\"Downloading $fn from $url\")\n        try\n            Downloads.download(url, fn)\n        catch e\n            @warn \"Download failed: $url -> $e\"\n            return \"\"\n        end\n    end\n\n    txt = read(fn, String)\n\n    if is_gutenberg\n        txt = replace(txt, r\"(?is)^.*?\\*{3}\\s*START OF (THE|THIS) PROJECT GUTENBERG.*?\\*{3}[\\r\\n]*\" => \"\")\n        txt = replace(txt, r\"(?is)\\*{3}\\s*END OF (THE|THIS) PROJECT GUTENBERG.*$\" => \"\")\n    end\n\n    txt = replace(txt, r\"\\r\\n\" => \"\\n\")\n    txt = replace(txt, r\"\\n{3,}\" => \"\\n\\n\")\n    txt = strip(txt)\n\n    return txt\nend\n\nsources = Dict(\n    \"grammar\"             => (\"https://www.gutenberg.org/files/15665/15665-0.txt\",        \"latin_grammar.txt\",      true),\n    \"categories\"          => (\"https://www.gutenberg.org/ebooks/2412.txt.utf-8\",          \"aristotle_categories.txt\", true),\n    \"rhetoric\"            => (\"http://classics.mit.edu/Aristotle/rhetoric.mb.txt\",        \"aristotle_rhetoric.txt\", false),\n    \"prior_analytics\"     => (\"http://classics.mit.edu/Aristotle/prior.mb.txt\",           \"prior_analytics.txt\",    false),\n    \"posterior_analytics\" => (\"http://classics.mit.edu/Aristotle/posterior.mb.txt\",       \"posterior_analytics.txt\", false),\n    \"topics\"              => (\"http://classics.mit.edu/Aristotle/topics.mb.txt\",          \"topics.txt\",             false),\n    \"boethius\"            => (\"https://www.gutenberg.org/files/14328/14328-0.txt\",        \"boethius_consolation.txt\", true),\n    \"heavens\"             => (\"http://classics.mit.edu/Aristotle/heavens.mb.txt\",         \"aristotle_heavens.txt\",  false),\n    \"republic\"            => (\"https://www.gutenberg.org/files/1497/1497-0.txt\",          \"plato_republic.txt\",     true),\n    \"apology\"             => (\"https://www.gutenberg.org/files/1656/1656-0.txt\",          \"plato_apology.txt\",      true),\n    \"ethics\"              => (\"https://www.gutenberg.org/files/8438/8438-0.txt\",          \"aristotle_ethics.txt\",   true),\n    \"emerson\"             => (\"https://www.gutenberg.org/files/2944/2944-0.txt\",          \"emerson_essays.txt\",     true),\n    \"walden\"              => (\"https://www.gutenberg.org/files/205/205-0.txt\",            \"thoreau_walden.txt\",     true),\n    \"epicurus\"            => (\"https://www.gutenberg.org/files/57342/57342-0.txt\",        \"diogenes_epicurus.txt\",  true)\n)\n\ntexts = Dict{String,String}()\nfor (key, (url, fn, is_gut)) in sources\n    texts[key] = download_and_clean(url, fn; is_gutenberg=is_gut)\nend\n\nprintln(\"Downloaded $(length(texts)) texts.\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Unicode normalization\nfor fn in [\n    \"latin_grammar.txt\",\n    \"aristotle_categories.txt\",\n    \"aristotle_rhetoric.txt\",\n    \"prior_analytics.txt\",\n    \"posterior_analytics.txt\",\n    \"topics.txt\",\n    \"boethius_consolation.txt\",\n    \"aristotle_heavens.txt\",\n    \"plato_republic.txt\",\n    \"plato_apology.txt\",\n    \"aristotle_ethics.txt\",\n    \"emerson_essays.txt\",\n    \"thoreau_walden.txt\",\n    \"diogenes_epicurus.txt\"\n]\n    isfile(fn) || continue\n\n    txt = read(fn, String)\n    txt = replace(txt,\n        \"\\u201c\" => \"\\\"\", \"\\u201d\" => \"\\\"\",\n        \"\\u2018\" => \"'\", \"\\u2019\" => \"'\",\n        \"\\u2014\" => \"--\", \"\\u2013\" => \"-\",\n        \"\\u2026\" => \"...\",\n        \"\\u00A0\" => \" \"\n    )\n\n    txt = replace(txt, r\"\\n{3,}\" => \"\\n\\n\")\n    txt = strip(txt)\n\n    open(fn, \"w\") do io\n        write(io, txt)\n    end\n\n    println(\"Normalized: $fn\")\nend",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "full_text = \"\"\nfor fn in [\n    \"latin_grammar.txt\",\n    \"aristotle_categories.txt\",\n    \"aristotle_rhetoric.txt\",\n    \"prior_analytics.txt\",\n    \"posterior_analytics.txt\",\n    \"topics.txt\",\n    \"boethius_consolation.txt\",\n    \"aristotle_heavens.txt\",\n    \"plato_republic.txt\",\n    \"plato_apology.txt\",\n    \"aristotle_ethics.txt\",\n    \"emerson_essays.txt\",\n    \"thoreau_walden.txt\",\n    \"diogenes_epicurus.txt\"\n]\n    isfile(fn) || continue\n    content = read(fn, String)\n    full_text *= \"\\n\\n=== $fn ===\\n\\n\" * content\nend\n\nfull_text = strip(full_text)\n\nchars = sort(unique(full_text))\nglobal vocab_size = length(chars)\n\nstoi = Dict(c => i for (i,c) in enumerate(chars))\nitos = Dict(i => c for (i,c) in enumerate(chars))\n\nencode(s) = [stoi[c] for c in s]\ndecode(ids) = join(itos[i] for i in ids)\n\ndata = encode(full_text)\nn = length(data)\ntrain_split = floor(Int, n * 0.9)\nglobal train_data = data[1:train_split]\nglobal val_data   = data[train_split+1:end]\n\nprintln(\"Vocab size: \", vocab_size)\nprintln(\"Train tokens: \", length(train_data))",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---",
    "## 5. Batch Loader & Model Architecture",
    "",
    "Mini-batch loader + full GPT model definition using Flux.jl structs."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "function get_batch(split=\"train\")\n",
    "    d = split == \"train\" ? train_data : val_data\n",
    "    ix = rand(1:length(d)-block_size, batch_size)\n",
    "    x = hcat([d[i:i+block_size-1] for i in ix]...)\n",
    "    y = hcat([d[i+1:i+block_size] for i in ix]...)\n",
    "    x = permutedims(x)   # now (B, T)\n",
    "    y = permutedims(y)\n",
    "    x = x |> device\n",
    "    y = y |> device\n",
    "    return x, y\n",
    "end\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ── All model structs in ONE cell (Julia structs can't be redefined) ──\n",
    "using NNlib: batched_mul\n",
    "\n",
    "struct CausalSelfAttention\n",
    "    qkv::Dense       # single projection: n_embd -> 3*n_embd\n",
    "    proj::Dense       # output projection: n_embd -> n_embd\n",
    "    n_head::Int\n",
    "end\n",
    "\n",
    "Flux.@layer CausalSelfAttention trainable=(qkv, proj)\n",
    "\n",
    "function CausalSelfAttention(n_embd::Int, n_head::Int; bias=false)\n",
    "    CausalSelfAttention(\n",
    "        Dense(n_embd => 3 * n_embd; bias),\n",
    "        Dense(n_embd => n_embd; bias),\n",
    "        n_head\n",
    "    )\n",
    "end\n",
    "\n",
    "function (attn::CausalSelfAttention)(x)\n",
    "    C, T, B = size(x)\n",
    "    hs = C / attn.n_head |> Int\n",
    "    nh = attn.n_head\n",
    "\n",
    "    qkv = attn.qkv(x)\n",
    "    q = qkv[1:C, :, :]\n",
    "    k = qkv[C+1:2C, :, :]\n",
    "    v = qkv[2C+1:3C, :, :]\n",
    "\n",
    "    q = reshape(permutedims(reshape(q, hs, nh, T, B), (1, 3, 2, 4)), hs, T, nh * B)\n",
    "    k = reshape(permutedims(reshape(k, hs, nh, T, B), (1, 3, 2, 4)), hs, T, nh * B)\n",
    "    v = reshape(permutedims(reshape(v, hs, nh, T, B), (1, 3, 2, 4)), hs, T, nh * B)\n",
    "\n",
    "    scale = Float32(1 / sqrt(hs))\n",
    "    wei = batched_mul(permutedims(q, (2, 1, 3)), k) .* scale\n",
    "\n",
    "    mask = triu(fill(typemin(Float32), T, T), 1)\n",
    "    wei = wei .+ mask\n",
    "    wei = softmax(wei; dims=2)\n",
    "\n",
    "    out = batched_mul(v, permutedims(wei, (2, 1, 3)))\n",
    "    out = reshape(permutedims(reshape(out, hs, T, nh, B), (1, 3, 2, 4)), C, T, B)\n",
    "\n",
    "    attn.proj(out)\n",
    "end\n",
    "\n",
    "struct FeedForward\n",
    "    net::Chain\n",
    "end\n",
    "\n",
    "Flux.@layer FeedForward\n",
    "\n",
    "function FeedForward(n_embd::Int; bias=false, dropout=0.0)\n",
    "    FeedForward(Chain(\n",
    "        Dense(n_embd => 4 * n_embd; bias),\n",
    "        gelu,\n",
    "        Dense(4 * n_embd => n_embd; bias),\n",
    "        Dropout(dropout)\n",
    "    ))\n",
    "end\n",
    "\n",
    "(ff::FeedForward)(x) = ff.net(x)\n",
    "\n",
    "struct TransformerBlock\n",
    "    ln1::LayerNorm\n",
    "    attn::CausalSelfAttention\n",
    "    ln2::LayerNorm\n",
    "    ffwd::FeedForward\n",
    "end\n",
    "\n",
    "Flux.@layer TransformerBlock\n",
    "\n",
    "function TransformerBlock(n_embd::Int, n_head::Int; dropout=0.0)\n",
    "    TransformerBlock(\n",
    "        LayerNorm(n_embd),\n",
    "        CausalSelfAttention(n_embd, n_head),\n",
    "        LayerNorm(n_embd),\n",
    "        FeedForward(n_embd; dropout)\n",
    "    )\n",
    "end\n",
    "\n",
    "function (block::TransformerBlock)(x)\n",
    "    x = x .+ block.attn(block.ln1(x))\n",
    "    x = x .+ block.ffwd(block.ln2(x))\n",
    "    x\n",
    "end\n",
    "\n",
    "struct GPT\n",
    "    wte::Embedding\n",
    "    wpe::Embedding\n",
    "    drop::Dropout\n",
    "    blocks::Chain\n",
    "    ln_f::LayerNorm\n",
    "    lm_head::Dense\n",
    "end\n",
    "\n",
    "Flux.@layer GPT\n",
    "\n",
    "function GPT(; vocab_size, n_embd, block_size, n_layer, n_head, dropout=0.1)\n",
    "    GPT(\n",
    "        Embedding(vocab_size => n_embd),\n",
    "        Embedding(block_size => n_embd),\n",
    "        Dropout(dropout),\n",
    "        Chain([TransformerBlock(n_embd, n_head; dropout) for _ in 1:n_layer]...),\n",
    "        LayerNorm(n_embd),\n",
    "        Dense(n_embd => vocab_size; bias=false)\n",
    "    )\n",
    "end\n",
    "\n",
    "function (m::GPT)(idx)\n",
    "    B, T = size(idx)\n",
    "    tok = permutedims(m.wte(idx), (1, 3, 2))\n",
    "    positions = repeat(collect(1:T)', B, 1)\n",
    "    pos = permutedims(m.wpe(positions), (1, 3, 2))\n",
    "    x = m.drop(tok .+ pos)\n",
    "    x = m.blocks(x)\n",
    "    x = m.ln_f(x)\n",
    "    m.lm_head(x)\n",
    "end\n",
    "\n",
    "println(\"All model structs defined: CausalSelfAttention, FeedForward, TransformerBlock, GPT\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "model = GPT(;\n",
    "    vocab_size = vocab_size,\n",
    "    n_embd     = n_embd,\n",
    "    block_size = block_size,\n",
    "    n_layer    = n_layer,\n",
    "    n_head     = n_head,\n",
    "    dropout    = dropout\n",
    ") |> device\n",
    "\n",
    "n_params = sum(length, Flux.params(model))\n",
    "println(\"Model created on $device\")\n",
    "println(\"Parameters: $(n_params) ($(round(n_params/1e6, digits=2))M)\")\n",
    "\n",
    "# Quick smoke test\n",
    "x_test, y_test = get_batch(\"train\")\n",
    "logits_test = model(x_test)\n",
    "println(\"Forward pass OK -- logits: $(size(logits_test))\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---",
    "## 6. Checkpoint Save/Load",
    "",
    "Save and load model weights + optimizer state as JLD2 files.",
    "Checkpoints saved to Google Drive persist across Colab sessions."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "LOCAL_CKPT = \"checkpoints\"\n",
    "DRIVE_CKPT = \"/content/drive/MyDrive/JuliaFluxGPT/checkpoints\"\n",
    "mkpath(LOCAL_CKPT)\n",
    "\n",
    "function save_to_drive(local_path::String)\n",
    "    if isdir(DRIVE_CKPT)\n",
    "        try\n",
    "            cp(local_path, joinpath(DRIVE_CKPT, basename(local_path)), force=true)\n",
    "            println(\"  -> synced to Drive\")\n",
    "        catch e\n",
    "            println(\"  Drive sync failed: $e\")\n",
    "        end\n",
    "    end\n",
    "end\n",
    "\n",
    "function save_checkpoint(path::String, model, opt_state;\n",
    "                          step::Int=0, best_val_loss::Float64=Inf,\n",
    "                          train_losses::Vector{Float64}=Float64[],\n",
    "                          val_losses::Vector{Float64}=Float64[])\n",
    "    mkpath(dirname(path))\n",
    "    model_cpu = cpu(model)\n",
    "    opt_cpu = cpu(opt_state)\n",
    "    JLD2.jldsave(path;\n",
    "        model_state = Flux.state(model_cpu),\n",
    "        opt_state = opt_cpu,\n",
    "        step = step,\n",
    "        best_val_loss = best_val_loss,\n",
    "        train_losses = train_losses,\n",
    "        val_losses = val_losses,\n",
    "        hyperparams = Dict(\n",
    "            \"vocab_size\" => vocab_size,\n",
    "            \"n_embd\" => n_embd,\n",
    "            \"block_size\" => block_size,\n",
    "            \"n_layer\" => n_layer,\n",
    "            \"n_head\" => n_head,\n",
    "            \"dropout\" => dropout\n",
    "        )\n",
    "    )\n",
    "    vl_str = best_val_loss == Inf ? \"Inf\" : @sprintf(\"%.4f\", best_val_loss)\n",
    "    println(\"Checkpoint saved: $path (step $step, best_val_loss=$vl_str)\")\n",
    "end\n",
    "\n",
    "function save_and_sync(path, model, opt_state; kwargs...)\n",
    "    save_checkpoint(path, model, opt_state; kwargs...)\n",
    "    save_to_drive(path)\n",
    "end\n",
    "\n",
    "function load_checkpoint(path::String, device_fn)\n",
    "    println(\"Loading checkpoint from $path ...\")\n",
    "    data = JLD2.load(path)\n",
    "\n",
    "    hp = data[\"hyperparams\"]\n",
    "    m = GPT(;\n",
    "        vocab_size = hp[\"vocab_size\"],\n",
    "        n_embd     = hp[\"n_embd\"],\n",
    "        block_size = hp[\"block_size\"],\n",
    "        n_layer    = hp[\"n_layer\"],\n",
    "        n_head     = hp[\"n_head\"],\n",
    "        dropout    = get(hp, \"dropout\", 0.1)\n",
    "    )\n",
    "    Flux.loadmodel!(m, data[\"model_state\"])\n",
    "    m = m |> device_fn\n",
    "\n",
    "    opt = data[\"opt_state\"]\n",
    "\n",
    "    println(\"  step=$(data[\"step\"]), best_val=$(round(data[\"best_val_loss\"], digits=4))\")\n",
    "    return (;\n",
    "        model = m,\n",
    "        opt_state = opt |> device_fn,\n",
    "        step = data[\"step\"],\n",
    "        best_val_loss = data[\"best_val_loss\"],\n",
    "        train_losses = get(data, \"train_losses\", Float64[]),\n",
    "        val_losses = get(data, \"val_losses\", Float64[])\n",
    "    )\n",
    "end\n",
    "\n",
    "println(\"Checkpoint save/load defined (JLD2 format)\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---",
    "## 7. Training Loop with Validation + Best-Model Checkpointing",
    "",
    "Adam optimizer with cosine LR decay + warmup.",
    "Validates every 500 steps, saves `best_model.jld2` when val loss improves.",
    "Defensive saves: try/catch with emergency checkpoint, time-based auto-save."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "using Printf\n",
    "\n",
    "function estimate_loss(model, n_iters=eval_iters)\n",
    "    model_eval = Flux.testmode!(deepcopy(model))\n",
    "    losses = Dict(\"train\" => 0.0, \"val\" => 0.0)\n",
    "    for split in [\"train\", \"val\"]\n",
    "        total = 0.0\n",
    "        for _ in 1:n_iters\n",
    "            x, y = get_batch(split)\n",
    "            logits = model_eval(x)\n",
    "            loss = Flux.logitcrossentropy(\n",
    "                reshape(logits, vocab_size, :),\n",
    "                Flux.onehotbatch(reshape(y, :), 1:vocab_size)\n",
    "            )\n",
    "            total += loss\n",
    "        end\n",
    "        losses[split] = total / n_iters\n",
    "    end\n",
    "    return losses\n",
    "end\n",
    "\n",
    "# LR schedule: warmup + cosine decay\n",
    "function get_lr(iter)\n",
    "    if iter < warmup_iters\n",
    "        return learning_rate * iter / warmup_iters\n",
    "    end\n",
    "    decay_ratio = (iter - warmup_iters) / (max_iters - warmup_iters)\n",
    "    coeff = 0.5 * (1.0 + cos(Float64(pi) * decay_ratio))\n",
    "    return min_lr + coeff * (learning_rate - min_lr)\n",
    "end\n",
    "\n",
    "opt_state = Flux.setup(Adam(learning_rate), model)\n",
    "\n",
    "best_val = Inf\n",
    "train_loss_history = Float64[]\n",
    "val_loss_history = Float64[]\n",
    "\n",
    "# ── Initialize W&B logging (if API key is set) ──\n",
    "if haskey(ENV, \"WANDB_API_KEY\") && !isempty(ENV[\"WANDB_API_KEY\"])\n",
    "    wandb_init()\n",
    "end\n",
    "\n",
    "SAVE_INTERVAL = 600  # auto-save every 10 min\n",
    "last_save_time = time()\n",
    "completed_iter = 0\n",
    "\n",
    "println(\"Training for $max_iters steps...\")\n",
    "println(\"    Local: $LOCAL_CKPT/  |  Drive: $(isdir(DRIVE_CKPT) ? DRIVE_CKPT : \\\"(not mounted)\\\")\")\n",
    "t_start = time()\n",
    "\n",
    "try\n",
    "    for iter in 1:max_iters\n",
    "        global completed_iter = iter\n",
    "\n",
    "        # Update LR\n",
    "        lr_t = get_lr(iter)\n",
    "        Flux.adjust!(opt_state, lr_t)\n",
    "\n",
    "        # Forward + backward + update\n",
    "        x, y = get_batch(\"train\")\n",
    "        loss, grads = Flux.withgradient(model) do m\n",
    "            logits = m(x)\n",
    "            Flux.logitcrossentropy(\n",
    "                reshape(logits, vocab_size, :),\n",
    "                Flux.onehotbatch(reshape(y, :), 1:vocab_size)\n",
    "            )\n",
    "        end\n",
    "        Flux.update!(opt_state, model, grads[1])\n",
    "        push!(train_loss_history, Float64(loss))\n",
    "\n",
    "        # Eval + print + checkpoint\n",
    "        if iter % eval_interval == 0 || iter == 1\n",
    "            losses = estimate_loss(model)\n",
    "            push!(val_loss_history, losses[\"val\"])\n",
    "            elapsed = round(time() - t_start, digits=1)\n",
    "            wandb_log(; step=iter, train_loss=Float64(loss), val_loss=losses[\"val\"], lr=lr_t)\n",
    "\n",
    "            improved = \"\"\n",
    "            if losses[\"val\"] < best_val\n",
    "                best_val = losses[\"val\"]\n",
    "                save_and_sync(\"checkpoints/best_model.jld2\", model, opt_state;\n",
    "                    step=iter, best_val_loss=best_val,\n",
    "                    train_losses=train_loss_history, val_losses=val_loss_history)\n",
    "                improved = \" << new best!\"\n",
    "            end\n",
    "\n",
    "            @printf(\"step %5d | train %.4f | val %.4f | lr %.2e | %.1fs%s\\n\",\n",
    "                    iter, losses[\"train\"], losses[\"val\"], lr_t, elapsed, improved)\n",
    "        end\n",
    "\n",
    "        # Periodic checkpoint every 1000 steps -> Drive\n",
    "        if iter % 1000 == 0\n",
    "            save_and_sync(\"checkpoints/checkpoint_latest.jld2\", model, opt_state;\n",
    "                step=iter, best_val_loss=best_val,\n",
    "                train_losses=train_loss_history, val_losses=val_loss_history)\n",
    "            last_save_time = time()\n",
    "        end\n",
    "\n",
    "        # Time-based auto-save every 10 min\n",
    "        if time() - last_save_time > SAVE_INTERVAL\n",
    "            save_and_sync(\"checkpoints/checkpoint_latest.jld2\", model, opt_state;\n",
    "                step=iter, best_val_loss=best_val,\n",
    "                train_losses=train_loss_history, val_losses=val_loss_history)\n",
    "            last_save_time = time()\n",
    "            println(\"  [auto-save at step $iter]\")\n",
    "        end\n",
    "    end\n",
    "catch e\n",
    "    if e isa InterruptException\n",
    "        println(\"\\n\\nTraining interrupted at step $completed_iter!\")\n",
    "    else\n",
    "        println(\"\\n\\nTraining error at step $completed_iter: $e\")\n",
    "    end\n",
    "    println(\"Saving emergency checkpoint...\")\n",
    "    save_and_sync(\"checkpoints/checkpoint_interrupted.jld2\", model, opt_state;\n",
    "        step=completed_iter, best_val_loss=best_val,\n",
    "        train_losses=train_loss_history, val_losses=val_loss_history)\n",
    "    if !(e isa InterruptException)\n",
    "        rethrow(e)\n",
    "    end\n",
    "end\n",
    "\n",
    "elapsed = round(time() - t_start, digits=1)\n",
    "println(\"\\nTraining complete in $(elapsed)s. Best val loss: $(round(best_val, digits=4))\")\n",
    "wandb_finish()\n",
    "\n",
    "# Final save -> local + Drive\n",
    "save_and_sync(\"checkpoints/final_model.jld2\", model, opt_state;\n",
    "    step=max_iters, best_val_loss=best_val,\n",
    "    train_losses=train_loss_history, val_losses=val_loss_history)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---",
    "## 8. Inference — Generate Text",
    "",
    "Generate new philosophy-like text using temperature-controlled sampling."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "function generate_text(model, max_tokens=200; temperature=0.8)\n",
    "    model_eval = Flux.testmode!(deepcopy(model))\n",
    "    # Start with a random token\n",
    "    idx = reshape([rand(1:vocab_size)], 1, 1) |> device\n",
    "    generated = Int[]\n",
    "\n",
    "    for _ in 1:max_tokens\n",
    "        # Crop to block_size\n",
    "        idx_cond = idx[:, max(1, end-block_size+1):end]\n",
    "        logits = model_eval(idx_cond)     # (vocab, T, B)\n",
    "        logits_last = logits[:, end, 1]   # (vocab,) last token logits\n",
    "\n",
    "        # Temperature scaling + softmax\n",
    "        probs = softmax(logits_last ./ Float32(temperature))\n",
    "        probs_cpu = Float64.(cpu(probs))\n",
    "\n",
    "        # Categorical sampling\n",
    "        r = rand()\n",
    "        cum = 0.0\n",
    "        next_id = 1\n",
    "        for (i, p) in enumerate(probs_cpu)\n",
    "            cum += p\n",
    "            if r <= cum\n",
    "                next_id = i\n",
    "                break\n",
    "            end\n",
    "        end\n",
    "\n",
    "        push!(generated, next_id)\n",
    "        next_token = reshape([next_id], 1, 1) |> device\n",
    "        idx = hcat(idx, next_token)\n",
    "    end\n",
    "\n",
    "    return decode(generated)\n",
    "end\n",
    "\n",
    "println(\"--- inference (hallucinated philosophy) ---\")\n",
    "for i in 1:5\n",
    "    text = generate_text(model, 300; temperature=0.8)\n",
    "    @printf(\"\\nsample %d:\\n%s\\n\", i, text[1:min(end, 500)])\n",
    "    println(\"---\")\n",
    "end\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---",
    "## 8a. Push Model to HuggingFace Hub",
    "",
    "Push your trained checkpoint to HuggingFace for persistence across Colab sessions.",
    "Set `HF_REPO_ID` in the login cell above."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "if @isdefined(HF_REPO_ID) && !isempty(HF_REPO_ID)\n",
    "    hf_create_repo(HF_REPO_ID)\n",
    "\n",
    "    if isfile(\"checkpoints/best_model.jld2\")\n",
    "        hf_push_checkpoint(HF_REPO_ID; checkpoint_path=\"checkpoints/best_model.jld2\")\n",
    "    else\n",
    "        println(\"No best_model.jld2 found -- train first!\")\n",
    "    end\n",
    "\n",
    "    if isfile(\"checkpoints/final_model.jld2\")\n",
    "        hf_push(HF_REPO_ID, \"checkpoints/final_model.jld2\")\n",
    "    end\n",
    "\n",
    "    println(\"\\nDone! View your model at: https://huggingface.co/$HF_REPO_ID\")\n",
    "else\n",
    "    println(\"Set HF_REPO_ID in the login cell (e.g. \\\"yourusername/juliaflux-philosophy\\\")\")\n",
    "end\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---",
    "## 8b. Pull Checkpoint from HuggingFace Hub",
    "",
    "Download a previously pushed checkpoint to resume training in a new Colab session."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "if @isdefined(HF_REPO_ID) && !isempty(HF_REPO_ID)\n",
    "    mkpath(\"checkpoints\")\n",
    "    hf_pull(HF_REPO_ID, \"best_model.jld2\"; local_dir=\"checkpoints\")\n",
    "    println(\"\\nReady to resume from checkpoints/best_model.jld2\")\n",
    "    println(\"Run the 'Resume Training' cell below.\")\n",
    "else\n",
    "    println(\"Set HF_REPO_ID in the login cell (e.g. \\\"yourusername/juliaflux-philosophy\\\")\")\n",
    "end\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---",
    "## 9. Resume Training from Checkpoint",
    "",
    "Load a saved checkpoint and continue training for more steps.",
    "Skip this cell if you're training from scratch above."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "RESUME_FROM = \"checkpoints/best_model.jld2\"\n",
    "EXTRA_ITERS = 2000\n",
    "\n",
    "if !isfile(RESUME_FROM)\n",
    "    # Try Drive\n",
    "    drive_path = joinpath(DRIVE_CKPT, basename(RESUME_FROM))\n",
    "    if isfile(drive_path)\n",
    "        mkpath(dirname(RESUME_FROM))\n",
    "        cp(drive_path, RESUME_FROM)\n",
    "        println(\"Copied from Drive: $drive_path\")\n",
    "    else\n",
    "        error(\"Checkpoint not found: $RESUME_FROM\")\n",
    "    end\n",
    "end\n",
    "\n",
    "ckpt = load_checkpoint(RESUME_FROM, device)\n",
    "model = ckpt.model\n",
    "opt_state = ckpt.opt_state\n",
    "start_iter = ckpt.step + 1\n",
    "best_val = ckpt.best_val_loss\n",
    "train_loss_history = copy(ckpt.train_losses)\n",
    "val_loss_history = copy(ckpt.val_losses)\n",
    "end_iter = ckpt.step + EXTRA_ITERS\n",
    "\n",
    "# ── Initialize W&B logging (if API key is set) ──\n",
    "if haskey(ENV, \"WANDB_API_KEY\") && !isempty(ENV[\"WANDB_API_KEY\"])\n",
    "    wandb_init()\n",
    "end\n",
    "\n",
    "println(\"\\nResuming from step $(ckpt.step) -> training to step $end_iter\")\n",
    "println(\"Best val loss so far: $(round(best_val, digits=4))\")\n",
    "t_start = time()\n",
    "last_save_time = time()\n",
    "\n",
    "try\n",
    "    for iter in start_iter:end_iter\n",
    "        global completed_iter = iter\n",
    "\n",
    "        lr_t = get_lr(min(iter, max_iters))\n",
    "        Flux.adjust!(opt_state, lr_t)\n",
    "\n",
    "        x, y = get_batch(\"train\")\n",
    "        loss, grads = Flux.withgradient(model) do m\n",
    "            logits = m(x)\n",
    "            Flux.logitcrossentropy(\n",
    "                reshape(logits, vocab_size, :),\n",
    "                Flux.onehotbatch(reshape(y, :), 1:vocab_size)\n",
    "            )\n",
    "        end\n",
    "        Flux.update!(opt_state, model, grads[1])\n",
    "        push!(train_loss_history, Float64(loss))\n",
    "\n",
    "        if iter % eval_interval == 0\n",
    "            losses = estimate_loss(model)\n",
    "            push!(val_loss_history, losses[\"val\"])\n",
    "            elapsed = round(time() - t_start, digits=1)\n",
    "            wandb_log(; step=iter, train_loss=Float64(loss), val_loss=losses[\"val\"], lr=lr_t)\n",
    "\n",
    "            improved = \"\"\n",
    "            if losses[\"val\"] < best_val\n",
    "                best_val = losses[\"val\"]\n",
    "                save_and_sync(\"checkpoints/best_model.jld2\", model, opt_state;\n",
    "                    step=iter, best_val_loss=best_val,\n",
    "                    train_losses=train_loss_history, val_losses=val_loss_history)\n",
    "                improved = \" << new best!\"\n",
    "            end\n",
    "\n",
    "            @printf(\"step %5d / %5d | train %.4f | val %.4f | lr %.2e | %.1fs%s\\n\",\n",
    "                    iter, end_iter, losses[\"train\"], losses[\"val\"], lr_t, elapsed, improved)\n",
    "        end\n",
    "\n",
    "        if iter % 1000 == 0\n",
    "            save_and_sync(\"checkpoints/checkpoint_latest.jld2\", model, opt_state;\n",
    "                step=iter, best_val_loss=best_val,\n",
    "                train_losses=train_loss_history, val_losses=val_loss_history)\n",
    "            last_save_time = time()\n",
    "        end\n",
    "\n",
    "        if time() - last_save_time > SAVE_INTERVAL\n",
    "            save_and_sync(\"checkpoints/checkpoint_latest.jld2\", model, opt_state;\n",
    "                step=iter, best_val_loss=best_val,\n",
    "                train_losses=train_loss_history, val_losses=val_loss_history)\n",
    "            last_save_time = time()\n",
    "            println(\"  [auto-save at step $iter]\")\n",
    "        end\n",
    "    end\n",
    "catch e\n",
    "    if e isa InterruptException\n",
    "        println(\"\\n\\nTraining interrupted at step $completed_iter!\")\n",
    "    else\n",
    "        println(\"\\n\\nTraining error at step $completed_iter: $e\")\n",
    "    end\n",
    "    println(\"Saving emergency checkpoint...\")\n",
    "    save_and_sync(\"checkpoints/checkpoint_interrupted.jld2\", model, opt_state;\n",
    "        step=completed_iter, best_val_loss=best_val,\n",
    "        train_losses=train_loss_history, val_losses=val_loss_history)\n",
    "    if !(e isa InterruptException)\n",
    "        rethrow(e)\n",
    "    end\n",
    "end\n",
    "\n",
    "elapsed = round(time() - t_start, digits=1)\n",
    "@printf(\"\\nResume training complete in %.1fs\\n\", elapsed)\n",
    "wandb_finish()\n",
    "\n",
    "save_and_sync(\"checkpoints/final_model.jld2\", model, opt_state;\n",
    "    step=end_iter, best_val_loss=best_val,\n",
    "    train_losses=train_loss_history, val_losses=val_loss_history)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---",
    "## 10. Download Checkpoint",
    "",
    "Download the best model checkpoint to use elsewhere.",
    "In Colab, use the Files panel (left sidebar) to download, or copy to Google Drive."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "if isdir(\"checkpoints\")\n",
    "    files = readdir(\"checkpoints\")\n",
    "    println(\"Saved checkpoints:\")\n",
    "    for f in files\n",
    "        path = joinpath(\"checkpoints\", f)\n",
    "        size_kb = round(filesize(path) / 1024, digits=1)\n",
    "        println(\"  $path ($(size_kb) KB)\")\n",
    "    end\n",
    "else\n",
    "    println(\"No checkpoints directory found. Train first!\")\n",
    "end\n"
   ],
   "outputs": [],
   "execution_count": null
  }
 ]
}