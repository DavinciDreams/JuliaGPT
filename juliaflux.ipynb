{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "machine_shape": "hm",
   "gpuType": "A100"
  },
  "kernelspec": {
   "name": "julia",
   "display_name": "Julia"
  },
  "language_info": {
   "name": "julia"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3QreApISq9BU",
    "outputId": "fe25aa12-27c0-4034-9945-683fc073a53e"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.11/Project.toml`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.11/Manifest.toml`\n"
     ]
    }
   ],
   "source": [
    "# Cell 1 – Install packages (run once, restart after if needed)\n",
    "using Pkg\n",
    "\n",
    "Pkg.add([\n",
    "    \"Flux\", \"Zygote\", \"Optimisers\",\n",
    "    \"CUDA\", \"cuDNN\",\n",
    "    \"Downloads\", \"Statistics\", \"Random\",\n",
    "    \"Printf\", \"LinearAlgebra\", \"JLD2\",\n",
    "    \"NNlib\"\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Cell 2 – Imports\n",
    "using Flux\n",
    "using Zygote\n",
    "using Optimisers\n",
    "using CUDA\n",
    "using NNlib\n",
    "using Downloads\n",
    "using Statistics\n",
    "using Random\n",
    "using Printf\n",
    "using LinearAlgebra\n",
    "using JLD2\n",
    "\n",
    "Random.seed!(1337)\n",
    "\n",
    "device = CUDA.functional() ? gpu : cpu\n",
    "println(\"Device: \", device)\n",
    "println(\"CUDA functional: \", CUDA.functional())"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BuVnG9lkrOrB",
    "outputId": "1d3f82ec-535f-4531-bf9b-0de93b4c8ba8"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Device: gpu\n",
      "CUDA functional: true\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Cell 3 – Hyperparameters\n",
    "block_size     = 128\n",
    "n_embd         = 256\n",
    "n_head         = 4\n",
    "n_layer        = 4\n",
    "dropout        = 0.1\n",
    "bias           = false\n",
    "\n",
    "batch_size     = 32\n",
    "learning_rate  = 4e-4\n",
    "max_iters      = 5000\n",
    "eval_interval  = 500\n",
    "eval_iters     = 100\n",
    "warmup_iters   = 200\n",
    "min_lr         = 1e-5\n",
    "\n",
    "println(\"n_embd = $n_embd\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5ccbm1gormJc",
    "outputId": "d0aa01f3-fa02-411b-e8a1-2dc96c422fa5"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "n_embd = 256\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Cell 4 – Download texts\n",
    "function download_and_clean(url::String, fn::String; is_gutenberg=true)\n",
    "    if !isfile(fn)\n",
    "        println(\"Downloading $fn from $url\")\n",
    "        try\n",
    "            Downloads.download(url, fn)\n",
    "        catch e\n",
    "            @warn \"Download failed: $url → $e\"\n",
    "            return \"\"\n",
    "        end\n",
    "    end\n",
    "\n",
    "    txt = read(fn, String)\n",
    "\n",
    "    if is_gutenberg\n",
    "        txt = replace(txt, r\"(?is)^.*?\\*{3}\\s*START OF (THE|THIS) PROJECT GUTENBERG.*?\\*{3}[\\r\\n]*\" => \"\")\n",
    "        txt = replace(txt, r\"(?is)\\*{3}\\s*END OF (THE|THIS) PROJECT GUTENBERG.*$\" => \"\")\n",
    "    end\n",
    "\n",
    "    txt = replace(txt, r\"\\r\\n\" => \"\\n\")\n",
    "    txt = replace(txt, r\"\\n{3,}\" => \"\\n\\n\")\n",
    "    txt = strip(txt)\n",
    "\n",
    "    return txt\n",
    "end\n",
    "\n",
    "sources = Dict(\n",
    "    \"grammar\"             => (\"https://www.gutenberg.org/files/15665/15665-0.txt\",        \"latin_grammar.txt\",      true),\n",
    "    \"categories\"          => (\"https://www.gutenberg.org/ebooks/2412.txt.utf-8\",          \"aristotle_categories.txt\", true),\n",
    "    \"rhetoric\"            => (\"http://classics.mit.edu/Aristotle/rhetoric.mb.txt\",        \"aristotle_rhetoric.txt\", false),\n",
    "    \"prior_analytics\"     => (\"http://classics.mit.edu/Aristotle/prior.mb.txt\",           \"prior_analytics.txt\",    false),\n",
    "    \"posterior_analytics\" => (\"http://classics.mit.edu/Aristotle/posterior.mb.txt\",       \"posterior_analytics.txt\", false),\n",
    "    \"topics\"              => (\"http://classics.mit.edu/Aristotle/topics.mb.txt\",          \"topics.txt\",             false),\n",
    "    \"boethius\"            => (\"https://www.gutenberg.org/files/14328/14328-0.txt\",        \"boethius_consolation.txt\", true),\n",
    "    \"heavens\"             => (\"http://classics.mit.edu/Aristotle/heavens.mb.txt\",         \"aristotle_heavens.txt\",  false),\n",
    "    \"republic\"            => (\"https://www.gutenberg.org/files/1497/1497-0.txt\",          \"plato_republic.txt\",     true),\n",
    "    \"apology\"             => (\"https://www.gutenberg.org/files/1656/1656-0.txt\",          \"plato_apology.txt\",      true),\n",
    "    \"ethics\"              => (\"https://www.gutenberg.org/files/8438/8438-0.txt\",          \"aristotle_ethics.txt\",   true),\n",
    "    \"emerson\"             => (\"https://www.gutenberg.org/files/2944/2944-0.txt\",          \"emerson_essays.txt\",     true),\n",
    "    \"walden\"              => (\"https://www.gutenberg.org/files/205/205-0.txt\",            \"thoreau_walden.txt\",     true),\n",
    "    \"epicurus\"            => (\"https://www.gutenberg.org/files/57342/57342-0.txt\",        \"diogenes_epicurus.txt\",  true)\n",
    ")\n",
    "\n",
    "texts = Dict{String,String}()\n",
    "for (key, (url, fn, is_gut)) in sources\n",
    "    texts[key] = download_and_clean(url, fn; is_gutenberg=is_gut)\n",
    "end\n",
    "\n",
    "println(\"Downloaded $(length(texts)) texts.\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "03ki0CtarpuD",
    "outputId": "77bd779c-2567-4dc7-adb0-95325779a48f"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Downloaded 14 texts.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Cell 5 – Unicode normalization\n",
    "for fn in [\n",
    "    \"latin_grammar.txt\",\n",
    "    \"aristotle_categories.txt\",\n",
    "    \"aristotle_rhetoric.txt\",\n",
    "    \"prior_analytics.txt\",\n",
    "    \"posterior_analytics.txt\",\n",
    "    \"topics.txt\",\n",
    "    \"boethius_consolation.txt\",\n",
    "    \"aristotle_heavens.txt\",\n",
    "    \"plato_republic.txt\",\n",
    "    \"plato_apology.txt\",\n",
    "    \"aristotle_ethics.txt\",\n",
    "    \"emerson_essays.txt\",\n",
    "    \"thoreau_walden.txt\",\n",
    "    \"diogenes_epicurus.txt\"\n",
    "]\n",
    "    isfile(fn) || continue\n",
    "\n",
    "    txt = read(fn, String)\n",
    "    txt = replace(txt,\n",
    "        \"“\" => \"\\\"\", \"”\" => \"\\\"\",\n",
    "        \"‘\" => \"'\", \"’\" => \"'\",\n",
    "        \"—\" => \"--\", \"–\" => \"-\",\n",
    "        \"…\" => \"...\",\n",
    "        \"\\u00A0\" => \" \"\n",
    "    )\n",
    "\n",
    "    txt = replace(txt, r\"\\n{3,}\" => \"\\n\\n\")\n",
    "    txt = strip(txt)\n",
    "\n",
    "    open(fn, \"w\") do io\n",
    "        write(io, txt)\n",
    "    end\n",
    "\n",
    "    println(\"Normalized: $fn\")\n",
    "end"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2IYY0k16rt_m",
    "outputId": "042a7944-ead2-4510-e8b3-68c4545bffb7"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Normalized: latin_grammar.txt\n",
      "Normalized: aristotle_categories.txt\n",
      "Normalized: aristotle_rhetoric.txt\n",
      "Normalized: prior_analytics.txt\n",
      "Normalized: posterior_analytics.txt\n",
      "Normalized: topics.txt\n",
      "Normalized: boethius_consolation.txt\n",
      "Normalized: aristotle_heavens.txt\n",
      "Normalized: plato_republic.txt\n",
      "Normalized: plato_apology.txt\n",
      "Normalized: aristotle_ethics.txt\n",
      "Normalized: emerson_essays.txt\n",
      "Normalized: thoreau_walden.txt\n",
      "Normalized: diogenes_epicurus.txt\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Cell 6 – Build full_text, vocab, data\n",
    "full_text = \"\"\n",
    "for fn in [\n",
    "    \"latin_grammar.txt\",\n",
    "    \"aristotle_categories.txt\",\n",
    "    \"aristotle_rhetoric.txt\",\n",
    "    \"prior_analytics.txt\",\n",
    "    \"posterior_analytics.txt\",\n",
    "    \"topics.txt\",\n",
    "    \"boethius_consolation.txt\",\n",
    "    \"aristotle_heavens.txt\",\n",
    "    \"plato_republic.txt\",\n",
    "    \"plato_apology.txt\",\n",
    "    \"aristotle_ethics.txt\",\n",
    "    \"emerson_essays.txt\",\n",
    "    \"thoreau_walden.txt\",\n",
    "    \"diogenes_epicurus.txt\"\n",
    "]\n",
    "    isfile(fn) || continue\n",
    "    content = read(fn, String)\n",
    "    full_text *= \"\\n\\n=== $fn ===\\n\\n\" * content\n",
    "end\n",
    "\n",
    "full_text = strip(full_text)\n",
    "\n",
    "chars = sort(unique(full_text))\n",
    "global vocab_size = length(chars)\n",
    "\n",
    "stoi = Dict(c => i for (i,c) in enumerate(chars))\n",
    "itos = Dict(i => c for (i,c) in enumerate(chars))\n",
    "\n",
    "encode(s) = [stoi[c] for c in s]\n",
    "decode(ids) = join(itos[i] for i in ids)\n",
    "\n",
    "data = encode(full_text)\n",
    "n = length(data)\n",
    "train_split = floor(Int, n * 0.9)\n",
    "global train_data = data[1:train_split]\n",
    "global val_data   = data[train_split+1:end]\n",
    "\n",
    "println(\"Vocab size: \", vocab_size)\n",
    "println(\"Train tokens: \", length(train_data))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bmSUV6iRrz9k",
    "outputId": "d96931d5-dcad-4862-b5ad-3598b39dfde0"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Vocab size: 254\n",
      "Train tokens: 6026460\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Cell 7 – Batch loader\n",
    "function get_batch(split=\"train\")\n",
    "    d = split == \"train\" ? train_data : val_data\n",
    "    ix = rand(1:length(d)-block_size, batch_size)\n",
    "    x = hcat([d[i:i+block_size-1] for i in ix]...)\n",
    "    y = hcat([d[i+1:i+block_size] for i in ix]...)\n",
    "    x = permutedims(x)   # now (B, T)\n",
    "    y = permutedims(y)\n",
    "    x = x |> device\n",
    "    y = y |> device\n",
    "    return x, y\n",
    "end"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E_NcAIisr-fO",
    "outputId": "16d04499-b382-47fa-a465-8ccbd6a6530e"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "get_batch (generic function with 2 methods)"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ]
  },
  {
   "cell_type": "code",
   "source": "# ── All model structs in ONE cell (Julia structs can't be redefined) ──\nusing NNlib: batched_mul\n\nstruct CausalSelfAttention\n    qkv::Dense       # single projection: n_embd → 3*n_embd\n    proj::Dense       # output projection: n_embd → n_embd\n    n_head::Int\nend\n\nFlux.@layer CausalSelfAttention trainable=(qkv, proj)\n\nfunction CausalSelfAttention(n_embd::Int, n_head::Int; bias=false)\n    CausalSelfAttention(\n        Dense(n_embd => 3 * n_embd; bias),\n        Dense(n_embd => n_embd; bias),\n        n_head\n    )\nend\n\nfunction (attn::CausalSelfAttention)(x)\n    # x: (C, T, B)  — Flux convention: features first\n    C, T, B = size(x)\n    hs = C ÷ attn.n_head\n    nh = attn.n_head\n\n    # Single QKV projection → split\n    qkv = attn.qkv(x)                          # (3C, T, B)\n    q = qkv[1:C, :, :]\n    k = qkv[C+1:2C, :, :]\n    v = qkv[2C+1:3C, :, :]\n\n    # Reshape to (hs, T, nh*B) for batched_mul\n    q = reshape(permutedims(reshape(q, hs, nh, T, B), (1, 3, 2, 4)), hs, T, nh * B)\n    k = reshape(permutedims(reshape(k, hs, nh, T, B), (1, 3, 2, 4)), hs, T, nh * B)\n    v = reshape(permutedims(reshape(v, hs, nh, T, B), (1, 3, 2, 4)), hs, T, nh * B)\n\n    # Attention scores: (T, T, nh*B)\n    scale = Float32(1 / sqrt(hs))\n    wei = batched_mul(permutedims(q, (2, 1, 3)), k) .* scale\n\n    # Causal mask: upper triangle = -Inf\n    mask = triu(fill(typemin(Float32), T, T), 1)\n    wei = wei .+ mask\n\n    wei = softmax(wei; dims=2)\n\n    # Apply attention: (hs, T, nh*B)\n    out = batched_mul(v, permutedims(wei, (2, 1, 3)))\n\n    # Reshape back to (C, T, B)\n    out = reshape(permutedims(reshape(out, hs, T, nh, B), (1, 3, 2, 4)), C, T, B)\n\n    attn.proj(out)\nend\n\nstruct FeedForward\n    net::Chain\nend\n\nFlux.@layer FeedForward\n\nfunction FeedForward(n_embd::Int; bias=false, dropout=0.0)\n    FeedForward(Chain(\n        Dense(n_embd => 4 * n_embd; bias),\n        gelu,\n        Dense(4 * n_embd => n_embd; bias),\n        Dropout(dropout)\n    ))\nend\n\n(ff::FeedForward)(x) = ff.net(x)\n\nstruct TransformerBlock\n    ln1::LayerNorm\n    attn::CausalSelfAttention\n    ln2::LayerNorm\n    ffwd::FeedForward\nend\n\nFlux.@layer TransformerBlock\n\nfunction TransformerBlock(n_embd::Int, n_head::Int; dropout=0.0)\n    TransformerBlock(\n        LayerNorm(n_embd),\n        CausalSelfAttention(n_embd, n_head),\n        LayerNorm(n_embd),\n        FeedForward(n_embd; dropout)\n    )\nend\n\nfunction (block::TransformerBlock)(x)\n    x = x .+ block.attn(block.ln1(x))\n    x = x .+ block.ffwd(block.ln2(x))\n    x\nend\n\nstruct GPT\n    wte::Embedding\n    wpe::Embedding\n    drop::Dropout\n    blocks::Chain\n    ln_f::LayerNorm\n    lm_head::Dense\nend\n\nFlux.@layer GPT\n\nfunction GPT(; vocab_size, n_embd, block_size, n_layer, n_head, dropout=0.1)\n    GPT(\n        Embedding(vocab_size => n_embd),\n        Embedding(block_size => n_embd),\n        Dropout(dropout),\n        Chain([TransformerBlock(n_embd, n_head; dropout) for _ in 1:n_layer]...),\n        LayerNorm(n_embd),\n        Dense(n_embd => vocab_size; bias=false)\n    )\nend\n\nfunction (m::GPT)(idx)\n    # idx: (B, T) integer token IDs\n    # Flux Embedding: input (B,T) → output (n_embd, B, T)\n    B, T = size(idx)\n\n    tok = permutedims(m.wte(idx), (1, 3, 2))               # (C, B, T) → (C, T, B)\n\n    positions = repeat(collect(1:T)', B, 1)                 # (B, T)\n    pos = permutedims(m.wpe(positions), (1, 3, 2))          # (C, T, B)\n\n    x = m.drop(tok .+ pos)\n    x = m.blocks(x)\n    x = m.ln_f(x)\n    m.lm_head(x)                                            # (vocab, T, B)\nend\n\nprintln(\"All model structs defined: CausalSelfAttention, FeedForward, TransformerBlock, GPT\")",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 124
    },
    "id": "TCW7URrU15D1",
    "outputId": "3bb620a2-6808-49d0-8742-d2f73abbad7d"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# ── Create model + move to GPU ──\nmodel = GPT(;\n    vocab_size = vocab_size,\n    n_embd     = n_embd,\n    block_size = block_size,\n    n_layer    = n_layer,\n    n_head     = n_head,\n    dropout    = dropout\n) |> device\n\n# Count parameters\nn_params = sum(length, Flux.params(model))\nprintln(\"Model created on $device\")\nprintln(\"Parameters: $(n_params) ($(round(n_params/1e6, digits=2))M)\")\n\n# Quick smoke test\nx_test, y_test = get_batch(\"train\")\nlogits_test = model(x_test)\nprintln(\"Forward pass OK — logits: $(size(logits_test))\")",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 280
    },
    "id": "WA_xkNSH1Y8y",
    "outputId": "1ffd1b10-e2b2-4be8-b151-f9e8ed1d23c5"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# ── Training loop ──\nusing Printf\n\nfunction estimate_loss(model, n_iters=eval_iters)\n    model_eval = Flux.testmode!(deepcopy(model))\n    losses = Dict(\"train\" => 0.0, \"val\" => 0.0)\n    for split in [\"train\", \"val\"]\n        total = 0.0\n        for _ in 1:n_iters\n            x, y = get_batch(split)\n            logits = model_eval(x)                              # (vocab, T, B)\n            loss = Flux.logitcrossentropy(\n                reshape(logits, vocab_size, :),                 # (vocab, T*B)\n                Flux.onehotbatch(reshape(y, :), 1:vocab_size)   # (vocab, T*B)\n            )\n            total += loss\n        end\n        losses[split] = total / n_iters\n    end\n    return losses\nend\n\n# LR schedule: warmup + cosine decay\nfunction get_lr(iter)\n    if iter < warmup_iters\n        return learning_rate * iter / warmup_iters\n    end\n    decay_ratio = (iter - warmup_iters) / (max_iters - warmup_iters)\n    coeff = 0.5 * (1.0 + cos(Float64(pi) * decay_ratio))\n    return min_lr + coeff * (learning_rate - min_lr)\nend\n\nopt_state = Flux.setup(Adam(learning_rate), model)\n\nprintln(\"Training for $max_iters steps...\")\nt_start = time()\nbest_val = Inf\n\nfor iter in 1:max_iters\n    # Update LR\n    lr_t = get_lr(iter)\n    Flux.adjust!(opt_state, lr_t)\n\n    # Forward + backward + update\n    x, y = get_batch(\"train\")\n    loss, grads = Flux.withgradient(model) do m\n        logits = m(x)\n        Flux.logitcrossentropy(\n            reshape(logits, vocab_size, :),\n            Flux.onehotbatch(reshape(y, :), 1:vocab_size)\n        )\n    end\n    Flux.update!(opt_state, model, grads[1])\n\n    # Eval + print\n    if iter % eval_interval == 0 || iter == 1\n        losses = estimate_loss(model)\n        improved = losses[\"val\"] < best_val ? \" *\" : \"\"\n        if losses[\"val\"] < best_val\n            best_val = losses[\"val\"]\n        end\n        elapsed = round(time() - t_start, digits=1)\n        @printf(\"step %5d | train %.4f | val %.4f | lr %.2e | %.1fs%s\\n\",\n                iter, losses[\"train\"], losses[\"val\"], lr_t, elapsed, improved)\n    end\nend\n\nelapsed = round(time() - t_start, digits=1)\nprintln(\"\\nTraining complete in $(elapsed)s. Best val loss: $(round(best_val, digits=4))\")",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 193
    },
    "id": "hrMTKpHEtHdi",
    "outputId": "ef9dd250-af9b-4982-9f3c-7d1b7b64dbac"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}