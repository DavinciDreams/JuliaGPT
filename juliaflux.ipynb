{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "name": "julia",
   "display_name": "Julia"
  },
  "language_info": {
   "name": "julia"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/DavinciDreams/JuliaGPT/blob/main/juliaflux.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# JuliaFlux GPT — A Flux.jl Transformer for Philosophy\n\nGPU-accelerated character-level GPT trained on classical philosophy texts.\nUses Flux.jl + Zygote.jl + CUDA.jl for automatic differentiation and GPU compute.\n\n**Architecture** (following GPT-2 with simplifications):\n- Multi-head causal self-attention with batched matrix ops\n- LayerNorm, GELU activations, residual connections\n- Cosine LR schedule with warmup\n- JLD2 checkpoint persistence (local + HuggingFace Hub)\n- W&B logging + HuggingFace Hub integration\n\nBased on: https://github.com/DavinciDreams/JuliaGPT"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 0. Login & Setup\n\nThis cell runs in **Python** to read your Colab secrets and save them for Julia.\n\n1. Add secrets via the key icon in the left sidebar:\n   - `HF_TOKEN` — your HuggingFace access token\n   - `WANDB_KEY` — your Weights & Biases API key\n   - `HF_REPO` — your model repo (e.g. `LisaMegaWatts/JuliaFluxGPT`)\n   - `HF_DATA_REPO` — your dataset repo (e.g. `LisaMegaWatts/philosophy-corpus`)\n2. Run cells 0-1 (login + install Julia, ~3-5 min)\n3. **Runtime > Change runtime type > Julia 1.10**\n4. Continue with the remaining Julia cells"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# ── Minimal Python setup: install tools + save Colab secrets for Julia ──\n!pip install -q wandb huggingface_hub datasets\n\nimport json, pathlib, os\n\nsecrets = {}\ntry:\n    from google.colab import userdata\n    for key in (\"HF_TOKEN\", \"WANDB_KEY\", \"HF_REPO\", \"HF_DATA_REPO\"):\n        try: secrets[key] = userdata.get(key)\n        except Exception: pass\nexcept ImportError:\n    pass\n\nsecrets_path = pathlib.Path.home() / \".julia_secrets.json\"\nsecrets_path.write_text(json.dumps(secrets))\nsecrets_path.chmod(0o600)\n\nfound = [k for k in secrets if secrets[k]]\nprint(f\"Secrets saved: {', '.join(found) if found else 'none found'}\")\nprint(f\"  -> {secrets_path}\")\nif not found:\n    print(\"Add HF_TOKEN, WANDB_KEY, HF_REPO via the key icon in the sidebar.\")\n\n# ── Download pre-cleaned dataset from HuggingFace ──\nDEFAULT_DATA_REPO = \"LisaMegaWatts/philosophy-corpus\"\ndata_repo = secrets.get(\"HF_DATA_REPO\", \"\") or DEFAULT_DATA_REPO\ndata_dir = pathlib.Path(\"data\")\ntrain_file = data_dir / \"train.txt\"\nval_file = data_dir / \"val.txt\"\n\nif not (train_file.exists() and val_file.exists()):\n    print(f\"\\nDownloading dataset from HuggingFace: {data_repo}\")\n    data_dir.mkdir(exist_ok=True)\n    if secrets.get(\"HF_TOKEN\"):\n        os.environ[\"HF_TOKEN\"] = secrets[\"HF_TOKEN\"]\n    try:\n        from datasets import load_dataset\n        ds = load_dataset(data_repo)\n        with open(train_file, \"w\") as f:\n            for row in ds[\"train\"]:\n                f.write(row[\"text\"] + \"\\n\")\n        split_name = \"validation\" if \"validation\" in ds else \"val\"\n        with open(val_file, \"w\") as f:\n            for row in ds[split_name]:\n                f.write(row[\"text\"] + \"\\n\")\n        print(f\"  train.txt: {train_file.stat().st_size:,} bytes\")\n        print(f\"  val.txt:   {val_file.stat().st_size:,} bytes\")\n    except Exception as e:\n        print(f\"  Dataset download failed: {e}\")\n        print(\"  Copy data/train.txt and data/val.txt manually.\")\nelse:\n    print(f\"\\nDataset already downloaded: {data_dir}/\")\n\nprint(\"\\nDone! Now run the next cell to install Julia (~3-5 min).\")\nprint(\"Then: Runtime > Change runtime type > Julia 1.10\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Julia Kernel",
    "",
    "This cell downloads and installs Julia + IJulia + Flux packages. **Takes ~5-10 minutes** on first run.",
    "",
    "**After it finishes:**",
    "1. Go to **Runtime > Change runtime type**",
    "2. Pick **Julia 1.10**",
    "3. Continue running the cells below"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "%%shell\n",
    "set -e\n",
    "\n",
    "JULIA_VERSION=\"1.10.5\"\n",
    "JULIA_MINOR=\"1.10\"\n",
    "\n",
    "if [ ! -d \"/usr/local/julia-${JULIA_VERSION}\" ]; then\n",
    "    echo \"Downloading Julia ${JULIA_VERSION}...\"\n",
    "    wget -q https://julialang-s3.julialang.org/bin/linux/x64/${JULIA_MINOR}/julia-${JULIA_VERSION}-linux-x86_64.tar.gz\n",
    "    tar xzf julia-${JULIA_VERSION}-linux-x86_64.tar.gz -C /usr/local/\n",
    "    rm julia-${JULIA_VERSION}-linux-x86_64.tar.gz\n",
    "    ln -sf /usr/local/julia-${JULIA_VERSION}/bin/julia /usr/local/bin/julia\n",
    "    echo \"Julia installed.\"\n",
    "else\n",
    "    echo \"Julia already installed.\"\n",
    "fi\n",
    "\n",
    "julia -e '\n",
    "    using Pkg\n",
    "    Pkg.add(\"IJulia\")\n",
    "    Pkg.add([\"Flux\", \"Zygote\", \"Optimisers\", \"CUDA\", \"cuDNN\",\n",
    "             \"Downloads\", \"Statistics\", \"Random\", \"Printf\",\n",
    "             \"LinearAlgebra\", \"JLD2\", \"NNlib\", \"JSON3\"])\n",
    "    using IJulia\n",
    "    installkernel(\"Julia\")\n",
    "'\n",
    "\n",
    "echo \"\"\n",
    "echo \"===========================================================\"\n",
    "echo \"  Julia kernel installed!                                   \"\n",
    "echo \"  Now: Runtime -> Change runtime type -> pick Julia 1.10       \"\n",
    "echo \"  Then run the cells below.                              \"\n",
    "echo \"===========================================================\" \n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1b. Load Credentials + W&B / HuggingFace Helpers (Julia)\n\nReads tokens from `~/.julia_secrets.json` (written by the Python setup cell).\nW&B logging uses a persistent Python subprocess fed JSON lines from Julia.\nHuggingFace helpers use `huggingface-cli` to push/pull checkpoints."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "using JSON3\n\n# ── Ensure pip-installed binaries (huggingface-cli, wandb) are on PATH ──\nfor p in [\"/usr/local/bin\", joinpath(homedir(), \".local/bin\"), \"/root/.local/bin\"]\n    if isdir(p) && !occursin(p, get(ENV, \"PATH\", \"\"))\n        ENV[\"PATH\"] = p * \":\" * get(ENV, \"PATH\", \"\")\n    end\nend\n\n# ── Read credentials from ~/.julia_secrets.json (written by Python setup cell) ──\nfunction load_secrets()\n    path = expanduser(\"~/.julia_secrets.json\")\n    if !isfile(path)\n        @warn \"No secrets file found at $path — run the Python setup cell first\"\n        return Dict{String,String}()\n    end\n    raw = JSON3.read(read(path, String))\n    return Dict{String,String}(string(k) => string(v) for (k, v) in pairs(raw) if !isempty(string(v)))\nend\n\nsecrets = load_secrets()\n\n# W&B\nif haskey(secrets, \"WANDB_KEY\")\n    ENV[\"WANDB_API_KEY\"] = secrets[\"WANDB_KEY\"]\n    println(\"W&B API key: found\")\nelse\n    println(\"W&B API key: not found (add WANDB_KEY to Colab secrets)\")\nend\n\n# HuggingFace token\nif haskey(secrets, \"HF_TOKEN\")\n    ENV[\"HF_TOKEN\"] = secrets[\"HF_TOKEN\"]\n    println(\"HF token: found\")\nelse\n    println(\"HF token: not found (add HF_TOKEN to Colab secrets)\")\nend\n\n# HuggingFace repo ID\nHF_REPO_ID = get(secrets, \"HF_REPO\", \"\")\nif !isempty(HF_REPO_ID)\n    println(\"HF repo: \", HF_REPO_ID)\nelse\n    println(\"HF repo: not set (add HF_REPO to Colab secrets or set HF_REPO_ID manually)\")\nend",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "WANDB_PROJECT = \"juliaflux-philosophy\"\nWANDB_RUN_ID = \"juliaflux-\" * join(rand('a':'z', 6))\n\n# Write a tiny Python helper that reads JSON lines on stdin\nwrite(\"_wandb_log.py\", \"\"\"\nimport wandb, json, sys, os\nproject = os.environ.get(\"WANDB_PROJECT\", \"juliaflux-philosophy\")\nrun_id = os.environ.get(\"WANDB_RUN_ID\", None)\nrun = wandb.init(project=project, id=run_id, resume=\"allow\",\n                 config={\"model\": \"juliaflux-gpt\", \"architecture\": \"multi-layer transformer\"})\nprint(f\"W&B run: {run.url}\", flush=True)\nfor line in sys.stdin:\n    line = line.strip()\n    if not line:\n        continue\n    try:\n        data = json.loads(line)\n        wandb.log(data)\n    except Exception as e:\n        print(f\"wandb log error: {e}\", file=sys.stderr, flush=True)\nwandb.finish()\n\"\"\")\n\nwandb_proc = nothing\n\nfunction wandb_init()\n    global wandb_proc, WANDB_PROJECT, WANDB_RUN_ID\n    if !haskey(ENV, \"WANDB_API_KEY\") || isempty(ENV[\"WANDB_API_KEY\"])\n        println(\"W&B: skipped (no API key)\")\n        return\n    end\n    ENV[\"WANDB_PROJECT\"] = WANDB_PROJECT\n    ENV[\"WANDB_RUN_ID\"] = WANDB_RUN_ID\n    wandb_proc = open(`python3 _wandb_log.py`, \"r+\")\n    println(\"W&B: initialized ($WANDB_PROJECT / $WANDB_RUN_ID)\")\nend\n\nfunction wandb_log(; kwargs...)\n    global wandb_proc\n    wandb_proc === nothing && return\n    metrics = Dict(string(k) => v for (k, v) in kwargs)\n    try\n        println(wandb_proc, JSON3.write(metrics))\n        flush(wandb_proc)\n    catch e\n        println(\"W&B log error: $e\")\n    end\nend\n\nfunction wandb_finish()\n    global wandb_proc\n    wandb_proc === nothing && return\n    try close(wandb_proc) catch end\n    wandb_proc = nothing\n    println(\"W&B: run finished\")\nend\n\n# ═══════════════════════════════════════════════════════════════\n# HuggingFace Hub helpers\n# ═══════════════════════════════════════════════════════════════\n\nfunction hf_push(repo_id::String, local_path::String; remote_path::String=\"\")\n    rp = isempty(remote_path) ? basename(local_path) : remote_path\n    run(`huggingface-cli upload $repo_id $local_path $rp`)\n    println(\"Pushed $local_path -> $repo_id/$rp\")\nend\n\nfunction hf_pull(repo_id::String, remote_path::String; local_dir::String=\"checkpoints\")\n    mkpath(local_dir)\n    run(`huggingface-cli download $repo_id $remote_path --local-dir $local_dir`)\n    println(\"Pulled $repo_id/$remote_path -> $local_dir/\")\nend\n\nfunction hf_push_checkpoint(repo_id::String; checkpoint_path::String=\"checkpoints/best_model.jld2\")\n    isfile(checkpoint_path) || error(\"Checkpoint not found: $checkpoint_path\")\n    hf_push(repo_id, checkpoint_path)\nend\n\nfunction hf_create_repo(repo_id::String)\n    try\n        run(`huggingface-cli repo create $repo_id --type model`)\n        println(\"Created HF repo: $repo_id\")\n    catch\n        println(\"HF repo already exists or creation skipped: $repo_id\")\n    end\nend\n\n# ═══════════════════════════════════════════════════════════════\n# Sync helper: push checkpoint to HuggingFace Hub\n# ═══════════════════════════════════════════════════════════════\n\nfunction hf_sync(local_path::String)\n    if !@isdefined(HF_REPO_ID) || isempty(HF_REPO_ID)\n        return\n    end\n    try\n        hf_push(HF_REPO_ID, local_path)\n    catch e\n        println(\"  HF sync failed: $e\")\n    end\nend\n\nprintln(\"W&B + HuggingFace helpers defined\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---",
    "## 2. Imports & Setup",
    "",
    "Load Flux.jl ecosystem packages. CUDA is auto-detected for GPU acceleration."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "using Flux\nusing Zygote\nusing Optimisers\nusing CUDA\nusing NNlib\nusing Downloads\nusing Statistics\nusing Random\nusing Printf\nusing LinearAlgebra\nusing JLD2\n\nRandom.seed!(1337)\n\ndevice = CUDA.functional() ? gpu : cpu\nprintln(\"Device: \", device)\nprintln(\"CUDA functional: \", CUDA.functional())\nif CUDA.functional()\n    println(\"GPU: \", CUDA.name(CUDA.device()))\n    mem = CUDA.totalmem(CUDA.device())\n    println(\"VRAM: \", round(mem / 1024^3, digits=1), \" GB\")\n    println(\"CUDA version: \", CUDA.runtime_version())\nend",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---",
    "## 3. Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# ── Model architecture ──\nblock_size     = 256       # context window (doubled from 128)\nn_embd         = 384       # embedding dim (up from 256 — better A100 utilization)\nn_head         = 6         # attention heads (up from 4)\nn_layer        = 6         # transformer layers (up from 4)\ndropout        = 0.1\nbias           = false\n\n# ── Training ──\nbatch_size     = 64        # batch size (doubled from 32 — A100 has plenty of VRAM)\nlearning_rate  = 3e-4      # slightly lower LR for larger model\nmax_iters      = 5000\neval_interval  = 500\neval_iters     = 100\nwarmup_iters   = 200\nmin_lr         = 1e-5\n\nprintln(\"n_embd=$n_embd, n_layer=$n_layer, n_head=$n_head, block_size=$block_size, batch_size=$batch_size\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## 4. Dataset — Load Pre-Cleaned Corpus\n\nLoads **pre-cleaned philosophy corpus** produced by the\n[text-pipeline](https://github.com/DavinciDreams/buildwithbooks).\n\n**Data flow:**\n```\ntext-pipeline/ -> download + clean + chunk -> output/train.txt, val.txt\n                -> push to HuggingFace Dataset\n                                    |\njuliaflux.ipynb -> pulls data/train.txt, val.txt -> tokenize -> train\n```\n\nThe pipeline applies an 8-stage cleaner (boilerplate stripping, unicode normalization,\ncharacter filtering to `a-z .` only) and sentence-boundary chunking (40-256 chars).\n\n**To add new texts:** add sources via the pipeline's Gradio UI or manifest,\nre-run the pipeline, push to HuggingFace, and re-run this cell."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# ── Load pre-cleaned data from text pipeline ──\n# The Python setup cell downloads from HuggingFace before kernel switch.\n# Default dataset: LisaMegaWatts/philosophy-corpus\n\nDATA_DIR = \"data\"\ntrain_file = joinpath(DATA_DIR, \"train.txt\")\nval_file   = joinpath(DATA_DIR, \"val.txt\")\n\n# Julia-side fallback: try huggingface-cli if Python download missed\nDEFAULT_DATA_REPO = \"LisaMegaWatts/philosophy-corpus\"\nHF_DATA_REPO = let r = get(secrets, \"HF_DATA_REPO\", \"\"); isempty(r) ? DEFAULT_DATA_REPO : r end\n\nif !isfile(train_file) || !isfile(val_file)\n    println(\"Data not found locally, trying huggingface-cli download...\")\n    mkpath(DATA_DIR)\n    try\n        run(`huggingface-cli download $HF_DATA_REPO train.txt val.txt --local-dir $DATA_DIR`)\n        println(\"Downloaded from $HF_DATA_REPO\")\n    catch e\n        @warn \"HuggingFace download failed: $e\"\n    end\nend\n\nif !isfile(train_file) || !isfile(val_file)\n    error(\"\"\"\n    No pre-cleaned data found in $DATA_DIR/!\n\n    The Python setup cell should download from $HF_DATA_REPO automatically.\n    Re-run the Python setup cell, or copy files manually:\n        cp text-pipeline/output/train.txt data/train.txt\n        cp text-pipeline/output/val.txt   data/val.txt\n    \"\"\")\nend\n\ntrain_text = read(train_file, String)\nval_text   = read(val_file, String)\n\nprintln(\"Pre-cleaned data loaded from $DATA_DIR/ ($HF_DATA_REPO)\")\nprintln(\"  train.txt: $(length(train_text)) chars ($(count('\\n', train_text)) chunks)\")\nprintln(\"  val.txt:   $(length(val_text)) chars ($(count('\\n', val_text)) chunks)\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# ── Build character-level tokenizer from pre-cleaned corpus ──\n# Pipeline uses allowed_chars: \"a-z .\" so vocab is ~28 chars (letters + space + period)\n\n# Join train + val to discover full character set\nfull_text = train_text * \"\\n\" * val_text\nchars = sort(unique(full_text))\n\n# Remove newline from vocab — it's a chunk separator, not content\nfilter!(c -> c != '\\n', chars)\nglobal vocab_size = length(chars)\n\nstoi = Dict(c => i for (i, c) in enumerate(chars))\nitos = Dict(i => c for (i, c) in enumerate(chars))\n\nencode(s) = [stoi[c] for c in s if haskey(stoi, c)]\ndecode(ids) = join(itos[i] for i in ids)\n\n# Encode train and val separately (pipeline already split 90/10)\n# Replace newlines (chunk separators) with spaces for continuous text\ntrain_clean = replace(strip(train_text), '\\n' => ' ')\nval_clean   = replace(strip(val_text), '\\n' => ' ')\n\nglobal train_data = encode(train_clean)\nglobal val_data   = encode(val_clean)\n\nprintln(\"Vocab: $(vocab_size) chars -> [$(join(chars))]\")\nprintln(\"Train: $(length(train_data)) tokens\")\nprintln(\"Val:   $(length(val_data)) tokens\")\nprintln(\"Total: $(length(train_data) + length(val_data)) tokens\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ue4yfbp6a1s",
   "source": "---\n## 5. Batch Loader & Model Architecture\n\nMini-batch loader + full GPT model definition using Flux.jl structs.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "7jpawnpibqg",
   "source": "function get_batch(split=\"train\")\n    d = split == \"train\" ? train_data : val_data\n    ix = rand(1:length(d)-block_size, batch_size)\n    x = hcat([d[i:i+block_size-1] for i in ix]...)\n    y = hcat([d[i+1:i+block_size] for i in ix]...)\n    x = permutedims(x)   # now (B, T)\n    y = permutedims(y)\n    x = x |> device\n    y = y |> device\n    return x, y\nend",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# ── All model structs in ONE cell (Julia structs can't be redefined) ──\nusing NNlib: batched_mul\n\n# Pre-compute causal mask once (avoids recreating on every forward pass)\nconst CAUSAL_MASK = triu(fill(typemin(Float32), block_size, block_size), 1)\nconst CAUSAL_MASK_GPU = CUDA.functional() ? cu(CAUSAL_MASK) : CAUSAL_MASK\n\n# Pre-compute position indices (avoids CPU allocation each forward pass)\nconst POS_RANGE = collect(Int32, 1:block_size)\nconst POS_RANGE_GPU = CUDA.functional() ? cu(POS_RANGE) : POS_RANGE\n\nstruct CausalSelfAttention\n    qkv::Dense       # single projection: n_embd -> 3*n_embd\n    proj::Dense       # output projection: n_embd -> n_embd\n    n_head::Int\nend\n\nFlux.@layer CausalSelfAttention trainable=(qkv, proj)\n\nfunction CausalSelfAttention(n_embd::Int, n_head::Int; bias=false)\n    CausalSelfAttention(\n        Dense(n_embd => 3 * n_embd; bias),\n        Dense(n_embd => n_embd; bias),\n        n_head\n    )\nend\n\nfunction (attn::CausalSelfAttention)(x)\n    C, T, B = size(x)\n    hs = C ÷ attn.n_head\n    nh = attn.n_head\n\n    qkv = attn.qkv(x)\n    q = qkv[1:C, :, :]\n    k = qkv[C+1:2C, :, :]\n    v = qkv[2C+1:3C, :, :]\n\n    q = reshape(permutedims(reshape(q, hs, nh, T, B), (1, 3, 2, 4)), hs, T, nh * B)\n    k = reshape(permutedims(reshape(k, hs, nh, T, B), (1, 3, 2, 4)), hs, T, nh * B)\n    v = reshape(permutedims(reshape(v, hs, nh, T, B), (1, 3, 2, 4)), hs, T, nh * B)\n\n    scale = Float32(1 / sqrt(hs))\n    wei = batched_mul(permutedims(q, (2, 1, 3)), k) .* scale\n\n    # Use pre-computed causal mask (slice to current sequence length)\n    mask = x isa CuArray ? CAUSAL_MASK_GPU[1:T, 1:T] : CAUSAL_MASK[1:T, 1:T]\n    wei = wei .+ mask\n    wei = softmax(wei; dims=2)\n\n    out = batched_mul(v, permutedims(wei, (2, 1, 3)))\n    out = reshape(permutedims(reshape(out, hs, T, nh, B), (1, 3, 2, 4)), C, T, B)\n\n    attn.proj(out)\nend\n\nstruct FeedForward\n    net::Chain\nend\n\nFlux.@layer FeedForward\n\nfunction FeedForward(n_embd::Int; bias=false, dropout=0.0)\n    FeedForward(Chain(\n        Dense(n_embd => 4 * n_embd; bias),\n        gelu,\n        Dense(4 * n_embd => n_embd; bias),\n        Dropout(dropout)\n    ))\nend\n\n(ff::FeedForward)(x) = ff.net(x)\n\nstruct TransformerBlock\n    ln1::LayerNorm\n    attn::CausalSelfAttention\n    ln2::LayerNorm\n    ffwd::FeedForward\nend\n\nFlux.@layer TransformerBlock\n\nfunction TransformerBlock(n_embd::Int, n_head::Int; dropout=0.0)\n    TransformerBlock(\n        LayerNorm(n_embd),\n        CausalSelfAttention(n_embd, n_head),\n        LayerNorm(n_embd),\n        FeedForward(n_embd; dropout)\n    )\nend\n\nfunction (block::TransformerBlock)(x)\n    x = x .+ block.attn(block.ln1(x))\n    x = x .+ block.ffwd(block.ln2(x))\n    x\nend\n\nstruct GPT\n    wte::Embedding\n    wpe::Embedding\n    drop::Dropout\n    blocks::Chain\n    ln_f::LayerNorm\n    lm_head::Dense\nend\n\nFlux.@layer GPT\n\nfunction GPT(; vocab_size, n_embd, block_size, n_layer, n_head, dropout=0.1)\n    GPT(\n        Embedding(vocab_size => n_embd),\n        Embedding(block_size => n_embd),\n        Dropout(dropout),\n        Chain([TransformerBlock(n_embd, n_head; dropout) for _ in 1:n_layer]...),\n        LayerNorm(n_embd),\n        Dense(n_embd => vocab_size; bias=false)\n    )\nend\n\nfunction (m::GPT)(idx)\n    B, T = size(idx)\n    tok = permutedims(m.wte(idx), (1, 3, 2))\n    # Use pre-computed position indices (already on correct device)\n    pos_src = idx isa CuArray ? POS_RANGE_GPU : POS_RANGE\n    pos_ids = repeat(reshape(pos_src[1:T], 1, T), B, 1)\n    pos = permutedims(m.wpe(pos_ids), (1, 3, 2))\n    x = m.drop(tok .+ pos)\n    x = m.blocks(x)\n    x = m.ln_f(x)\n    m.lm_head(x)\nend\n\nprintln(\"All model structs defined: CausalSelfAttention, FeedForward, TransformerBlock, GPT\")\nprintln(\"Pre-computed: causal mask ($(size(CAUSAL_MASK))), position indices ($(length(POS_RANGE)))\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "model = GPT(;\n    vocab_size = vocab_size,\n    n_embd     = n_embd,\n    block_size = block_size,\n    n_layer    = n_layer,\n    n_head     = n_head,\n    dropout    = dropout\n) |> device\n\nn_params = sum(length, Flux.trainables(model))\nprintln(\"Model created on $device\")\nprintln(\"Parameters: $(n_params) ($(round(n_params/1e6, digits=2))M)\")\n\nif CUDA.functional()\n    used = CUDA.memory_status()\n    println(\"GPU memory after model: $(round(CUDA.used_memory() / 1024^2, digits=1)) MB used\")\nend\n\n# Quick smoke test\nx_test, y_test = get_batch(\"train\")\nlogits_test = model(x_test)\nprintln(\"Forward pass OK -- logits: $(size(logits_test))\")\n\nif CUDA.functional()\n    println(\"GPU memory after forward pass: $(round(CUDA.used_memory() / 1024^2, digits=1)) MB used\")\nend",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## 6. Checkpoint Save/Load\n\nSave and load model weights + optimizer state as JLD2 files.\nCheckpoints are synced to HuggingFace Hub for persistence across Colab sessions."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "LOCAL_CKPT = \"checkpoints\"\nmkpath(LOCAL_CKPT)\n\nfunction save_checkpoint(path::String, model, opt_state;\n                          step::Int=0, best_val_loss::Float64=Inf,\n                          train_losses::Vector{Float64}=Float64[],\n                          val_losses::Vector{Float64}=Float64[])\n    mkpath(dirname(path))\n    model_cpu = cpu(model)\n    opt_cpu = cpu(opt_state)\n    JLD2.jldsave(path;\n        model_state = Flux.state(model_cpu),\n        opt_state = opt_cpu,\n        step = step,\n        best_val_loss = best_val_loss,\n        train_losses = train_losses,\n        val_losses = val_losses,\n        hyperparams = Dict(\n            \"vocab_size\" => vocab_size,\n            \"n_embd\" => n_embd,\n            \"block_size\" => block_size,\n            \"n_layer\" => n_layer,\n            \"n_head\" => n_head,\n            \"dropout\" => dropout\n        )\n    )\n    vl_str = best_val_loss == Inf ? \"Inf\" : @sprintf(\"%.4f\", best_val_loss)\n    println(\"Checkpoint saved: $path (step $step, best_val_loss=$vl_str)\")\nend\n\nfunction save_and_sync(path, model, opt_state; kwargs...)\n    save_checkpoint(path, model, opt_state; kwargs...)\n    hf_sync(path)\nend\n\nfunction load_checkpoint(path::String, device_fn)\n    println(\"Loading checkpoint from $path ...\")\n    data = JLD2.load(path)\n\n    hp = data[\"hyperparams\"]\n    m = GPT(;\n        vocab_size = hp[\"vocab_size\"],\n        n_embd     = hp[\"n_embd\"],\n        block_size = hp[\"block_size\"],\n        n_layer    = hp[\"n_layer\"],\n        n_head     = hp[\"n_head\"],\n        dropout    = get(hp, \"dropout\", 0.1)\n    )\n    Flux.loadmodel!(m, data[\"model_state\"])\n    m = m |> device_fn\n\n    opt = data[\"opt_state\"]\n\n    println(\"  step=$(data[\"step\"]), best_val=$(round(data[\"best_val_loss\"], digits=4))\")\n    return (;\n        model = m,\n        opt_state = opt |> device_fn,\n        step = data[\"step\"],\n        best_val_loss = data[\"best_val_loss\"],\n        train_losses = get(data, \"train_losses\", Float64[]),\n        val_losses = get(data, \"val_losses\", Float64[])\n    )\nend\n\nprintln(\"Checkpoint save/load defined (JLD2 + HuggingFace sync)\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## 7. Training Loop with Validation + Best-Model Checkpointing\n\nAdam optimizer with cosine LR decay + warmup.\nValidates every 500 steps, saves `best_model.jld2` when val loss improves.\nCheckpoints sync to HuggingFace Hub automatically.\nDefensive saves: try/catch with emergency checkpoint, time-based auto-save."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "using Printf\n\nfunction estimate_loss(model, n_iters=eval_iters)\n    model_eval = Flux.testmode!(deepcopy(model))\n    losses = Dict(\"train\" => 0.0, \"val\" => 0.0)\n    for split in [\"train\", \"val\"]\n        total = 0.0\n        for _ in 1:n_iters\n            x, y = get_batch(split)\n            logits = model_eval(x)\n            # Compute loss entirely on GPU — reshape targets to match logits\n            y_flat = reshape(y, :)\n            logits_flat = reshape(logits, vocab_size, :)\n            onehot = Flux.onehotbatch(y_flat, 1:vocab_size) |> device\n            loss = Flux.logitcrossentropy(logits_flat, onehot)\n            total += loss\n        end\n        losses[split] = total / n_iters\n    end\n    return losses\nend\n\n# LR schedule: warmup + cosine decay\nfunction get_lr(iter)\n    if iter < warmup_iters\n        return learning_rate * iter / warmup_iters\n    end\n    decay_ratio = (iter - warmup_iters) / (max_iters - warmup_iters)\n    coeff = 0.5 * (1.0 + cos(Float64(pi) * decay_ratio))\n    return min_lr + coeff * (learning_rate - min_lr)\nend\n\nopt_state = Flux.setup(Adam(learning_rate), model)\n\nbest_val = Inf\ntrain_loss_history = Float64[]\nval_loss_history = Float64[]\n\n# ── Initialize W&B logging (if API key is set) ──\nif haskey(ENV, \"WANDB_API_KEY\") && !isempty(ENV[\"WANDB_API_KEY\"])\n    wandb_init()\nend\n\nSAVE_INTERVAL = 600  # auto-save every 10 min\nlast_save_time = time()\ncompleted_iter = 0\n\nhf_status = !isempty(HF_REPO_ID) ? \"HF:$HF_REPO_ID\" : \"HF:(not configured)\"\nprintln(\"Training for $max_iters steps...\")\nprintln(\"    Local: $LOCAL_CKPT/  |  $hf_status\")\nt_start = time()\n\ntry\n    for iter in 1:max_iters\n        global completed_iter = iter\n\n        # Update LR\n        lr_t = get_lr(iter)\n        Flux.adjust!(opt_state, lr_t)\n\n        # Forward + backward + update (loss computed entirely on GPU)\n        x, y = get_batch(\"train\")\n        loss, grads = Flux.withgradient(model) do m\n            logits = m(x)\n            y_flat = reshape(y, :)\n            logits_flat = reshape(logits, vocab_size, :)\n            onehot = Flux.onehotbatch(y_flat, 1:vocab_size) |> device\n            Flux.logitcrossentropy(logits_flat, onehot)\n        end\n        Flux.update!(opt_state, model, grads[1])\n        push!(train_loss_history, Float64(loss))\n\n        # Incremental GC to prevent GPU memory fragmentation\n        if iter % 100 == 0 && CUDA.functional()\n            GC.gc(false)\n        end\n\n        # Eval + print + checkpoint\n        if iter % eval_interval == 0 || iter == 1\n            losses = estimate_loss(model)\n            push!(val_loss_history, losses[\"val\"])\n            elapsed = round(time() - t_start, digits=1)\n            wandb_log(; step=iter, train_loss=Float64(loss), val_loss=losses[\"val\"], lr=lr_t)\n\n            improved = \"\"\n            if losses[\"val\"] < best_val\n                best_val = losses[\"val\"]\n                save_and_sync(\"checkpoints/best_model.jld2\", model, opt_state;\n                    step=iter, best_val_loss=best_val,\n                    train_losses=train_loss_history, val_losses=val_loss_history)\n                improved = \" << new best!\"\n            end\n\n            @printf(\"step %5d | train %.4f | val %.4f | lr %.2e | %.1fs%s\\n\",\n                    iter, losses[\"train\"], losses[\"val\"], lr_t, elapsed, improved)\n        end\n\n        # Periodic checkpoint every 1000 steps\n        if iter % 1000 == 0\n            save_and_sync(\"checkpoints/checkpoint_latest.jld2\", model, opt_state;\n                step=iter, best_val_loss=best_val,\n                train_losses=train_loss_history, val_losses=val_loss_history)\n            last_save_time = time()\n        end\n\n        # Time-based auto-save every 10 min\n        if time() - last_save_time > SAVE_INTERVAL\n            save_and_sync(\"checkpoints/checkpoint_latest.jld2\", model, opt_state;\n                step=iter, best_val_loss=best_val,\n                train_losses=train_loss_history, val_losses=val_loss_history)\n            last_save_time = time()\n            println(\"  [auto-save at step $iter]\")\n        end\n    end\ncatch e\n    if e isa InterruptException\n        println(\"\\n\\nTraining interrupted at step $completed_iter!\")\n    else\n        println(\"\\n\\nTraining error at step $completed_iter: $e\")\n    end\n    println(\"Saving emergency checkpoint...\")\n    save_and_sync(\"checkpoints/checkpoint_interrupted.jld2\", model, opt_state;\n        step=completed_iter, best_val_loss=best_val,\n        train_losses=train_loss_history, val_losses=val_loss_history)\n    if !(e isa InterruptException)\n        rethrow(e)\n    end\nend\n\nelapsed = round(time() - t_start, digits=1)\nprintln(\"\\nTraining complete in $(elapsed)s. Best val loss: $(round(best_val, digits=4))\")\nwandb_finish()\n\n# Final save\nsave_and_sync(\"checkpoints/final_model.jld2\", model, opt_state;\n    step=max_iters, best_val_loss=best_val,\n    train_losses=train_loss_history, val_losses=val_loss_history)",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---",
    "## 8. Inference — Generate Text",
    "",
    "Generate new philosophy-like text using temperature-controlled sampling."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "function generate_text(model, max_tokens=200; temperature=0.8)\n",
    "    model_eval = Flux.testmode!(deepcopy(model))\n",
    "    # Start with a random token\n",
    "    idx = reshape([rand(1:vocab_size)], 1, 1) |> device\n",
    "    generated = Int[]\n",
    "\n",
    "    for _ in 1:max_tokens\n",
    "        # Crop to block_size\n",
    "        idx_cond = idx[:, max(1, end-block_size+1):end]\n",
    "        logits = model_eval(idx_cond)     # (vocab, T, B)\n",
    "        logits_last = logits[:, end, 1]   # (vocab,) last token logits\n",
    "\n",
    "        # Temperature scaling + softmax\n",
    "        probs = softmax(logits_last ./ Float32(temperature))\n",
    "        probs_cpu = Float64.(cpu(probs))\n",
    "\n",
    "        # Categorical sampling\n",
    "        r = rand()\n",
    "        cum = 0.0\n",
    "        next_id = 1\n",
    "        for (i, p) in enumerate(probs_cpu)\n",
    "            cum += p\n",
    "            if r <= cum\n",
    "                next_id = i\n",
    "                break\n",
    "            end\n",
    "        end\n",
    "\n",
    "        push!(generated, next_id)\n",
    "        next_token = reshape([next_id], 1, 1) |> device\n",
    "        idx = hcat(idx, next_token)\n",
    "    end\n",
    "\n",
    "    return decode(generated)\n",
    "end\n",
    "\n",
    "println(\"--- inference (hallucinated philosophy) ---\")\n",
    "for i in 1:5\n",
    "    text = generate_text(model, 300; temperature=0.8)\n",
    "    @printf(\"\\nsample %d:\\n%s\\n\", i, text[1:min(end, 500)])\n",
    "    println(\"---\")\n",
    "end\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---",
    "## 8a. Push Model to HuggingFace Hub",
    "",
    "Push your trained checkpoint to HuggingFace for persistence across Colab sessions.",
    "Set `HF_REPO_ID` in the login cell above."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "if @isdefined(HF_REPO_ID) && !isempty(HF_REPO_ID)\n",
    "    hf_create_repo(HF_REPO_ID)\n",
    "\n",
    "    if isfile(\"checkpoints/best_model.jld2\")\n",
    "        hf_push_checkpoint(HF_REPO_ID; checkpoint_path=\"checkpoints/best_model.jld2\")\n",
    "    else\n",
    "        println(\"No best_model.jld2 found -- train first!\")\n",
    "    end\n",
    "\n",
    "    if isfile(\"checkpoints/final_model.jld2\")\n",
    "        hf_push(HF_REPO_ID, \"checkpoints/final_model.jld2\")\n",
    "    end\n",
    "\n",
    "    println(\"\\nDone! View your model at: https://huggingface.co/$HF_REPO_ID\")\n",
    "else\n",
    "    println(\"Set HF_REPO_ID in the login cell (e.g. \\\"yourusername/juliaflux-philosophy\\\")\")\n",
    "end\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---",
    "## 8b. Pull Checkpoint from HuggingFace Hub",
    "",
    "Download a previously pushed checkpoint to resume training in a new Colab session."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "if @isdefined(HF_REPO_ID) && !isempty(HF_REPO_ID)\n",
    "    mkpath(\"checkpoints\")\n",
    "    hf_pull(HF_REPO_ID, \"best_model.jld2\"; local_dir=\"checkpoints\")\n",
    "    println(\"\\nReady to resume from checkpoints/best_model.jld2\")\n",
    "    println(\"Run the 'Resume Training' cell below.\")\n",
    "else\n",
    "    println(\"Set HF_REPO_ID in the login cell (e.g. \\\"yourusername/juliaflux-philosophy\\\")\")\n",
    "end\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---",
    "## 9. Resume Training from Checkpoint",
    "",
    "Load a saved checkpoint and continue training for more steps.",
    "Skip this cell if you're training from scratch above."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "RESUME_FROM = \"checkpoints/best_model.jld2\"\nEXTRA_ITERS = 2000\n\nif !isfile(RESUME_FROM)\n    # Try pulling from HuggingFace\n    if @isdefined(HF_REPO_ID) && !isempty(HF_REPO_ID)\n        println(\"Checkpoint not found locally, pulling from HuggingFace...\")\n        hf_pull(HF_REPO_ID, basename(RESUME_FROM); local_dir=\"checkpoints\")\n    end\n    isfile(RESUME_FROM) || error(\"Checkpoint not found: $RESUME_FROM\")\nend\n\nckpt = load_checkpoint(RESUME_FROM, device)\nmodel = ckpt.model\nopt_state = ckpt.opt_state\nstart_iter = ckpt.step + 1\nbest_val = ckpt.best_val_loss\ntrain_loss_history = copy(ckpt.train_losses)\nval_loss_history = copy(ckpt.val_losses)\nend_iter = ckpt.step + EXTRA_ITERS\n\n# ── Initialize W&B logging (if API key is set) ──\nif haskey(ENV, \"WANDB_API_KEY\") && !isempty(ENV[\"WANDB_API_KEY\"])\n    wandb_init()\nend\n\nprintln(\"\\nResuming from step $(ckpt.step) -> training to step $end_iter\")\nprintln(\"Best val loss so far: $(round(best_val, digits=4))\")\nt_start = time()\nlast_save_time = time()\n\ntry\n    for iter in start_iter:end_iter\n        global completed_iter = iter\n\n        lr_t = get_lr(min(iter, max_iters))\n        Flux.adjust!(opt_state, lr_t)\n\n        x, y = get_batch(\"train\")\n        loss, grads = Flux.withgradient(model) do m\n            logits = m(x)\n            y_flat = reshape(y, :)\n            logits_flat = reshape(logits, vocab_size, :)\n            onehot = Flux.onehotbatch(y_flat, 1:vocab_size) |> device\n            Flux.logitcrossentropy(logits_flat, onehot)\n        end\n        Flux.update!(opt_state, model, grads[1])\n        push!(train_loss_history, Float64(loss))\n\n        # Incremental GC to prevent GPU memory fragmentation\n        if iter % 100 == 0 && CUDA.functional()\n            GC.gc(false)\n        end\n\n        if iter % eval_interval == 0\n            losses = estimate_loss(model)\n            push!(val_loss_history, losses[\"val\"])\n            elapsed = round(time() - t_start, digits=1)\n            wandb_log(; step=iter, train_loss=Float64(loss), val_loss=losses[\"val\"], lr=lr_t)\n\n            improved = \"\"\n            if losses[\"val\"] < best_val\n                best_val = losses[\"val\"]\n                save_and_sync(\"checkpoints/best_model.jld2\", model, opt_state;\n                    step=iter, best_val_loss=best_val,\n                    train_losses=train_loss_history, val_losses=val_loss_history)\n                improved = \" << new best!\"\n            end\n\n            @printf(\"step %5d / %5d | train %.4f | val %.4f | lr %.2e | %.1fs%s\\n\",\n                    iter, end_iter, losses[\"train\"], losses[\"val\"], lr_t, elapsed, improved)\n        end\n\n        if iter % 1000 == 0\n            save_and_sync(\"checkpoints/checkpoint_latest.jld2\", model, opt_state;\n                step=iter, best_val_loss=best_val,\n                train_losses=train_loss_history, val_losses=val_loss_history)\n            last_save_time = time()\n        end\n\n        if time() - last_save_time > SAVE_INTERVAL\n            save_and_sync(\"checkpoints/checkpoint_latest.jld2\", model, opt_state;\n                step=iter, best_val_loss=best_val,\n                train_losses=train_loss_history, val_losses=val_loss_history)\n            last_save_time = time()\n            println(\"  [auto-save at step $iter]\")\n        end\n    end\ncatch e\n    if e isa InterruptException\n        println(\"\\n\\nTraining interrupted at step $completed_iter!\")\n    else\n        println(\"\\n\\nTraining error at step $completed_iter: $e\")\n    end\n    println(\"Saving emergency checkpoint...\")\n    save_and_sync(\"checkpoints/checkpoint_interrupted.jld2\", model, opt_state;\n        step=completed_iter, best_val_loss=best_val,\n        train_losses=train_loss_history, val_losses=val_loss_history)\n    if !(e isa InterruptException)\n        rethrow(e)\n    end\nend\n\nelapsed = round(time() - t_start, digits=1)\n@printf(\"\\nResume training complete in %.1fs\\n\", elapsed)\nwandb_finish()\n\nsave_and_sync(\"checkpoints/final_model.jld2\", model, opt_state;\n    step=end_iter, best_val_loss=best_val,\n    train_losses=train_loss_history, val_losses=val_loss_history)",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## 10. Download Checkpoint\n\nDownload the best model checkpoint to use elsewhere.\nIn Colab, use the Files panel (left sidebar) to download, or pull from HuggingFace Hub."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "if isdir(\"checkpoints\")\n",
    "    files = readdir(\"checkpoints\")\n",
    "    println(\"Saved checkpoints:\")\n",
    "    for f in files\n",
    "        path = joinpath(\"checkpoints\", f)\n",
    "        size_kb = round(filesize(path) / 1024, digits=1)\n",
    "        println(\"  $path ($(size_kb) KB)\")\n",
    "    end\n",
    "else\n",
    "    println(\"No checkpoints directory found. Train first!\")\n",
    "end\n"
   ],
   "outputs": [],
   "execution_count": null
  }
 ]
}